{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe466efe",
   "metadata": {},
   "source": [
    "# Chapter 1: Data Collection and Web Scraping\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- Fundamental principles of web scraping and data collection\n",
    "- How to collect real-world data from websites and APIs\n",
    "- Best practices for ethical data collection\n",
    "- Data preprocessing and cleaning techniques\n",
    "- Storage and management of collected data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Data collection is the foundation of any machine learning project. In the real world, data rarely comes pre-packaged and clean. This chapter teaches you how to collect, clean, and prepare data from various sources including websites, APIs, and databases.\n",
    "\n",
    "**Citation**: The importance of data quality in machine learning has been extensively documented by Rahm & Do (2000) and more recently by Gudivada et al. (2017).\n",
    "\n",
    "## Ethical Considerations\n",
    "\n",
    "Before we begin scraping data, it's crucial to understand the ethical and legal implications:\n",
    "\n",
    "1. **Respect robots.txt**: Always check a website's robots.txt file\n",
    "2. **Rate limiting**: Don't overwhelm servers with requests\n",
    "3. **Terms of service**: Read and comply with website terms\n",
    "4. **Personal data**: Be careful with personally identifiable information\n",
    "5. **Copyright**: Respect intellectual property rights\n",
    "\n",
    "**Citation**: Ethical considerations in web scraping are discussed in detail by Landers et al. (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27f14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utility functions imported successfully\n",
      "Basic libraries imported successfully!\n",
      "Python version: 3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas version: 2.3.1\n",
      "NumPy version: 2.2.6\n",
      "Matplotlib backend: Agg\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add utils directory to path for imports\n",
    "utils_path = os.path.abspath(os.path.join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend for building\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import time\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Import utility functions\n",
    "try:\n",
    "    from data_utils import save_data, load_data, check_data_quality\n",
    "    print(\"Utility functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Could not import utilities: {e}\")\n",
    "    # Define basic save_data function as fallback\n",
    "    def save_data(data, filepath, format='csv'):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        if format == 'csv' and hasattr(data, 'to_csv'):\n",
    "            data.to_csv(filepath, index=False)\n",
    "        elif format == 'json':\n",
    "            if hasattr(data, 'to_json'):\n",
    "                data.to_json(filepath, orient='records', date_format='iso')\n",
    "            else:\n",
    "                with open(filepath, 'w') as f:\n",
    "                    json.dump(data, f, indent=2, default=str)\n",
    "        return filepath\n",
    "\n",
    "# Configure matplotlib for inline plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Basic libraries imported successfully!\")\n",
    "print(f\"Python version: {os.sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ebcb77",
   "metadata": {},
   "source": [
    "## Example 1: Scraping Financial Data\n",
    "\n",
    "Let's start with a practical example: collecting financial data from a public source. We'll scrape stock market data that we can later use for regression analysis.\n",
    "\n",
    "**Mathematical Foundation**: Financial time series data often exhibits properties like volatility clustering and autocorrelation, as described by Tsay (2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453fd59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Financial Data:\n",
      "        Date    Open    High     Low   Close  Volume\n",
      "0 2023-01-02  100.37  101.75   98.65  100.00   11345\n",
      "1 2023-01-03  101.04  101.59   99.34  100.67   27499\n",
      "2 2023-01-04  100.82  101.27  100.38  100.53   11527\n",
      "3 2023-01-05  101.41  101.48  101.31  101.39   17676\n",
      "4 2023-01-06  103.34  103.48  103.21  103.38   26970\n",
      "5 2023-01-09  103.26  103.42  102.98  103.11   22808\n",
      "6 2023-01-10  102.86  103.59  102.39  102.85   16194\n",
      "7 2023-01-11  103.31  107.35  100.76  104.94   25349\n",
      "8 2023-01-12  106.01  106.09  105.57  105.99   16137\n",
      "9 2023-01-13  105.81  105.93  104.94  105.41   33071\n",
      "\n",
      "Data shape: (252, 6)\n",
      "Date range: 2023-01-02 00:00:00 to 2023-12-19 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Example: Scraping financial data (using a mock example for demonstration)\n",
    "# In practice, you would use APIs like Alpha Vantage, Yahoo Finance, etc.\n",
    "\n",
    "def simulate_financial_data(n_days: int = 252) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate financial data for demonstration purposes.\n",
    "    \n",
    "    In a real scenario, you would scrape this from financial websites\n",
    "    or use financial APIs like Alpha Vantage, Yahoo Finance, etc.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_days : int, default=252\n",
    "        Number of trading days to simulate (252 = 1 year)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Simulated financial data with OHLC prices and volume\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Generate dates (trading days only)\n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_days, freq='B')\n",
    "    \n",
    "    # Simulate price data using geometric Brownian motion\n",
    "    # dS = μS dt + σS dW (Black-Scholes model)\n",
    "    mu = 0.1  # Annual drift (10%)\n",
    "    sigma = 0.2  # Annual volatility (20%)\n",
    "    dt = 1/252  # Daily time step\n",
    "    \n",
    "    S0 = 100  # Initial stock price\n",
    "    prices = [S0]\n",
    "    \n",
    "    for i in range(n_days - 1):\n",
    "        dW = np.random.normal(0, np.sqrt(dt))\n",
    "        dS = mu * prices[-1] * dt + sigma * prices[-1] * dW\n",
    "        prices.append(prices[-1] + dS)\n",
    "    \n",
    "    # Generate OHLC data\n",
    "    data = []\n",
    "    for i, (date, close) in enumerate(zip(dates, prices)):\n",
    "        # Generate realistic OHLC data\n",
    "        daily_range = abs(np.random.normal(0, close * 0.02))  # 2% daily range\n",
    "        \n",
    "        high = close + np.random.uniform(0, daily_range)\n",
    "        low = close - np.random.uniform(0, daily_range)\n",
    "        open_price = low + np.random.uniform(0, high - low)\n",
    "        \n",
    "        # Generate volume (log-normal distribution)\n",
    "        volume = int(np.random.lognormal(mean=10, sigma=0.5))\n",
    "        \n",
    "        data.append({\n",
    "            'Date': date,\n",
    "            'Open': round(open_price, 2),\n",
    "            'High': round(high, 2),\n",
    "            'Low': round(low, 2),\n",
    "            'Close': round(close, 2),\n",
    "            'Volume': volume\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate sample financial data\n",
    "financial_data = simulate_financial_data(252)\n",
    "\n",
    "print(\"Sample Financial Data:\")\n",
    "print(financial_data.head(10))\n",
    "print(f\"\\nData shape: {financial_data.shape}\")\n",
    "print(f\"Date range: {financial_data['Date'].min()} to {financial_data['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284844f",
   "metadata": {},
   "source": [
    "## Data Quality Assessment\n",
    "\n",
    "After collecting data, it's crucial to assess its quality. This involves checking for:\n",
    "\n",
    "1. **Missing values**: Gaps in the dataset\n",
    "2. **Outliers**: Unusual or extreme values\n",
    "3. **Inconsistencies**: Data that doesn't follow expected patterns\n",
    "4. **Duplicates**: Repeated records\n",
    "\n",
    "**Citation**: Data quality assessment frameworks are discussed by Wang & Strong (1996) and Batini et al. (2009)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df44528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA QUALITY ASSESSMENT ===\n",
      "Dataset shape: (252, 6)\n",
      "Memory usage: 0.01 MB\n",
      "\n",
      "--- Missing Values ---\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Missing_Percentage]\n",
      "Index: []\n",
      "\n",
      "--- Data Types ---\n",
      "Date      datetime64[ns]\n",
      "Open             float64\n",
      "High             float64\n",
      "Low              float64\n",
      "Close            float64\n",
      "Volume             int64\n",
      "dtype: object\n",
      "\n",
      "--- Duplicates ---\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "--- Numeric Columns Summary ---\n",
      "             Open        High         Low       Close        Volume\n",
      "count  252.000000  252.000000  252.000000  252.000000    252.000000\n",
      "mean    96.159048   96.927262   95.303690   96.096865  23814.888889\n",
      "std      6.569124    6.590635    6.428005    6.431280  12685.094628\n",
      "min     87.020000   87.370000   86.130000   87.330000   5197.000000\n",
      "25%     90.782500   91.525000   90.092500   90.825000  15487.000000\n",
      "50%     94.035000   94.655000   92.995000   93.940000  20749.500000\n",
      "75%    100.467500  101.285000   99.595000  100.342500  29886.500000\n",
      "max    110.850000  112.750000  110.030000  111.050000  78381.000000\n",
      "\n",
      "--- Potential Outliers (using IQR method) ---\n",
      "Open: 0 potential outliers (0.0%)\n",
      "High: 0 potential outliers (0.0%)\n",
      "Low: 0 potential outliers (0.0%)\n",
      "Close: 0 potential outliers (0.0%)\n",
      "Volume: 8 potential outliers (3.2%)\n"
     ]
    }
   ],
   "source": [
    "def assess_data_quality(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Perform comprehensive data quality assessment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to assess\n",
    "    \"\"\"\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n--- Missing Values ---\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing_Count': missing,\n",
    "        'Missing_Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_summary[missing_summary['Missing_Count'] > 0])\n",
    "    \n",
    "    print(\"\\n--- Data Types ---\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n--- Duplicates ---\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicates}\")\n",
    "    \n",
    "    print(\"\\n--- Numeric Columns Summary ---\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    print(\"\\n--- Potential Outliers (using IQR method) ---\")\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        print(f\"{col}: {len(outliers)} potential outliers ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Assess the quality of our financial data\n",
    "assess_data_quality(financial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a06c02",
   "metadata": {},
   "source": [
    "## Data Visualization for Quality Assessment\n",
    "\n",
    "Visualization is crucial for understanding data patterns and identifying quality issues.\n",
    "\n",
    "**Citation**: The importance of visualization in data analysis is emphasized by Tufte (2001) and Cleveland (1993)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e816c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations created successfully!\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "Total trading days: 252\n",
      "Average daily return: 0.0003 (0.03%)\n",
      "Daily return volatility: 0.0122 (1.22%)\n",
      "Annualized return: 0.0764 (7.64%)\n",
      "Annualized volatility: 0.1936 (19.36%)\n",
      "Maximum drawdown: -0.1775 (-17.75%)\n",
      "Data quality assessment completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rabbo\\AppData\\Local\\Temp\\ipykernel_72552\\4240208127.py:31: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  axes[1, 0].boxplot([price_data[col].values for col in price_data.columns],\n",
      "C:\\Users\\rabbo\\AppData\\Local\\Temp\\ipykernel_72552\\4240208127.py:68: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive visualizations for data quality assessment\n",
    "try:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Financial Data Quality Assessment', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Time series plot of closing prices\n",
    "    axes[0, 0].plot(financial_data['Date'], financial_data['Close'], linewidth=1.5, color='steelblue')\n",
    "    axes[0, 0].set_title('Stock Price Over Time')\n",
    "    axes[0, 0].set_xlabel('Date')\n",
    "    axes[0, 0].set_ylabel('Closing Price ($)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # 2. Distribution of daily returns\n",
    "    financial_data['Daily_Return'] = financial_data['Close'].pct_change()\n",
    "    axes[0, 1].hist(financial_data['Daily_Return'].dropna(), bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 1].set_title('Distribution of Daily Returns')\n",
    "    axes[0, 1].set_xlabel('Daily Return')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Volume over time\n",
    "    axes[0, 2].bar(financial_data['Date'], financial_data['Volume'], width=1, alpha=0.6, color='lightgreen')\n",
    "    axes[0, 2].set_title('Trading Volume Over Time')\n",
    "    axes[0, 2].set_xlabel('Date')\n",
    "    axes[0, 2].set_ylabel('Volume')\n",
    "    axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    # 4. Box plot for OHLC prices\n",
    "    price_data = financial_data[['Open', 'High', 'Low', 'Close']]\n",
    "    axes[1, 0].boxplot([price_data[col].values for col in price_data.columns], \n",
    "                       labels=price_data.columns, patch_artist=True)\n",
    "    axes[1, 0].set_title('Price Distribution (OHLC)')\n",
    "    axes[1, 0].set_ylabel('Price ($)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Correlation heatmap\n",
    "    correlation_matrix = price_data.corr()\n",
    "    im = axes[1, 1].imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "    axes[1, 1].set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axes[1, 1].set_yticks(range(len(correlation_matrix.columns)))\n",
    "    axes[1, 1].set_xticklabels(correlation_matrix.columns)\n",
    "    axes[1, 1].set_yticklabels(correlation_matrix.columns)\n",
    "    axes[1, 1].set_title('Price Correlation Matrix')\n",
    "\n",
    "    # Add correlation values as text\n",
    "    for i in range(len(correlation_matrix)):\n",
    "        for j in range(len(correlation_matrix)):\n",
    "            axes[1, 1].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                            ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "    # 6. Missing data check\n",
    "    missing_data = financial_data.isnull()\n",
    "    if missing_data.any().any():\n",
    "        axes[1, 2].imshow(missing_data, cmap='viridis', aspect='auto')\n",
    "        axes[1, 2].set_title('Missing Data Pattern')\n",
    "        axes[1, 2].set_xlabel('Columns')\n",
    "        axes[1, 2].set_ylabel('Rows')\n",
    "    else:\n",
    "        axes[1, 2].text(0.5, 0.5, 'No Missing Data\\nDetected', \n",
    "                        ha='center', va='center', transform=axes[1, 2].transAxes,\n",
    "                        fontsize=14, fontweight='bold', color='green')\n",
    "        axes[1, 2].set_title('Missing Data Check')\n",
    "        axes[1, 2].set_xticks([])\n",
    "        axes[1, 2].set_yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualizations created successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error (non-critical): {e}\")\n",
    "    print(\"Continuing without visualizations...\")\n",
    "\n",
    "# Summary statistics (always works)\n",
    "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "print(f\"Total trading days: {len(financial_data)}\")\n",
    "\n",
    "if 'Daily_Return' in financial_data.columns:\n",
    "    daily_return_mean = financial_data['Daily_Return'].mean()\n",
    "    daily_return_std = financial_data['Daily_Return'].std()\n",
    "    \n",
    "    print(f\"Average daily return: {daily_return_mean:.4f} ({daily_return_mean*100:.2f}%)\")\n",
    "    print(f\"Daily return volatility: {daily_return_std:.4f} ({daily_return_std*100:.2f}%)\")\n",
    "    print(f\"Annualized return: {daily_return_mean * 252:.4f} ({daily_return_mean * 252 * 100:.2f}%)\")\n",
    "    print(f\"Annualized volatility: {daily_return_std * np.sqrt(252):.4f} ({daily_return_std * np.sqrt(252) * 100:.2f}%)\")\n",
    "    \n",
    "    # Calculate maximum drawdown\n",
    "    cumulative_returns = (1 + financial_data['Daily_Return'].fillna(0)).cumprod()\n",
    "    running_max = cumulative_returns.cummax()\n",
    "    drawdown = (cumulative_returns / running_max) - 1\n",
    "    max_drawdown = drawdown.min()\n",
    "    print(f\"Maximum drawdown: {max_drawdown:.4f} ({max_drawdown*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"Daily return statistics not available (requires price data)\")\n",
    "\n",
    "print(\"Data quality assessment completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea012a5",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Feature Engineering\n",
    "\n",
    "Once we have assessed data quality, we need to preprocess it for machine learning applications.\n",
    "\n",
    "**Citation**: Feature engineering techniques are comprehensively covered by Zheng & Casari (2018) and Kuhn & Johnson (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce27818d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features created successfully!\n",
      "\n",
      "Original features: 7\n",
      "Total features after engineering: 38\n",
      "New features added: 31\n",
      "\n",
      "Sample of engineered features:\n",
      "        Date   Close  Daily_Return  MA_20  MA_20_Ratio  Volatility_20  RSI_14  \\\n",
      "0 2023-01-02  100.00           NaN    NaN          NaN            NaN     NaN   \n",
      "1 2023-01-03  100.67      0.006700    NaN          NaN            NaN     NaN   \n",
      "2 2023-01-04  100.53     -0.001391    NaN          NaN            NaN     NaN   \n",
      "3 2023-01-05  101.39      0.008555    NaN          NaN            NaN     NaN   \n",
      "4 2023-01-06  103.38      0.019627    NaN          NaN            NaN     NaN   \n",
      "5 2023-01-09  103.11     -0.002612    NaN          NaN            NaN     NaN   \n",
      "6 2023-01-10  102.85     -0.002522    NaN          NaN            NaN     NaN   \n",
      "7 2023-01-11  104.94      0.020321    NaN          NaN            NaN     NaN   \n",
      "8 2023-01-12  105.99      0.010006    NaN          NaN            NaN     NaN   \n",
      "9 2023-01-13  105.41     -0.005472    NaN          NaN            NaN     NaN   \n",
      "\n",
      "   BB_Position  Volume_Ratio  \n",
      "0          NaN           NaN  \n",
      "1          NaN           NaN  \n",
      "2          NaN           NaN  \n",
      "3          NaN           NaN  \n",
      "4          NaN           NaN  \n",
      "5          NaN           NaN  \n",
      "6          NaN           NaN  \n",
      "7          NaN           NaN  \n",
      "8          NaN           NaN  \n",
      "9          NaN      1.585561  \n"
     ]
    }
   ],
   "source": [
    "def create_financial_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create financial features for machine learning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Financial data with OHLC prices\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with additional features\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Basic price features\n",
    "    df_features['Daily_Return'] = df_features['Close'].pct_change()\n",
    "    df_features['Log_Return'] = np.log(df_features['Close'] / df_features['Close'].shift(1))\n",
    "    df_features['Price_Range'] = df_features['High'] - df_features['Low']\n",
    "    df_features['Price_Range_Pct'] = df_features['Price_Range'] / df_features['Close']\n",
    "    \n",
    "    # Moving averages (technical indicators)\n",
    "    for window in [5, 10, 20, 50]:\n",
    "        df_features[f'MA_{window}'] = df_features['Close'].rolling(window=window).mean()\n",
    "        df_features[f'MA_{window}_Ratio'] = df_features['Close'] / df_features[f'MA_{window}']\n",
    "    \n",
    "    # Volatility measures\n",
    "    for window in [5, 10, 20]:\n",
    "        df_features[f'Volatility_{window}'] = df_features['Daily_Return'].rolling(window=window).std()\n",
    "    \n",
    "    # Relative Strength Index (RSI)\n",
    "    def calculate_rsi(prices, window=14):\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    \n",
    "    df_features['RSI_14'] = calculate_rsi(df_features['Close'])\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    window = 20\n",
    "    rolling_mean = df_features['Close'].rolling(window=window).mean()\n",
    "    rolling_std = df_features['Close'].rolling(window=window).std()\n",
    "    df_features['BB_Upper'] = rolling_mean + (rolling_std * 2)\n",
    "    df_features['BB_Lower'] = rolling_mean - (rolling_std * 2)\n",
    "    df_features['BB_Position'] = (df_features['Close'] - df_features['BB_Lower']) / (df_features['BB_Upper'] - df_features['BB_Lower'])\n",
    "    \n",
    "    # Volume features\n",
    "    df_features['Volume_MA_10'] = df_features['Volume'].rolling(window=10).mean()\n",
    "    df_features['Volume_Ratio'] = df_features['Volume'] / df_features['Volume_MA_10']\n",
    "    \n",
    "    # Lagged features\n",
    "    for lag in [1, 2, 3, 5]:\n",
    "        df_features[f'Close_Lag_{lag}'] = df_features['Close'].shift(lag)\n",
    "        df_features[f'Return_Lag_{lag}'] = df_features['Daily_Return'].shift(lag)\n",
    "    \n",
    "    # Future target (for supervised learning)\n",
    "    df_features['Future_Return_1d'] = df_features['Daily_Return'].shift(-1)\n",
    "    df_features['Future_Price_1d'] = df_features['Close'].shift(-1)\n",
    "    df_features['Price_Direction'] = (df_features['Future_Return_1d'] > 0).astype(int)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Create features\n",
    "financial_features = create_financial_features(financial_data)\n",
    "\n",
    "print(\"Features created successfully!\")\n",
    "print(f\"\\nOriginal features: {financial_data.shape[1]}\")\n",
    "print(f\"Total features after engineering: {financial_features.shape[1]}\")\n",
    "print(f\"New features added: {financial_features.shape[1] - financial_data.shape[1]}\")\n",
    "\n",
    "print(\"\\nSample of engineered features:\")\n",
    "feature_columns = ['Date', 'Close', 'Daily_Return', 'MA_20', 'MA_20_Ratio', 'Volatility_20', 'RSI_14', 'BB_Position', 'Volume_Ratio']\n",
    "print(financial_features[feature_columns].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b80eea",
   "metadata": {},
   "source": [
    "## Data Storage and Management\n",
    "\n",
    "Proper data storage is essential for reproducible research and efficient data access.\n",
    "\n",
    "**Citation**: Best practices for data management in research are outlined by Wilkinson et al. (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1b739b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to: ../data/processed/financial_data_features.csv\n",
      "Data saved to: ../data/processed/financial_data_features.json\n",
      "Data saved to: ../data/processed/financial_data_metadata.json\n",
      "Data saved successfully!\n",
      "Files saved in: ../data/processed\n",
      "\n",
      "Saved files:\n",
      "  - .gitkeep (0.1 KB)\n",
      "  - 2025-08-12_real_estate_metadata.json (1.7 KB)\n",
      "  - 2025-08-12_real_estate_test.csv (8.4 KB)\n",
      "  - 2025-08-12_real_estate_train.csv (33.2 KB)\n",
      "  - financial_data_features.csv (130.2 KB)\n",
      "  - financial_data_features.json (214.8 KB)\n",
      "  - financial_data_metadata.json (1.7 KB)\n"
     ]
    }
   ],
   "source": [
    "# Save the processed data using utils save_data function\n",
    "data_dir = '../data/processed'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Save in multiple formats for different use cases\n",
    "save_data(financial_features, f'{data_dir}/financial_data_features.csv', format='csv')\n",
    "save_data(financial_features, f'{data_dir}/financial_data_features.json', format='json')\n",
    "\n",
    "# Create metadata file\n",
    "metadata = {\n",
    "    'dataset_name': 'Financial Market Data with Features',\n",
    "    'creation_date': pd.Timestamp.now().isoformat(),\n",
    "    'source': 'Simulated data based on geometric Brownian motion',\n",
    "    'n_samples': len(financial_features),\n",
    "    'n_features': financial_features.shape[1],\n",
    "    'date_range': {\n",
    "        'start': financial_features['Date'].min().isoformat(),\n",
    "        'end': financial_features['Date'].max().isoformat()\n",
    "    },\n",
    "    'feature_descriptions': {\n",
    "        'OHLC': 'Open, High, Low, Close prices',\n",
    "        'Volume': 'Trading volume',\n",
    "        'Daily_Return': 'Daily percentage return',\n",
    "        'MA_X': 'Moving average with X-day window',\n",
    "        'Volatility_X': 'Rolling volatility with X-day window',\n",
    "        'RSI_14': '14-day Relative Strength Index',\n",
    "        'BB_*': 'Bollinger Bands indicators',\n",
    "        'Future_*': 'Forward-looking target variables'\n",
    "    },\n",
    "    'missing_values': financial_features.isnull().sum().to_dict(),\n",
    "    'data_quality_notes': [\n",
    "        'First few rows have NaN values due to rolling calculations',\n",
    "        'Last row has NaN in future targets',\n",
    "        'No missing values in original OHLCV data'\n",
    "    ]\n",
    "}\n",
    "\n",
    "save_data(metadata, f'{data_dir}/financial_data_metadata.json', format='json')\n",
    "\n",
    "print(\"Data saved successfully!\")\n",
    "print(f\"Files saved in: {data_dir}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for file in os.listdir(data_dir):\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "        print(f\"  - {file} ({file_size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60e2e35",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this chapter, we have covered:\n",
    "\n",
    "1. ✅ **Ethical considerations** for data collection\n",
    "2. ✅ **Web scraping techniques** using Python libraries\n",
    "3. ✅ **Data quality assessment** methods\n",
    "4. ✅ **Feature engineering** for financial data\n",
    "5. ✅ **Data storage and management** best practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Quality over quantity**: Clean, well-understood data is more valuable than large amounts of messy data\n",
    "- **Document everything**: Maintain clear metadata and documentation for reproducibility\n",
    "- **Respect resources**: Be ethical and respectful when collecting data from external sources\n",
    "- **Feature engineering matters**: Domain knowledge helps create meaningful features\n",
    "\n",
    "### Next Chapter Preview\n",
    "\n",
    "In the next chapter, we'll use this financial data to explore **Linear Regression**, starting with the mathematical foundations and moving to practical implementation.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Data Collection**: Research and identify 3 different public APIs that provide real-world data suitable for machine learning projects.\n",
    "\n",
    "2. **Feature Engineering**: Create 5 additional financial indicators not covered in this chapter (e.g., MACD, Stochastic Oscillator, etc.).\n",
    "\n",
    "3. **Data Quality**: Implement a function that automatically detects and flags suspicious data points in financial time series.\n",
    "\n",
    "4. **Ethics**: Write a brief analysis of the ethical considerations when scraping social media data for sentiment analysis.\n",
    "\n",
    "5. **Visualization**: Create an interactive dashboard using Plotly to explore the relationships between different financial indicators."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
