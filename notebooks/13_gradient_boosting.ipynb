{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e2dcf61",
   "metadata": {},
   "source": [
    "# Chapter 13: Gradient Boosting Machines\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of gradient boosting algorithms\n",
    "- **XGBoost, LightGBM, and CatBoost** implementations\n",
    "- **Regularization techniques** to prevent overfitting\n",
    "- **Hyperparameter tuning** for optimal performance\n",
    "- **Feature importance** and model interpretation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gradient Boosting Machines (GBM) build strong predictors by sequentially combining weak learners, typically decision trees, where each new model corrects errors from previous models.\n",
    "\n",
    "**Mathematical Foundation**: Gradient boosting minimizes a loss function by fitting new models to the negative gradient of the loss function with respect to predictions.\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### Gradient Boosting Algorithm\n",
    "\n",
    "The algorithm iteratively builds an ensemble:\n",
    "\n",
    "$$F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)$$\n",
    "\n",
    "Where:\n",
    "- $F_m(x)$ is the ensemble after $m$ iterations\n",
    "- $h_m(x)$ is the $m$-th weak learner\n",
    "- $\\gamma_m$ is the step size\n",
    "\n",
    "The weak learner $h_m$ is trained on the negative gradient:\n",
    "\n",
    "$$r_{im} = -\\frac{\\partial L(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}$$\n",
    "\n",
    "**Citation**: Gradient boosting theory and algorithms are detailed in ensemble learning literature."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
