{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af277ae",
   "metadata": {},
   "source": [
    "# Chapter 19: Q-Learning (Reinforcement Learning)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of Markov Decision Processes\n",
    "- **Q-learning algorithm** and value function approximation\n",
    "- **Exploration vs exploitation** strategies\n",
    "- **Implementation** from scratch for grid world environments\n",
    "- **Convergence guarantees** and theoretical properties\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that learns optimal action-value functions without requiring knowledge of the environment's transition probabilities.\n",
    "\n",
    "**Mathematical Foundation**: Q-Learning uses temporal difference learning to estimate the optimal action-value function through iterative updates based on observed rewards and future value estimates.\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### Markov Decision Process\n",
    "\n",
    "An MDP is defined by the tuple $(S, A, P, R, \\gamma)$:\n",
    "- $S$: state space\n",
    "- $A$: action space\n",
    "- $P$: transition probabilities\n",
    "- $R$: reward function\n",
    "- $\\gamma$: discount factor\n",
    "\n",
    "### Q-Learning Update Rule\n",
    "\n",
    "$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t)\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$ is the learning rate\n",
    "- $r_{t+1}$ is the immediate reward\n",
    "- $\\gamma$ is the discount factor\n",
    "\n",
    "**Citation**: Reinforcement learning theory and Q-learning convergence properties are covered in computational learning literature."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
