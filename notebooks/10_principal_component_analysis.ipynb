{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9feebc9c",
   "metadata": {},
   "source": [
    "# Chapter 10: Principal Component Analysis (PCA)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of PCA and eigenvalue decomposition\n",
    "- **Dimensionality reduction** techniques and applications\n",
    "- **Variance explanation** and component interpretation\n",
    "- **Implementation** from scratch using NumPy\n",
    "- **Visualization** of high-dimensional data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Principal Component Analysis (PCA) is a fundamental dimensionality reduction technique that projects high-dimensional data onto lower-dimensional spaces while preserving maximum variance.\n",
    "\n",
    "**Mathematical Foundation**: PCA finds the principal components as eigenvectors of the covariance matrix, providing optimal linear projection for variance preservation.\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### Covariance Matrix and Eigendecomposition\n",
    "\n",
    "For a data matrix $X$ with centered columns, the covariance matrix is:\n",
    "\n",
    "$$C = \\frac{1}{n-1} X^T X$$\n",
    "\n",
    "The principal components are eigenvectors of $C$:\n",
    "\n",
    "$$C v_i = \\lambda_i v_i$$\n",
    "\n",
    "Where $v_i$ are the principal components and $\\lambda_i$ are the corresponding eigenvalues representing explained variance.\n",
    "\n",
    "**Citation**: The mathematical foundations of PCA are covered in multivariate statistics and linear algebra textbooks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
