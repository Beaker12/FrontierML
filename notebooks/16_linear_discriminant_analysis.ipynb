{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2608a8a",
   "metadata": {},
   "source": [
    "# Chapter 16: Linear Discriminant Analysis (LDA)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of Fisher's linear discriminant\n",
    "- **Dimensionality reduction** for classification\n",
    "- **Assumptions** about data distribution and homoscedasticity\n",
    "- **Implementation** from scratch and with Scikit-learn\n",
    "- **Comparison** with PCA and other dimensionality reduction techniques\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear Discriminant Analysis (LDA) is both a dimensionality reduction technique and a classifier that finds linear combinations of features that best separate different classes.\n",
    "\n",
    "**Mathematical Foundation**: LDA maximizes the ratio of between-class variance to within-class variance, finding optimal projection directions for class separation.\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### Fisher's Linear Discriminant\n",
    "\n",
    "LDA seeks to maximize the Fisher criterion:\n",
    "\n",
    "$$J(w) = \\frac{w^T S_B w}{w^T S_W w}$$\n",
    "\n",
    "Where:\n",
    "- $S_B$ is the between-class scatter matrix\n",
    "- $S_W$ is the within-class scatter matrix\n",
    "- $w$ is the projection vector\n",
    "\n",
    "### Scatter Matrices\n",
    "\n",
    "**Between-class scatter**: $S_B = \\sum_{i=1}^{c} N_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T$\n",
    "\n",
    "**Within-class scatter**: $S_W = \\sum_{i=1}^{c} \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T$\n",
    "\n",
    "**Citation**: Fisher's discriminant analysis and its extensions are covered in pattern recognition and multivariate statistics literature."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
