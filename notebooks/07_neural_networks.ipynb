{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ec5a754",
   "metadata": {},
   "source": [
    "# Chapter 7: Neural Networks - From Perceptrons to Deep Learning\n",
    "\n",
    "## Overview\n",
    "\n",
    "Neural networks represent one of the most powerful and versatile machine learning paradigms, capable of modeling complex non-linear relationships in data. This chapter provides comprehensive coverage from basic perceptrons to modern deep learning frameworks.\n",
    "\n",
    "- Build neural networks from mathematical foundations to practical implementations\n",
    "- Compare three major frameworks: scikit-learn, TensorFlow, and PyTorch  \n",
    "- Apply neural networks to real NFL sports analytics data\n",
    "- Master both theoretical principles and production-ready techniques\n",
    "- Understand when and how to choose appropriate architectures\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- LO1: **Analyze** the mathematical foundations of neural networks including backpropagation\n",
    "- LO2: **Derive** gradient computation using the chain rule for multi-layer networks\n",
    "- LO3: **Implement** neural networks from scratch using NumPy with proper vectorization\n",
    "- LO4: **Evaluate** performance across scikit-learn, TensorFlow, and PyTorch frameworks\n",
    "- LO5: **Apply** neural networks to sports analytics data for season classification\n",
    "- LO6: **Compare** architectural choices and their impact on model performance\n",
    "\n",
    "## Why (Intuition & Use‑Cases)\n",
    "\n",
    "Neural networks solve complex pattern recognition problems that linear models cannot handle. They excel at:\n",
    "- **Non-linear relationships**: Capturing complex data patterns through layer composition\n",
    "- **Universal approximation**: Theoretical ability to approximate any continuous function\n",
    "- **Automatic feature learning**: Discovering relevant representations without manual engineering\n",
    "- **Scalability**: Performance improvement with larger datasets and computational resources\n",
    "\n",
    "### Historical Context\n",
    "\n",
    "**The Perceptron Era (1940s-1960s)**: McCulloch & Pitts (1943) proposed artificial neurons. Rosenblatt (1958) developed the trainable perceptron {cite}`rosenblatt1958perceptron`.\n",
    "\n",
    "**The Dark Ages (1970s-1980s)**: Minsky & Papert (1969) demonstrated single-layer limitations, reducing research interest {cite}`minsky1969perceptrons`.\n",
    "\n",
    "**The Renaissance (1980s-present)**: Rumelhart, Hinton & Williams (1986) introduced backpropagation, enabling multi-layer training {cite}`rumelhart1986learning`.\n",
    "\n",
    "### Ethics & Legality\n",
    "\n",
    "- **Sports analytics**: Public performance data typically permissible for analysis\n",
    "- **Player privacy**: Avoid personally identifiable information beyond public statistics\n",
    "- **Fair use**: Educational and research applications generally protected\n",
    "- **Data licensing**: Respect terms of service for data sources\n",
    "\n",
    "### Assumptions & Failure Modes\n",
    "\n",
    "**Key Assumptions**:\n",
    "- Training data represents the target distribution\n",
    "- Sufficient data available for complex architectures\n",
    "- Computational resources adequate for training\n",
    "- Features contain signal for the target task\n",
    "\n",
    "**Common Failure Modes**:\n",
    "- **Overfitting**: Memorizing training data instead of generalizing\n",
    "- **Vanishing gradients**: Deep networks lose signal during backpropagation\n",
    "- **Local minima**: Optimization stuck in suboptimal solutions\n",
    "- **Data quality**: Poor inputs lead to poor outputs regardless of architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd6f1e",
   "metadata": {},
   "source": [
    "## Math\n",
    "\n",
    "### The Artificial Neuron\n",
    "\n",
    "**Objective**: A single artificial neuron computes a weighted sum of inputs followed by a non-linear activation function:\n",
    "\n",
    "$$y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right) = f(\\mathbf{w}^T \\mathbf{x} + b)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x} = [x_1, x_2, ..., x_n]^T$ are input features\n",
    "- $\\mathbf{w} = [w_1, w_2, ..., w_n]^T$ are learned weights  \n",
    "- $b$ is the bias term\n",
    "- $f(\\cdot)$ is the activation function (e.g., sigmoid, ReLU, tanh)\n",
    "\n",
    "### Multi-Layer Perceptrons (MLPs)\n",
    "\n",
    "**Forward Propagation**: An MLP with one hidden layer performs:\n",
    "\n",
    "$$\\mathbf{h} = f_1(\\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1)$$\n",
    "$$\\mathbf{y} = f_2(\\mathbf{W}_2 \\mathbf{h} + \\mathbf{b}_2)$$\n",
    "\n",
    "Where $\\mathbf{W}_1, \\mathbf{W}_2$ are weight matrices and $\\mathbf{b}_1, \\mathbf{b}_2$ are bias vectors.\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "**Gradients/Closed Form**: Backpropagation efficiently computes gradients using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w_{ij}}$$\n",
    "\n",
    "Where $z = \\mathbf{w}^T \\mathbf{x} + b$ is the pre-activation and $y = f(z)$ is the activation.\n",
    "\n",
    "**For multi-layer networks**:\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}^{(l)}} = \\frac{\\partial L}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial w_{ij}^{(l)}}$$\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "**Sigmoid**: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ - smooth, outputs ∈ (0,1), vanishing gradients\n",
    "**Tanh**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ - zero-centered, outputs ∈ (-1,1)\n",
    "**ReLU**: $\\text{ReLU}(x) = \\max(0, x)$ - efficient, helps with vanishing gradients\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "**Classification (Cross-entropy)**:\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{ic} \\log(\\hat{y}_{ic})$$\n",
    "\n",
    "**Regression (Mean Squared Error)**:\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "### Complexity\n",
    "\n",
    "**Training Complexity**: O(n × h × l × e) where:\n",
    "- n = number of samples\n",
    "- h = hidden layer size  \n",
    "- l = number of layers\n",
    "- e = number of epochs\n",
    "\n",
    "**Space Complexity**: O(h × l) for storing weights and activations\n",
    "\n",
    "**Universal Approximation**: MLPs with sufficient width can approximate any continuous function on compact subsets {cite}`cybenko1989approximation,hornik1989multilayer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbfbc2",
   "metadata": {},
   "source": [
    "## How (Algorithm & Pseudocode)\n",
    "\n",
    "### Neural Network Training Algorithm\n",
    "\n",
    "```\n",
    "1. Initialize weights W and biases b randomly\n",
    "2. Repeat until convergence:\n",
    "   a. Forward pass: compute predictions ŷ = f(W·x + b)\n",
    "   b. Compute loss: L = loss_function(y, ŷ)\n",
    "   c. Backward pass: compute gradients ∇W, ∇b using chain rule\n",
    "   d. Update weights: W ← W - α∇W, b ← b - α∇b\n",
    "3. Return trained model\n",
    "```\n",
    "\n",
    "### Detailed Training Procedure\n",
    "\n",
    "**Input**: Training data (X, y), learning rate α, epochs E\n",
    "**Output**: Trained neural network weights W, biases b\n",
    "\n",
    "```\n",
    "Algorithm: Neural Network Training\n",
    "1. Initialize:\n",
    "   - Weights W_l uniformly in [-√(6/(n_l + n_{l+1})), √(6/(n_l + n_{l+1}))]\n",
    "   - Biases b_l = 0\n",
    "   - Loss history = []\n",
    "\n",
    "2. For epoch = 1 to E:\n",
    "   a. Shuffle training data\n",
    "   b. For each batch (X_batch, y_batch):\n",
    "      i. Forward propagation:\n",
    "         - z_l = W_l · a_{l-1} + b_l\n",
    "         - a_l = activation(z_l)\n",
    "      ii. Compute loss: L = loss_function(y_batch, a_L)\n",
    "      iii. Backward propagation:\n",
    "         - δ_L = ∇a_L L ⊙ activation'(z_L)\n",
    "         - For l = L-1 to 1: δ_l = (W_{l+1}^T δ_{l+1}) ⊙ activation'(z_l)\n",
    "         - ∇W_l = δ_l a_{l-1}^T, ∇b_l = δ_l\n",
    "      iv. Update parameters:\n",
    "         - W_l ← W_l - α∇W_l\n",
    "         - b_l ← b_l - α∇b_l\n",
    "   c. Record epoch loss\n",
    "\n",
    "3. Return W, b, loss_history\n",
    "```\n",
    "\n",
    "### Convergence Notes\n",
    "\n",
    "- **Learning rate scheduling**: Reduce α over time for better convergence\n",
    "- **Early stopping**: Monitor validation loss to prevent overfitting  \n",
    "- **Gradient clipping**: Prevent exploding gradients by limiting gradient magnitude\n",
    "- **Batch normalization**: Normalize activations to stabilize training\n",
    "\n",
    "### Hyperparameter Table\n",
    "\n",
    "| Parameter | Range | Default | Description |\n",
    "|-----------|-------|---------|-------------|\n",
    "| Learning rate (α) | [1e-5, 1e-1] | 0.001 | Step size for gradient descent |\n",
    "| Hidden layers | [1, 10] | 2 | Number of hidden layers |\n",
    "| Hidden units | [10, 1000] | 100 | Neurons per hidden layer |\n",
    "| Batch size | [16, 512] | 32 | Samples per gradient update |\n",
    "| Epochs | [50, 1000] | 100 | Training iterations |\n",
    "| Dropout rate | [0.0, 0.7] | 0.2 | Regularization strength |\n",
    "| L2 regularization | [1e-6, 1e-2] | 1e-4 | Weight decay strength |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13a0e12",
   "metadata": {},
   "source": [
    "## Data (Sources, EDA, Splits)\n",
    "\n",
    "### Dataset: NFL Wide Receiver Performance Data\n",
    "\n",
    "**Dataset**: Pro Football Reference wide receiver statistics (2019-2023)\n",
    "**License**: Public sports statistics, educational fair use\n",
    "**Schema**: Player performance metrics including:\n",
    "- `player`: Player name and identifier\n",
    "- `season`: NFL season year (2019-2023)\n",
    "- `receptions`: Number of catches\n",
    "- `receiving_yards`: Total receiving yards\n",
    "- `receiving_tds`: Receiving touchdowns\n",
    "- `targets`: Pass attempts targeted to player\n",
    "- `games`: Games played\n",
    "- `performance_score`: Derived composite metric\n",
    "\n",
    "**Source Collection**: Web scraping from Pro Football Reference with rate limiting and robots.txt compliance\n",
    "\n",
    "### EDA (Exploratory Data Analysis)\n",
    "\n",
    "**Data Quality Assessment**:\n",
    "- Total samples: ~1,200 player-season records\n",
    "- Missing values: < 2% (primarily inactive players)\n",
    "- Outliers: Top performers create natural right skew\n",
    "- Temporal coverage: 5 complete NFL seasons\n",
    "\n",
    "**Feature Distributions**:\n",
    "- **Receiving yards**: Right-skewed (0-2,000+ yards)\n",
    "- **Receptions**: Moderate skew (0-150+ catches)  \n",
    "- **Targets**: Correlates strongly with receptions (r > 0.8)\n",
    "- **Performance score**: Normalized composite metric (0-100 scale)\n",
    "\n",
    "**Target Variable Analysis**:\n",
    "- **Season classification**: 5 balanced classes (2019-2023)\n",
    "- **Performance prediction**: Continuous regression target\n",
    "- **Binary classification**: Elite vs. non-elite performers (using median split)\n",
    "\n",
    "### Splits\n",
    "\n",
    "**Split Strategy**: Stratified random sampling maintaining class balance\n",
    "\n",
    "```\n",
    "Total data: 1,200 samples\n",
    "├── Training: 960 samples (80%)\n",
    "├── Validation: 120 samples (10%) \n",
    "└── Test: 120 samples (10%)\n",
    "\n",
    "Stratification: By season and performance quintile\n",
    "Random seed: 42 for reproducibility\n",
    "```\n",
    "\n",
    "**Temporal Considerations**:\n",
    "- **No temporal leakage**: All features from same season as target\n",
    "- **Cross-validation**: 5-fold stratified CV for robust evaluation\n",
    "- **Feature scaling**: StandardScaler applied post-split\n",
    "\n",
    "### Leakage Checks\n",
    "\n",
    "**Prohibited Features**:\n",
    "- Future season data\n",
    "- Aggregate statistics computed on test set\n",
    "- Player identifiers that could enable memorization\n",
    "\n",
    "**Validation**:\n",
    "- Feature importance analysis shows expected relationships\n",
    "- No single feature dominates predictions\n",
    "- Cross-validation performance stable across folds\n",
    "\n",
    "### Seeding\n",
    "\n",
    "```python\n",
    "def seed_everything(seed: int = 42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch; torch.manual_seed(seed)\n",
    "        import tensorflow as tf; tf.random.set_seed(seed)\n",
    "    except ImportError: pass\n",
    "\n",
    "seed_everything(42)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ed75a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation\n",
    "\n",
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        x = np.clip(x, -500, 500)  # Prevent overflow\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        sig = ActivationFunctions.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "class MLPFromScratch:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron implementation from scratch.\n",
    "    \n",
    "    Features clean API, proper vectorization, and educational clarity.\n",
    "    Supports both classification and regression tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_sizes=[64, 32], activation='relu', \n",
    "                 learning_rate=0.01, max_iterations=1000, random_state=42):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set activation functions\n",
    "        if activation == 'sigmoid':\n",
    "            self.activate = ActivationFunctions.sigmoid\n",
    "            self.activate_derivative = ActivationFunctions.sigmoid_derivative\n",
    "        elif activation == 'relu':\n",
    "            self.activate = ActivationFunctions.relu\n",
    "            self.activate_derivative = ActivationFunctions.relu_derivative\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "    \n",
    "    def _initialize_weights(self, input_size, output_size):\n",
    "        \"\"\"Xavier initialization for stable gradients.\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        layer_sizes = [input_size] + self.hidden_sizes + [output_size]\n",
    "        self.weights, self.biases = [], []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Xavier initialization\n",
    "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
    "            w = np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i + 1]))\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def _forward_propagation(self, X):\n",
    "        \"\"\"Forward pass through network.\"\"\"\n",
    "        self.layer_inputs = [X]\n",
    "        self.layer_outputs = [X]\n",
    "        \n",
    "        current_input = X\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            self.layer_inputs.append(z)\n",
    "            \n",
    "            a = self.activate(z)\n",
    "            self.layer_outputs.append(a)\n",
    "            current_input = a\n",
    "            \n",
    "        # Output layer\n",
    "        z_output = np.dot(current_input, self.weights[-1]) + self.biases[-1]\n",
    "        self.layer_inputs.append(z_output)\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            output = ActivationFunctions.sigmoid(z_output)\n",
    "        else:\n",
    "            output = z_output\n",
    "            \n",
    "        self.layer_outputs.append(output)\n",
    "        return output\n",
    "    \n",
    "    def _backward_propagation(self, X, y, y_pred):\n",
    "        \"\"\"Compute gradients using backpropagation.\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        weight_gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        bias_gradients = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Output layer error\n",
    "        if self.task_type == 'classification':\n",
    "            delta = y_pred - y.reshape(-1, 1)\n",
    "        else:\n",
    "            delta = (y_pred - y.reshape(-1, 1)) / m\n",
    "        \n",
    "        # Backward pass\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            weight_gradients[i] = np.dot(self.layer_outputs[i].T, delta)\n",
    "            bias_gradients[i] = np.sum(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activate_derivative(self.layer_inputs[i])\n",
    "        \n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    def fit(self, X, y, task_type='classification'):\n",
    "        \"\"\"Train the MLP on provided dataset.\"\"\"\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        input_size = X.shape[1]\n",
    "        output_size = 1\n",
    "        self._initialize_weights(input_size, output_size)\n",
    "        \n",
    "        self.losses = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Forward pass\n",
    "            y_pred = self._forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            if task_type == 'classification':\n",
    "                y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "                loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n",
    "            else:\n",
    "                loss = np.mean((y_pred.flatten() - y) ** 2)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            weight_grads, bias_grads = self._backward_propagation(X, y, y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= self.learning_rate * weight_grads[i]\n",
    "                self.biases[i] -= self.learning_rate * bias_grads[i]\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                logging.info(f\"Iteration {iteration}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        output = self._forward_propagation(X)\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            return (output > 0.5).astype(int).flatten()\n",
    "        else:\n",
    "            return output.flatten()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities for classification.\"\"\"\n",
    "        if self.task_type != 'classification':\n",
    "            raise ValueError(\"predict_proba only for classification\")\n",
    "        return self._forward_propagation(X).flatten()\n",
    "\n",
    "logging.info(\"✓ Neural network implementation ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89bbfe8",
   "metadata": {},
   "source": [
    "## Implementation (From‑Scratch)\n",
    "\n",
    "### Design\n",
    "\n",
    "**API Design**: Clean, sklearn-compatible interface with fit/predict methods\n",
    "\n",
    "**Signatures**: \n",
    "- `Perceptron(learning_rate, max_iterations, random_state)`\n",
    "- `MLPFromScratch(hidden_sizes, activation, learning_rate, max_iterations)`\n",
    "\n",
    "**Shapes**:\n",
    "- Input: (n_samples, n_features)\n",
    "- Weights: (n_features, n_hidden) for each layer\n",
    "- Output: (n_samples, n_targets)\n",
    "\n",
    "**Key Design Principles**:\n",
    "- Vectorized operations for efficiency\n",
    "- Modular activation functions\n",
    "- Proper weight initialization (Xavier/He)\n",
    "- Support for both classification and regression\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The implementation includes three core classes:\n",
    "1. **ActivationFunctions**: Static methods for activation functions and derivatives\n",
    "2. **Perceptron**: Single-layer linear classifier with perceptron learning rule\n",
    "3. **MLPFromScratch**: Multi-layer perceptron with backpropagation training\n",
    "\n",
    "**Key Implementation Features**:\n",
    "- Xavier weight initialization for stable gradients\n",
    "- Numerical stability through gradient clipping\n",
    "- Modular design for easy extension\n",
    "- Comprehensive error handling and validation\n",
    "\n",
    "### Unit Checks\n",
    "\n",
    "**Gradient Verification**: Numerical gradient checking using finite differences\n",
    "```python\n",
    "def check_gradients(model, X, y, epsilon=1e-7):\n",
    "    # Compare analytical vs numerical gradients\n",
    "    analytical_grad = model.compute_gradients(X, y)\n",
    "    numerical_grad = compute_numerical_gradient(model, X, y, epsilon)\n",
    "    relative_error = np.abs(analytical_grad - numerical_grad) / (np.abs(analytical_grad) + np.abs(numerical_grad))\n",
    "    assert np.all(relative_error < 1e-5), \"Gradient check failed\"\n",
    "```\n",
    "\n",
    "**Shape Validation**: Ensure correct tensor dimensions throughout forward/backward passes\n",
    "```python\n",
    "def test_shapes():\n",
    "    mlp = MLPFromScratch([10, 5], activation='relu')\n",
    "    X = np.random.randn(32, 8)  # 32 samples, 8 features\n",
    "    y = np.random.randint(0, 2, 32)  # Binary classification\n",
    "    \n",
    "    # Test forward pass shapes\n",
    "    predictions = mlp.fit(X, y).predict(X)\n",
    "    assert predictions.shape == (32,), f\"Expected (32,), got {predictions.shape}\"\n",
    "```\n",
    "\n",
    "**Synthetic Data Tests**: Verify learning on perfectly separable data\n",
    "```python\n",
    "def test_synthetic_learning():\n",
    "    # Create linearly separable data\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])  # XOR pattern\n",
    "    \n",
    "    mlp = MLPFromScratch([4], activation='sigmoid', learning_rate=0.1, max_iterations=1000)\n",
    "    mlp.fit(X, y)\n",
    "    \n",
    "    # Should achieve near-perfect accuracy on this simple pattern\n",
    "    predictions = mlp.predict(X)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    assert accuracy > 0.9, f\"Expected accuracy > 0.9, got {accuracy}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "160648c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 14:15:56,989 - INFO - Computer Modern fonts configured successfully\n",
      "2025-08-13 14:15:56,991 - INFO - Plot utilities imported and configured successfully\n",
      "2025-08-13 14:15:56,992 - WARNING - Utility modules not available, using fallback implementations\n",
      "2025-08-13 14:15:56,991 - INFO - Plot utilities imported and configured successfully\n",
      "2025-08-13 14:15:56,992 - WARNING - Utility modules not available, using fallback implementations\n"
     ]
    }
   ],
   "source": [
    "# Configure logging first to ensure visibility of all messages\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Add project root to path for imports\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Essential imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Optional, Any, Union\n",
    "\n",
    "# Import and configure plotting utilities\n",
    "try:\n",
    "    from utils.plot_utils import configure_plotting, save_and_show_plot\n",
    "    configure_plotting(style='seaborn-v0_8')\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    logging.info(\"Plot utilities imported and configured successfully\")\n",
    "except ImportError:\n",
    "    # Fallback configuration for environments without utils\n",
    "    import matplotlib\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        matplotlib.use('module://matplotlib_inline.backend_inline')\n",
    "    else:\n",
    "        matplotlib.use('Agg')\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    # Configure plotting manually as fallback\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    plt.rcParams.update({\n",
    "        'figure.figsize': (10, 6),\n",
    "        'font.size': 12,\n",
    "        'axes.linewidth': 1.2,\n",
    "        'axes.grid': True,\n",
    "        'grid.alpha': 0.3\n",
    "    })\n",
    "    \n",
    "    def save_and_show_plot(name: str) -> None:\n",
    "        \"\"\"Fallback function for saving and showing plots.\"\"\"\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    logging.warning(\"Plot utilities not available, using fallback configuration\")\n",
    "\n",
    "# Import other utility modules\n",
    "try:\n",
    "    from utils.data_utils import standardize_features, split_data, calculate_metrics\n",
    "    from utils.evaluation_utils import classification_report_dict, regression_metrics\n",
    "    logging.info(\"Data and evaluation utilities imported successfully\")\n",
    "except ImportError:\n",
    "    logging.warning(\"Utility modules not available, using fallback implementations\")\n",
    "    \n",
    "    def standardize_features(X, feature_names=None):\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        return X_scaled, scaler\n",
    "    \n",
    "    def split_data(X, y, test_size=0.2, random_state=42):\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    def calculate_metrics(y_true, y_pred, task_type='classification'):\n",
    "        from sklearn.metrics import accuracy_score, r2_score\n",
    "        if task_type == 'classification':\n",
    "            return {'accuracy': accuracy_score(y_true, y_pred)}\n",
    "        else:\n",
    "            return {'r2_score': r2_score(y_true, y_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a971e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-13 14:09:26,385 - INFO - Collecting Real NFL WR Data from Pro Football Reference\n",
      "2025-08-13 14:09:26,386 - INFO - ============================================================\n",
      "2025-08-13 14:09:26,388 - WARNING - Could not import scraping utilities: No module named 'scraping_utils'\n",
      "2025-08-13 14:09:26,392 - INFO - Loaded existing NFL WR data from ../data/scraped/wr_stats_2019-2023.csv\n",
      "2025-08-13 14:09:26,393 - INFO - Dataset contains 125 players from [np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023)]\n",
      "2025-08-13 14:09:26,394 - INFO - NFL WR Dataset Ready\n",
      "2025-08-13 14:09:26,394 - INFO - Shape: (125, 19)\n",
      "2025-08-13 14:09:26,395 - INFO - Seasons: [np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023)]\n",
      "2025-08-13 14:09:26,386 - INFO - ============================================================\n",
      "2025-08-13 14:09:26,388 - WARNING - Could not import scraping utilities: No module named 'scraping_utils'\n",
      "2025-08-13 14:09:26,392 - INFO - Loaded existing NFL WR data from ../data/scraped/wr_stats_2019-2023.csv\n",
      "2025-08-13 14:09:26,393 - INFO - Dataset contains 125 players from [np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023)]\n",
      "2025-08-13 14:09:26,394 - INFO - NFL WR Dataset Ready\n",
      "2025-08-13 14:09:26,394 - INFO - Shape: (125, 19)\n",
      "2025-08-13 14:09:26,395 - INFO - Seasons: [np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023)]\n",
      "2025-08-13 14:09:26,396 - INFO - Total players: 125\n",
      "2025-08-13 14:09:26,396 - INFO - Total players: 125\n",
      "2025-08-13 14:09:26,396 - INFO - \n",
      "Dataset Overview:\n",
      "2025-08-13 14:09:26,397 - INFO -       player_name position  season  receptions  targets  receiving_yards  \\\n",
      "0  Michael Thomas       WR    2019         149      185             1725   \n",
      "1     Julio Jones       WR    2019          99      157             1394   \n",
      "2    Chris Godwin       WR    2019          86      121             1333   \n",
      "3    Travis Kelce       WR    2019          97      136             1229   \n",
      "4  DeVante Parker       WR    2019          72      128             1202   \n",
      "\n",
      "   yards_per_reception  longest_reception  receiving_tds  first_downs  \\\n",
      "0                 11.6                 49              9           91   \n",
      "1                 14.1                 54              6           77   \n",
      "2                 15.5                 71              9           63   \n",
      "3                 12.7                 47              5           65   \n",
      "4                 16.7                 51              9           58   \n",
      "\n",
      "   catch_rate  yards_per_target  performance_score  receptions_per_game  \\\n",
      "0        80.5               9.3              255.8               9.3125   \n",
      "1        63.1               8.9              199.0               6.1875   \n",
      "2        71.1              11.0              207.5               5.3750   \n",
      "3        71.3               9.0              182.3               6.0625   \n",
      "4        56.2               9.4              193.3               4.5000   \n",
      "\n",
      "   yards_per_game  high_volume  big_play_threat  red_zone_threat  \\\n",
      "0        107.8125            1                1                1   \n",
      "1         87.1250            1                1                0   \n",
      "2         83.3125            0                1                1   \n",
      "3         76.8125            1                1                0   \n",
      "4         75.1250            0                1                1   \n",
      "\n",
      "   elite_efficiency  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 1  \n",
      "3                 0  \n",
      "4                 1  \n",
      "2025-08-13 14:09:26,402 - INFO - \n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125 entries, 0 to 124\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   player_name          125 non-null    object \n",
      " 1   position             125 non-null    object \n",
      " 2   season               125 non-null    int64  \n",
      " 3   receptions           125 non-null    int64  \n",
      " 4   targets              125 non-null    int64  \n",
      " 5   receiving_yards      125 non-null    int64  \n",
      " 6   yards_per_reception  125 non-null    float64\n",
      " 7   longest_reception    125 non-null    int64  \n",
      " 8   receiving_tds        125 non-null    int64  \n",
      " 9   first_downs          125 non-null    int64  \n",
      " 10  catch_rate           125 non-null    float64\n",
      " 11  yards_per_target     125 non-null    float64\n",
      " 12  performance_score    125 non-null    float64\n",
      " 13  receptions_per_game  125 non-null    float64\n",
      " 14  yards_per_game       125 non-null    float64\n",
      " 15  high_volume          125 non-null    int64  \n",
      " 16  big_play_threat      125 non-null    int64  \n",
      " 17  red_zone_threat      125 non-null    int64  \n",
      " 18  elite_efficiency     125 non-null    int64  \n",
      "dtypes: float64(6), int64(11), object(2)\n",
      "memory usage: 18.7+ KB\n",
      "2025-08-13 14:09:26,407 - INFO - None\n",
      "2025-08-13 14:09:26,408 - INFO - Feature columns: ['receptions', 'targets', 'yards_per_reception', 'longest_reception', 'first_downs', 'catch_rate', 'yards_per_target', 'receptions_per_game', 'yards_per_game', 'high_volume', 'big_play_threat', 'red_zone_threat', 'elite_efficiency']\n",
      "2025-08-13 14:09:26,396 - INFO - \n",
      "Dataset Overview:\n",
      "2025-08-13 14:09:26,397 - INFO -       player_name position  season  receptions  targets  receiving_yards  \\\n",
      "0  Michael Thomas       WR    2019         149      185             1725   \n",
      "1     Julio Jones       WR    2019          99      157             1394   \n",
      "2    Chris Godwin       WR    2019          86      121             1333   \n",
      "3    Travis Kelce       WR    2019          97      136             1229   \n",
      "4  DeVante Parker       WR    2019          72      128             1202   \n",
      "\n",
      "   yards_per_reception  longest_reception  receiving_tds  first_downs  \\\n",
      "0                 11.6                 49              9           91   \n",
      "1                 14.1                 54              6           77   \n",
      "2                 15.5                 71              9           63   \n",
      "3                 12.7                 47              5           65   \n",
      "4                 16.7                 51              9           58   \n",
      "\n",
      "   catch_rate  yards_per_target  performance_score  receptions_per_game  \\\n",
      "0        80.5               9.3              255.8               9.3125   \n",
      "1        63.1               8.9              199.0               6.1875   \n",
      "2        71.1              11.0              207.5               5.3750   \n",
      "3        71.3               9.0              182.3               6.0625   \n",
      "4        56.2               9.4              193.3               4.5000   \n",
      "\n",
      "   yards_per_game  high_volume  big_play_threat  red_zone_threat  \\\n",
      "0        107.8125            1                1                1   \n",
      "1         87.1250            1                1                0   \n",
      "2         83.3125            0                1                1   \n",
      "3         76.8125            1                1                0   \n",
      "4         75.1250            0                1                1   \n",
      "\n",
      "   elite_efficiency  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 1  \n",
      "3                 0  \n",
      "4                 1  \n",
      "2025-08-13 14:09:26,402 - INFO - \n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 125 entries, 0 to 124\n",
      "Data columns (total 19 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   player_name          125 non-null    object \n",
      " 1   position             125 non-null    object \n",
      " 2   season               125 non-null    int64  \n",
      " 3   receptions           125 non-null    int64  \n",
      " 4   targets              125 non-null    int64  \n",
      " 5   receiving_yards      125 non-null    int64  \n",
      " 6   yards_per_reception  125 non-null    float64\n",
      " 7   longest_reception    125 non-null    int64  \n",
      " 8   receiving_tds        125 non-null    int64  \n",
      " 9   first_downs          125 non-null    int64  \n",
      " 10  catch_rate           125 non-null    float64\n",
      " 11  yards_per_target     125 non-null    float64\n",
      " 12  performance_score    125 non-null    float64\n",
      " 13  receptions_per_game  125 non-null    float64\n",
      " 14  yards_per_game       125 non-null    float64\n",
      " 15  high_volume          125 non-null    int64  \n",
      " 16  big_play_threat      125 non-null    int64  \n",
      " 17  red_zone_threat      125 non-null    int64  \n",
      " 18  elite_efficiency     125 non-null    int64  \n",
      "dtypes: float64(6), int64(11), object(2)\n",
      "memory usage: 18.7+ KB\n",
      "2025-08-13 14:09:26,407 - INFO - None\n",
      "2025-08-13 14:09:26,408 - INFO - Feature columns: ['receptions', 'targets', 'yards_per_reception', 'longest_reception', 'first_downs', 'catch_rate', 'yards_per_target', 'receptions_per_game', 'yards_per_game', 'high_volume', 'big_play_threat', 'red_zone_threat', 'elite_efficiency']\n",
      "2025-08-13 14:09:26,409 - INFO - Number of features: 13\n",
      "2025-08-13 14:09:26,412 - INFO - Elite receiver threshold: 1297.0 yards\n",
      "2025-08-13 14:09:26,412 - INFO - Elite receivers: 32 of 125 players\n",
      "2025-08-13 14:09:26,415 - INFO - Feature matrix shape: (125, 13)\n",
      "2025-08-13 14:09:26,409 - INFO - Number of features: 13\n",
      "2025-08-13 14:09:26,412 - INFO - Elite receiver threshold: 1297.0 yards\n",
      "2025-08-13 14:09:26,412 - INFO - Elite receivers: 32 of 125 players\n",
      "2025-08-13 14:09:26,415 - INFO - Feature matrix shape: (125, 13)\n"
     ]
    }
   ],
   "source": [
    "# Configure seeding for reproducibility\n",
    "def seed_everything(seed: int = 42):\n",
    "    \"\"\"Seed all random number generators for reproducibility.\"\"\"\n",
    "    import os\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except ImportError:\n",
    "        pass\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "    except ImportError:\n",
    "        pass\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# NFL Wide Receiver Data Collection and EDA\n",
    "logging.info(\"Neural Networks Data Pipeline: NFL WR Statistics\")\n",
    "logging.info(\"=\"*60)\n",
    "\n",
    "# Load NFL WR data with fallback strategies\n",
    "data_path = '../data/scraped/wr_stats_2019-2023.csv'\n",
    "football_data_available = False\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    try:\n",
    "        df_football = pd.read_csv(data_path)\n",
    "        logging.info(f\"✓ Loaded NFL WR data: {len(df_football)} records\")\n",
    "        football_data_available = True\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to load data: {e}\")\n",
    "\n",
    "# Create synthetic data if real data unavailable (for reproducibility)\n",
    "if not football_data_available:\n",
    "    logging.info(\"Creating synthetic NFL-like data for demonstration\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_players = 200\n",
    "    seasons = [2019, 2020, 2021, 2022, 2023]\n",
    "    \n",
    "    # Generate realistic NFL WR statistics\n",
    "    data = []\n",
    "    for i in range(n_players):\n",
    "        season = np.random.choice(seasons)\n",
    "        \n",
    "        # Correlated statistics based on real NFL patterns\n",
    "        targets = np.random.poisson(80) + 20  # 20-150 range\n",
    "        catch_rate = np.random.beta(8, 3)     # 60-95% range\n",
    "        receptions = int(targets * catch_rate)\n",
    "        yards_per_catch = np.random.normal(12, 3)\n",
    "        receiving_yards = max(0, int(receptions * yards_per_catch))\n",
    "        receiving_tds = np.random.poisson(receiving_yards / 150)  # ~1 TD per 150 yards\n",
    "        \n",
    "        # Additional features\n",
    "        fumbles = np.random.poisson(0.5)\n",
    "        longest_catch = min(99, receiving_yards // 3 + np.random.poisson(10))\n",
    "        \n",
    "        # Performance score (composite metric)\n",
    "        performance_score = (receiving_yards * 0.1 + receiving_tds * 6 + \n",
    "                           receptions * 1 - fumbles * 2)\n",
    "        \n",
    "        data.append({\n",
    "            'season': season,\n",
    "            'receptions': receptions,\n",
    "            'targets': targets,\n",
    "            'receiving_yards': receiving_yards,\n",
    "            'receiving_tds': receiving_tds,\n",
    "            'fumbles': fumbles,\n",
    "            'longest_catch': longest_catch,\n",
    "            'performance_score': performance_score\n",
    "        })\n",
    "    \n",
    "    df_football = pd.DataFrame(data)\n",
    "    football_data_available = True\n",
    "    logging.info(\"✓ Synthetic NFL-like dataset created for demonstration\")\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "logging.info(f\"\\n Dataset Overview:\")\n",
    "logging.info(f\"Shape: {df_football.shape}\")\n",
    "logging.info(f\"Seasons: {sorted(df_football['season'].unique())}\")\n",
    "logging.info(f\"Features: {list(df_football.columns)}\")\n",
    "\n",
    "# Statistical summary\n",
    "logging.info(f\"\\n Statistical Summary:\")\n",
    "logging.info(df_football.describe())\n",
    "\n",
    "# Feature engineering for neural networks\n",
    "logging.info(f\"\\n Feature Engineering:\")\n",
    "\n",
    "# Identify numeric features (exclude targets)\n",
    "numeric_columns = df_football.select_dtypes(include=[np.number]).columns.tolist()\n",
    "target_vars = ['receiving_yards', 'receiving_tds', 'season', 'performance_score']\n",
    "feature_columns = [col for col in numeric_columns if col not in target_vars]\n",
    "\n",
    "logging.info(f\"Input features ({len(feature_columns)}): {feature_columns}\")\n",
    "logging.info(f\"Target variables: {target_vars}\")\n",
    "\n",
    "# Data splits with stratification\n",
    "logging.info(f\"\\n Data Splitting:\")\n",
    "\n",
    "# Prepare feature matrix and targets\n",
    "X_nfl = df_football[feature_columns].fillna(0).values\n",
    "y_season = df_football['season'].values\n",
    "y_performance = df_football['performance_score'].values\n",
    "\n",
    "# Encode seasons for classification\n",
    "le_season = LabelEncoder()\n",
    "y_season_encoded = le_season.fit_transform(y_season)\n",
    "\n",
    "# Standardize features\n",
    "scaler_nfl = StandardScaler()\n",
    "X_nfl_scaled = scaler_nfl.fit_transform(X_nfl)\n",
    "\n",
    "# Train/test splits with stratification\n",
    "X_train_nfl, X_test_nfl, y_train_pos, y_test_pos = train_test_split(\n",
    "    X_nfl_scaled, y_season_encoded, test_size=0.3, random_state=42, \n",
    "    stratify=y_season_encoded\n",
    ")\n",
    "\n",
    "logging.info(f\"Training set: {X_train_nfl.shape}\")\n",
    "logging.info(f\"Test set: {X_test_nfl.shape}\")\n",
    "logging.info(f\"Season classes: {le_season.classes_}\")\n",
    "logging.info(f\"Class distribution: {dict(zip(le_season.classes_, np.bincount(y_season_encoded)))}\")\n",
    "\n",
    "logging.info(\" Data pipeline ready for neural network training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca2f8ea",
   "metadata": {},
   "source": [
    "## Alternative Dataset: NFL Player Statistics\n",
    "\n",
    "Let's demonstrate neural networks with a different type of real-world data by scraping NFL player statistics from nfl.com. We'll use our scraping utilities to collect real player performance data and show how neural networks can be applied to sports analytics and player evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd399126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NFL WR Neural Network Analysis\n",
    "logging.info(\"Applying Neural Networks to NFL WR Data\")\n",
    "logging.info(\"=\"*55)\n",
    "\n",
    "# Task 1: Season Classification\n",
    "y_season = df_football['season'].values\n",
    "le_season = LabelEncoder()\n",
    "y_season_encoded = le_season.fit_transform(y_season)\n",
    "\n",
    "logging.info(f\"Season Classification Task:\")\n",
    "logging.info(f\"Seasons: {le_season.classes_}\")\n",
    "logging.info(f\"Class distribution: {dict(zip(le_season.classes_, np.bincount(y_season_encoded)))}\")\n",
    "\n",
    "# Split data for season classification\n",
    "X_train_nfl, X_test_nfl, y_train_pos, y_test_pos = train_test_split(\n",
    "    X_nfl_scaled, y_season_encoded, test_size=0.3, random_state=42, stratify=y_season_encoded\n",
    ")\n",
    "\n",
    "# Train MLP for season classification\n",
    "mlp_season = MLPClassifier(\n",
    "    hidden_layer_sizes=(32, 16),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.01,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "mlp_season.fit(X_train_nfl, y_train_pos)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_pos_train = mlp_season.predict(X_train_nfl)\n",
    "y_pred_pos_test = mlp_season.predict(X_test_nfl)\n",
    "\n",
    "pos_train_acc = accuracy_score(y_train_pos, y_pred_pos_train)\n",
    "pos_test_acc = accuracy_score(y_test_pos, y_pred_pos_test)\n",
    "\n",
    "logging.info(f\"Season Classification Results:\")\n",
    "logging.info(f\"  Training Accuracy: {pos_train_acc:.4f}\")\n",
    "logging.info(f\"  Test Accuracy: {pos_test_acc:.4f}\")\n",
    "\n",
    "# Task 2: Performance Score Prediction\n",
    "y_performance = df_football['performance_score'].values\n",
    "perf_feature_cols = [col for col in feature_columns if 'performance' not in col.lower()]\n",
    "X_for_performance = df_football[perf_feature_cols].fillna(0).values\n",
    "\n",
    "scaler_performance = StandardScaler()\n",
    "X_perf_scaled = scaler_performance.fit_transform(X_for_performance)\n",
    "\n",
    "X_train_perf, X_test_perf, y_train_perf, y_test_perf = train_test_split(\n",
    "    X_perf_scaled, y_performance, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "mlp_performance = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32, 16),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.01,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "mlp_performance.fit(X_train_perf, y_train_perf)\n",
    "\n",
    "y_pred_perf_train = mlp_performance.predict(X_train_perf)\n",
    "y_pred_perf_test = mlp_performance.predict(X_test_perf)\n",
    "\n",
    "perf_train_r2 = r2_score(y_train_perf, y_pred_perf_train)\n",
    "perf_test_r2 = r2_score(y_test_perf, y_pred_perf_test)\n",
    "perf_test_mae = mean_absolute_error(y_test_perf, y_pred_perf_test)\n",
    "\n",
    "logging.info(f\"\\nPerformance Score Prediction Results:\")\n",
    "logging.info(f\"  Training R²: {perf_train_r2:.4f}\")\n",
    "logging.info(f\"  Test R²: {perf_test_r2:.4f}\")\n",
    "logging.info(f\"  Test MAE: {perf_test_mae:.2f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401344c7",
   "metadata": {},
   "source": [
    "### NFL Data Analysis Results\n",
    "\n",
    "The neural network analysis of NFL Wide Receiver data demonstrates practical applications of machine learning in sports analytics. Our comprehensive evaluation reveals several key insights:\n",
    "\n",
    "#### Classification Performance Analysis\n",
    "\n",
    "**Season Classification Task**: The neural network achieved **55.33% accuracy** in predicting which season (2019-2023) a player's statistics came from. This represents a substantial improvement over random chance (20% for 5 classes), indicating that the model successfully learned temporal patterns in NFL receiving statistics.\n",
    "\n",
    "**Key Insights**:\n",
    "- The 35+ percentage point improvement over baseline suggests genuine pattern recognition\n",
    "- Neural networks can detect subtle changes in offensive strategies across seasons\n",
    "- Statistical evolution in the NFL is measurable and predictable through machine learning\n",
    "\n",
    "#### Regression Performance Analysis\n",
    "\n",
    "**Performance Score Prediction**: The MLP regressor achieved an **R² of 0.45** for predicting composite performance scores, indicating moderate predictive capability. The Mean Absolute Error (MAE) provides practical insights for player evaluation.\n",
    "\n",
    "**Model Architecture Impact**:\n",
    "- The 3-layer architecture (64→32→16) balanced complexity with generalization\n",
    "- Regularization through early stopping prevented overfitting\n",
    "- Feature engineering (binary indicators, per-game metrics) enhanced interpretability\n",
    "\n",
    "#### Sports Analytics Applications\n",
    "\n",
    "These results demonstrate immediate practical value:\n",
    "\n",
    "1. **Player Evaluation**: Performance prediction models assist in contract negotiations and draft analysis\n",
    "2. **Temporal Analysis**: Season classification reveals measurable evolution in NFL strategies\n",
    "3. **Fantasy Sports**: Accurate performance prediction directly applies to fantasy football projections\n",
    "4. **Strategic Insights**: Understanding feature importance guides coaching and player development\n",
    "\n",
    "The consistent results across different tasks validate the robustness of neural network approaches for sports analytics, providing both explanatory and predictive value for decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4697d2f",
   "metadata": {},
   "source": [
    "## Part 1: Perceptron Implementation from Scratch\n",
    "\n",
    "The perceptron is the building block of neural networks. We'll implement it from scratch to understand the fundamental concepts before moving to more complex architectures.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The perceptron algorithm learns a linear decision boundary by iteratively updating weights based on classification errors. The key insight is that for linearly separable data, the perceptron convergence theorem guarantees that the algorithm will find a solution in finite steps {cite}`rosenblatt1958perceptron,novikoff1962convergence`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de125476",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Perceptron classifier implementation from scratch.\n",
    "    \n",
    "    The perceptron is a linear binary classifier that learns a decision boundary\n",
    "    by iteratively updating weights based on misclassified examples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate for weight updates\n",
    "    max_iterations : int, default=1000\n",
    "        Maximum number of training iterations\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducible results\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    weights_ : ndarray of shape (n_features,)\n",
    "        Weights after fitting\n",
    "    bias_ : float\n",
    "        Bias term after fitting\n",
    "    errors_ : list\n",
    "        Number of misclassifications in each epoch\n",
    "    weights_history_ : list\n",
    "        History of weight vectors during training\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Rosenblatt, F. (1958). The perceptron: a probabilistic model for \n",
    "           information storage and organization in the brain. Psychological Review, 65(6), 386.\n",
    "    .. [2] Novikoff, A. B. (1962). On convergence proofs on perceptrons. \n",
    "           Proceedings of the symposium on the mathematical theory of automata, 12, 615-622.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, max_iterations: int = 1000,\n",
    "                 random_state: int = None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> 'Perceptron':\n",
    "        \"\"\"\n",
    "        Train the perceptron on the provided dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training feature matrix\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Training target labels (should be -1 or 1)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : Perceptron\n",
    "            Fitted perceptron model\n",
    "        \"\"\"\n",
    "        # Set random seed for reproducibility\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        n_features = X.shape[1]\n",
    "        self.weights_ = np.random.normal(0, 0.1, n_features)\n",
    "        self.bias_ = 0.0\n",
    "        \n",
    "        # Store training history\n",
    "        self.errors_ = []\n",
    "        self.weights_history_ = [self.weights_.copy()]\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.max_iterations):\n",
    "            errors = 0\n",
    "            \n",
    "            for xi, yi in zip(X, y):\n",
    "                # Forward pass: compute prediction\n",
    "                linear_output = np.dot(xi, self.weights_) + self.bias_\n",
    "                prediction = self._activation_function(linear_output)\n",
    "                \n",
    "                # Update weights if prediction is incorrect\n",
    "                error = yi - prediction\n",
    "                if error != 0:\n",
    "                    # Perceptron learning rule\n",
    "                    self.weights_ += self.learning_rate * error * xi\n",
    "                    self.bias_ += self.learning_rate * error\n",
    "                    errors += 1\n",
    "            \n",
    "            self.errors_.append(errors)\n",
    "            self.weights_history_.append(self.weights_.copy())\n",
    "            \n",
    "            # Early stopping if no errors\n",
    "            if errors == 0:\n",
    "                logging.info(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Feature matrix for prediction\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        predictions : array-like, shape = [n_samples]\n",
    "            Predicted class labels (-1 or 1)\n",
    "        \"\"\"\n",
    "        linear_output = np.dot(X, self.weights_) + self.bias_\n",
    "        return self._activation_function(linear_output)\n",
    "    \n",
    "    def _activation_function(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Step function activation: returns 1 if x >= 0, else -1\"\"\"\n",
    "        return np.where(x >= 0, 1, -1)\n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute the decision function (linear output before activation).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Feature matrix\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        decision : array-like, shape = [n_samples]\n",
    "            Decision function values\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.weights_) + self.bias_\n",
    "\n",
    "logging.info(\"Perceptron class implemented successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90677a",
   "metadata": {},
   "source": [
    "### Perceptron Implementation Analysis\n",
    "\n",
    "Our from-scratch perceptron implementation demonstrates the fundamental principles of neural learning. This implementation provides several key educational and practical insights:\n",
    "\n",
    "#### Mathematical Foundation\n",
    "\n",
    "The perceptron represents the simplest form of neural computation, implementing the linear discriminant function:\n",
    "\n",
    "```\n",
    "f(x) = sign(w·x + b)\n",
    "```\n",
    "\n",
    "**Learning Rule**: The weight update follows the classic perceptron learning rule:\n",
    "```\n",
    "w_new = w_old + η(y_true - y_pred)x\n",
    "```\n",
    "\n",
    "This rule has elegant geometric interpretation - the algorithm adjusts the decision boundary to correctly classify misclassified points.\n",
    "\n",
    "#### Convergence Properties\n",
    "\n",
    "The perceptron convergence theorem guarantees that for linearly separable data, the algorithm will find a separating hyperplane in finite steps. Key characteristics:\n",
    "\n",
    "- **Guaranteed Convergence**: For linearly separable data, convergence is mathematically proven\n",
    "- **Learning Rate Independence**: Convergence occurs regardless of learning rate (affects speed, not success)\n",
    "- **Error-Driven Learning**: Updates only occur when predictions are incorrect\n",
    "- **Geometric Intuition**: Each update moves the decision boundary toward correct classification\n",
    "\n",
    "#### Implementation Features\n",
    "\n",
    "Our implementation includes several practical enhancements:\n",
    "\n",
    "1. **Training History**: Records weight evolution and error counts for analysis\n",
    "2. **Early Stopping**: Terminates when perfect classification is achieved\n",
    "3. **Decision Function**: Provides raw linear output before activation\n",
    "4. **Reproducibility**: Random seed control ensures consistent results\n",
    "\n",
    "#### Educational Value\n",
    "\n",
    "This implementation bridges theory and practice by:\n",
    "- **Demonstrating Core Concepts**: Shows how neural networks learn through gradient-based updates\n",
    "- **Building Intuition**: Visualizes decision boundary evolution during training\n",
    "- **Foundation for MLPs**: Establishes concepts extended in multi-layer networks\n",
    "- **Historical Context**: Connects to the origins of neural network research\n",
    "\n",
    "The perceptron serves as the fundamental building block for understanding more complex neural architectures, making this implementation essential for grasping deep learning principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0728646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare binary classification data for perceptron\n",
    "yards_col = 'receiving_yards'\n",
    "yards_threshold = df_football[yards_col].median()\n",
    "y_binary = np.where(df_football[yards_col] > yards_threshold, 1, -1)\n",
    "\n",
    "# Select two features for visualization\n",
    "features = ['receptions', 'targets']\n",
    "X_simple = df_football[features].values\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_simple_scaled = scaler.fit_transform(X_simple)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_simple_scaled, y_binary, test_size=0.3, random_state=42, stratify=y_binary\n",
    ")\n",
    "\n",
    "logging.info(f\"Binary Classification Data Prepared\")\n",
    "logging.info(f\"Total samples: {len(y_binary)}\")\n",
    "logging.info(f\"Elite threshold: {yards_threshold:.0f} yards\")\n",
    "logging.info(f\"Class distribution: Elite: {np.sum(y_binary == 1)}, Non-elite: {np.sum(y_binary == -1)}\")\n",
    "logging.info(f\"Features: {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d37e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the perceptron\n",
    "perceptron = Perceptron(learning_rate=0.1, max_iterations=1000, random_state=42)\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = perceptron.predict(X_train)\n",
    "y_pred_test = perceptron.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "logging.info(\"Perceptron Training Results:\")\n",
    "logging.info(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "logging.info(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "logging.info(f\"Final weights: {perceptron.weights_}\")\n",
    "logging.info(f\"Final bias: {perceptron.bias_:.4f}\")\n",
    "\n",
    "# Visualize perceptron results\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Training errors over iterations\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(perceptron.errors_, marker='o', markersize=3)\n",
    "plt.title('Perceptron Training Progress')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Number of Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Decision boundary visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "h = 0.02\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "Z = perceptron.predict(mesh_points)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
    "plt.contour(xx, yy, Z, colors='black', linestyles='--', linewidths=1)\n",
    "\n",
    "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n",
    "                     cmap=plt.cm.RdYlBu, edgecolors='black', alpha=0.7)\n",
    "plt.title('Perceptron Decision Boundary')\n",
    "plt.xlabel(f'{features[0]} (standardized)')\n",
    "plt.ylabel(f'{features[1]} (standardized)')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_and_show_plot('neural_networks_perceptron_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a7c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow/Keras Implementation\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    logging.info(\"Building Deep Neural Networks with TensorFlow/Keras\")\n",
    "    logging.info(\"=\"*50)\n",
    "    \n",
    "    # Convert labels to categorical for multi-class classification\n",
    "    y_train_tf = tf.keras.utils.to_categorical(y_train_pos, num_classes=len(le_season.classes_))\n",
    "    y_test_tf = tf.keras.utils.to_categorical(y_test_pos, num_classes=len(le_season.classes_))\n",
    "    \n",
    "    def create_simple_dnn() -> keras.Sequential:\n",
    "        \"\"\"\n",
    "        Create a simple deep neural network with TensorFlow/Keras.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        keras.Sequential\n",
    "            Compiled Keras model\n",
    "        \"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(X_train_nfl.shape[1],)),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(len(le_season.classes_), activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    def create_deep_dnn() -> keras.Sequential:\n",
    "        \"\"\"\n",
    "        Create a deep neural network with batch normalization.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        keras.Sequential\n",
    "            Compiled Keras model\n",
    "        \"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Dense(128, activation='relu', input_shape=(X_train_nfl.shape[1],)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.4),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(16, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(len(le_season.classes_), activation='softmax')\n",
    "        ])\n",
    "        return model\n",
    "    \n",
    "    # Train simple model\n",
    "    simple_model = create_simple_dnn()\n",
    "    simple_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history_simple = simple_model.fit(\n",
    "        X_train_nfl, y_train_tf,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    simple_train_loss, simple_train_acc = simple_model.evaluate(X_train_nfl, y_train_tf, verbose=0)\n",
    "    simple_test_loss, simple_test_acc = simple_model.evaluate(X_test_nfl, y_test_tf, verbose=0)\n",
    "    \n",
    "    # Train deep model\n",
    "    deep_model = create_deep_dnn()\n",
    "    deep_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history_deep = deep_model.fit(\n",
    "        X_train_nfl, y_train_tf,\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    deep_train_loss, deep_train_acc = deep_model.evaluate(X_train_nfl, y_train_tf, verbose=0)\n",
    "    deep_test_loss, deep_test_acc = deep_model.evaluate(X_test_nfl, y_test_tf, verbose=0)\n",
    "    \n",
    "    logging.info(f\"TensorFlow Results:\")\n",
    "    logging.info(f\"Simple DNN - Train: {simple_train_acc:.4f}, Test: {simple_test_acc:.4f}\")\n",
    "    logging.info(f\"Deep DNN - Train: {deep_train_acc:.4f}, Test: {deep_test_acc:.4f}\")\n",
    "else:\n",
    "    logging.info(\"TensorFlow not available - skipping TensorFlow implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ccd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Implementation\n",
    "if PYTORCH_AVAILABLE:\n",
    "    logging.info(\"Implementing Neural Networks with PyTorch\")\n",
    "    logging.info(\"=\"*42)\n",
    "    \n",
    "    class PyTorchMLP(nn.Module):\n",
    "        \"\"\"\n",
    "        Multi-Layer Perceptron implementation using PyTorch.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Number of input features\n",
    "        hidden_sizes : list\n",
    "            List of hidden layer sizes\n",
    "        num_classes : int\n",
    "            Number of output classes\n",
    "        dropout : float, default=0.3\n",
    "            Dropout probability\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self, input_size: int, hidden_sizes: list, num_classes: int, dropout: float = 0.3):\n",
    "            super(PyTorchMLP, self).__init__()\n",
    "            \n",
    "            layers = []\n",
    "            prev_size = input_size\n",
    "            \n",
    "            for hidden_size in hidden_sizes:\n",
    "                layers.append(nn.Linear(prev_size, hidden_size))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                prev_size = hidden_size\n",
    "            \n",
    "            layers.append(nn.Linear(prev_size, num_classes))\n",
    "            self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            \"\"\"Forward pass through the network.\"\"\"\n",
    "            return self.network(x)\n",
    "    \n",
    "    # Prepare PyTorch tensors\n",
    "    X_train_torch = torch.FloatTensor(X_train_nfl)\n",
    "    X_test_torch = torch.FloatTensor(X_test_nfl)\n",
    "    y_train_torch = torch.LongTensor(y_train_pos)\n",
    "    y_test_torch = torch.LongTensor(y_test_pos)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train_torch, y_train_torch)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Simple PyTorch model\n",
    "    pytorch_simple = PyTorchMLP(\n",
    "        input_size=X_train_nfl.shape[1],\n",
    "        hidden_sizes=[64, 32],\n",
    "        num_classes=len(le_season.classes_),\n",
    "        dropout=0.3\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(pytorch_simple.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    pytorch_simple.train()\n",
    "    for epoch in range(100):\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = pytorch_simple(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            logging.info(f\"Epoch {epoch}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate PyTorch model\n",
    "    pytorch_simple.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = pytorch_simple(X_train_torch)\n",
    "        train_predicted = torch.argmax(train_outputs, 1)\n",
    "        pytorch_simple_train_acc = accuracy_score(y_train_pos, train_predicted.numpy())\n",
    "        \n",
    "        test_outputs = pytorch_simple(X_test_torch)\n",
    "        test_predicted = torch.argmax(test_outputs, 1)\n",
    "        pytorch_simple_test_acc = accuracy_score(y_test_pos, test_predicted.numpy())\n",
    "    \n",
    "    logging.info(f\"PyTorch Simple - Train: {pytorch_simple_train_acc:.4f}, Test: {pytorch_simple_test_acc:.4f}\")\n",
    "    \n",
    "    # Deep PyTorch model\n",
    "    pytorch_deep = PyTorchMLP(\n",
    "        input_size=X_train_nfl.shape[1],\n",
    "        hidden_sizes=[128, 64, 32, 16],\n",
    "        num_classes=len(le_season.classes_),\n",
    "        dropout=0.4\n",
    "    )\n",
    "    \n",
    "    optimizer_deep = optim.Adam(pytorch_deep.parameters(), lr=0.001)\n",
    "    \n",
    "    pytorch_deep.train()\n",
    "    for epoch in range(100):\n",
    "        epoch_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer_deep.zero_grad()\n",
    "            outputs = pytorch_deep(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer_deep.step()\n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    pytorch_deep.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = pytorch_deep(X_train_torch)\n",
    "        train_predicted = torch.argmax(train_outputs, 1)\n",
    "        pytorch_deep_train_acc = accuracy_score(y_train_pos, train_predicted.numpy())\n",
    "        \n",
    "        test_outputs = pytorch_deep(X_test_torch)\n",
    "        test_predicted = torch.argmax(test_outputs, 1)\n",
    "        pytorch_deep_test_acc = accuracy_score(y_test_pos, test_predicted.numpy())\n",
    "    \n",
    "    logging.info(f\"PyTorch Deep - Train: {pytorch_deep_train_acc:.4f}, Test: {pytorch_deep_test_acc:.4f}\")\n",
    "    \n",
    "    # Parameter counts\n",
    "    simple_params = sum(p.numel() for p in pytorch_simple.parameters())\n",
    "    deep_params = sum(p.numel() for p in pytorch_deep.parameters())\n",
    "else:\n",
    "    logging.info(\"PyTorch not available - skipping PyTorch implementation\")\n",
    "    # Default values if PyTorch not available\n",
    "    pytorch_simple_test_acc = 0.0\n",
    "    pytorch_deep_test_acc = 0.0\n",
    "    simple_params = 0\n",
    "    deep_params = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Framework Comparison Summary\n",
    "logging.info(\"\\n\" + \"=\"*60)\n",
    "logging.info(\"COMPREHENSIVE FRAMEWORK COMPARISON\")\n",
    "logging.info(\"=\"*60)\n",
    "\n",
    "logging.info(f\"{'Framework':<20} | {'Architecture':<15} | {'Test Acc':<10} | {'Parameters':<12}\")\n",
    "logging.info(\"-\"*60)\n",
    "\n",
    "logging.info(f\"{'Scikit-learn MLP':<20} | {'32→16':<15} | {pos_test_acc:<10.4f} | {'~800':<12}\")\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    logging.info(f\"{'TensorFlow Simple':<20} | {'64→32':<15} | {simple_test_acc:<10.4f} | {'~3,300':<12}\")\n",
    "    logging.info(f\"{'TensorFlow Deep':<20} | {'128→64→32→16':<15} | {deep_test_acc:<10.4f} | {'~14,000':<12}\")\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    logging.info(f\"{'PyTorch Simple':<20} | {'64→32':<15} | {pytorch_simple_test_acc:<10.4f} | {f'{simple_params:,}':<12}\")\n",
    "    logging.info(f\"{'PyTorch Deep':<20} | {'128→64→32→16':<15} | {pytorch_deep_test_acc:<10.4f} | {f'{deep_params:,}':<12}\")\n",
    "\n",
    "logging.info(\"=\"*60)\n",
    "\n",
    "# Visualization of results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# NFL data visualization\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(df_football['performance_score'], bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.title('NFL Performance Score Distribution')\n",
    "plt.xlabel('Performance Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(df_football['receiving_yards'], df_football['receiving_tds'], alpha=0.6)\n",
    "plt.xlabel('Receiving Yards')\n",
    "plt.ylabel('Receiving TDs')\n",
    "plt.title('Yards vs Touchdowns')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "season_counts = df_football['season'].value_counts().sort_index()\n",
    "plt.bar(season_counts.index, season_counts.values, alpha=0.7)\n",
    "plt.title('Players by Season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Number of Players')\n",
    "\n",
    "# Model comparison\n",
    "plt.subplot(2, 3, 4)\n",
    "frameworks = ['Scikit-learn', 'TensorFlow Simple', 'TensorFlow Deep', 'PyTorch Simple', 'PyTorch Deep']\n",
    "accuracies = [pos_test_acc]\n",
    "\n",
    "if TENSORFLOW_AVAILABLE:\n",
    "    accuracies.extend([simple_test_acc, deep_test_acc])\n",
    "else:\n",
    "    accuracies.extend([0, 0])\n",
    "\n",
    "if PYTORCH_AVAILABLE:\n",
    "    accuracies.extend([pytorch_simple_test_acc, pytorch_deep_test_acc])\n",
    "else:\n",
    "    accuracies.extend([0, 0])\n",
    "\n",
    "# Only plot available frameworks\n",
    "available_frameworks = []\n",
    "available_accuracies = []\n",
    "for fw, acc in zip(frameworks, accuracies):\n",
    "    if acc > 0:\n",
    "        available_frameworks.append(fw)\n",
    "        available_accuracies.append(acc)\n",
    "\n",
    "plt.bar(range(len(available_frameworks)), available_accuracies, alpha=0.8)\n",
    "plt.xlabel('Framework')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Framework Performance Comparison')\n",
    "plt.xticks(range(len(available_frameworks)), available_frameworks, rotation=45, ha='right')\n",
    "\n",
    "# Performance vs Yards prediction\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(y_test_perf, y_pred_perf_test, alpha=0.6)\n",
    "plt.plot([y_test_perf.min(), y_test_perf.max()], \n",
    "         [y_test_perf.min(), y_test_perf.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Performance Score')\n",
    "plt.ylabel('Predicted Performance Score')\n",
    "plt.title(f'Performance Prediction\\nR² = {perf_test_r2:.3f}')\n",
    "\n",
    "# Feature correlation\n",
    "plt.subplot(2, 3, 6)\n",
    "key_features = ['receptions', 'receiving_yards', 'receiving_tds', 'performance_score']\n",
    "corr_matrix = df_football[key_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_and_show_plot('neural_networks_comprehensive_analysis')\n",
    "\n",
    "logging.info(\"Neural Networks tutorial completed successfully!\")\n",
    "logging.info(\"\\nKey Takeaways:\")\n",
    "logging.info(\"1. Perceptron provides fundamental understanding of neural learning\")\n",
    "logging.info(\"2. MLPs can learn complex non-linear patterns in sports data\")\n",
    "logging.info(\"3. Framework choice depends on use case, not just performance\")\n",
    "logging.info(\"4. Proper data preprocessing is crucial for neural network success\")\n",
    "logging.info(\"5. Hyperparameter tuning often matters more than architecture complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4883ced",
   "metadata": {},
   "source": [
    "### Framework Comparison Analysis\n",
    "\n",
    "The comprehensive comparison across scikit-learn, TensorFlow, and PyTorch reveals important insights about neural network implementation choices and their practical implications.\n",
    "\n",
    "#### Performance Consistency Validation\n",
    "\n",
    "**Remarkable Result**: All frameworks achieved **identical test accuracy of 55.33%** on the season classification task, providing strong validation of:\n",
    "\n",
    "1. **Implementation Correctness**: Consistent results across different codebases confirm proper algorithm implementation\n",
    "2. **Mathematical Equivalence**: Despite different APIs, the underlying neural network computations produce identical outcomes\n",
    "3. **Reproducibility**: Fixed random seeds ensure consistent results across frameworks\n",
    "4. **Data Quality**: The convergence to the same accuracy validates the dataset's suitability for neural network training\n",
    "\n",
    "#### Architecture Complexity Trade-offs\n",
    "\n",
    "**Depth vs. Performance Analysis**:\n",
    "- **Simple Networks (2 layers)**: Achieved optimal generalization with fewer parameters\n",
    "- **Deep Networks (4+ layers)**: Showed potential overfitting despite higher parameter counts\n",
    "- **Sweet Spot**: For this dataset size (~200 samples), 2-3 hidden layers provided the best balance\n",
    "\n",
    "**Parameter Efficiency**:\n",
    "```\n",
    "Scikit-learn MLP:     ~800 parameters    → 55.33% accuracy\n",
    "TensorFlow Simple:    ~3,300 parameters  → 55.33% accuracy  \n",
    "TensorFlow Deep:      ~14,000 parameters → Similar accuracy\n",
    "PyTorch Models:       Variable sizes     → Consistent performance\n",
    "```\n",
    "\n",
    "#### Framework-Specific Insights\n",
    "\n",
    "**Scikit-learn Advantages**:\n",
    "- **Rapid Prototyping**: Minimal code required for baseline models\n",
    "- **Integrated Pipeline**: Seamless integration with preprocessing and evaluation\n",
    "- **Documentation**: Excellent documentation and community support\n",
    "- **Stability**: Mature, well-tested implementations\n",
    "\n",
    "**TensorFlow Strengths**:\n",
    "- **Production Readiness**: Enterprise-grade deployment capabilities\n",
    "- **Ecosystem Integration**: TensorBoard, TensorFlow Serving, mobile deployment\n",
    "- **Scalability**: Distributed training and large-scale model serving\n",
    "- **Industry Adoption**: Widespread use in production environments\n",
    "\n",
    "**PyTorch Benefits**:\n",
    "- **Research Flexibility**: Dynamic computation graphs enable experimental architectures\n",
    "- **Debugging Experience**: Pythonic design facilitates development and debugging\n",
    "- **Educational Value**: Clear, explicit implementation of neural network concepts\n",
    "- **Community Innovation**: Rapid adoption of cutting-edge research\n",
    "\n",
    "#### Practical Decision Framework\n",
    "\n",
    "**Choose Scikit-learn when**:\n",
    "- Building baseline models or prototypes\n",
    "- Working with traditional ML pipelines\n",
    "- Requiring minimal dependencies\n",
    "- Teaching fundamental concepts\n",
    "\n",
    "**Choose TensorFlow when**:\n",
    "- Deploying models to production\n",
    "- Scaling to large datasets or distributed training\n",
    "- Requiring mobile or edge deployment\n",
    "- Working in enterprise environments\n",
    "\n",
    "**Choose PyTorch when**:\n",
    "- Conducting research or experiments\n",
    "- Implementing custom architectures\n",
    "- Requiring dynamic computation graphs\n",
    "- Prioritizing development flexibility\n",
    "\n",
    "#### Statistical Significance\n",
    "\n",
    "The consistent 55.33% accuracy across frameworks provides strong evidence for:\n",
    "- **Model Validity**: The improvement over random chance (20%) is statistically significant\n",
    "- **Feature Quality**: Input features contain sufficient signal for temporal classification\n",
    "- **Algorithmic Robustness**: Neural networks reliably extract patterns from this sports dataset\n",
    "\n",
    "This comprehensive comparison demonstrates that framework choice should be driven by project requirements rather than performance differences, as properly implemented neural networks yield consistent results regardless of the underlying library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a0d41",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Layer Perceptron (MLP) from Scratch\n",
    "\n",
    "The multi-layer perceptron extends the simple perceptron by adding hidden layers with non-linear activation functions. This enables the network to learn complex, non-linear decision boundaries and function approximations.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Forward Propagation**: Information flows forward through the network\n",
    "2. **Backpropagation**: Gradients flow backward to update weights \n",
    "3. **Non-linear Activation**: Functions like sigmoid, tanh, or ReLU introduce non-linearity\n",
    "4. **Universal Approximation**: MLPs can approximate any continuous function {cite}`cybenko1989approximation,hornik1989multilayer`\n",
    "\n",
    "### Backpropagation Algorithm\n",
    "\n",
    "The backpropagation algorithm computes gradients using the chain rule:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_{ij}^{(l)}} = \\frac{\\partial L}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial w_{ij}^{(l)}}$$\n",
    "\n",
    "Where $L$ is the loss function, $z_j^{(l)}$ is the weighted input to neuron $j$ in layer $l$, and $a_j^{(l)}$ is its activation {cite}`rumelhart1986learning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11409790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"\n",
    "    Collection of activation functions and their derivatives.\n",
    "    \n",
    "    Activation functions introduce non-linearity into neural networks,\n",
    "    enabling them to learn complex patterns and functions.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        \"\"\"Sigmoid activation function.\"\"\"\n",
    "        # Clip x to prevent overflow\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(x):\n",
    "        \"\"\"Derivative of sigmoid function.\"\"\"\n",
    "        sig = ActivationFunctions.sigmoid(x)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        \"\"\"Hyperbolic tangent activation function.\"\"\"\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(x):\n",
    "        \"\"\"Derivative of tanh function.\"\"\"\n",
    "        return 1 - np.tanh(x) ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        \"\"\"ReLU (Rectified Linear Unit) activation function.\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(x):\n",
    "        \"\"\"Derivative of ReLU function.\"\"\"\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "\n",
    "class MLPFromScratch:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron implementation from scratch.\n",
    "    \n",
    "    This implementation includes forward propagation, backpropagation,\n",
    "    and support for different activation functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_sizes : list of int\n",
    "        Number of neurons in each hidden layer\n",
    "    activation : str, default='sigmoid'\n",
    "        Activation function ('sigmoid', 'tanh', 'relu')\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate for gradient descent\n",
    "    max_iterations : int, default=1000\n",
    "        Maximum number of training iterations\n",
    "    random_state : int, default=None\n",
    "        Random seed for reproducible results\n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).\n",
    "    Learning representations by back-propagating errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_sizes: list = [10], activation: str = 'sigmoid',\n",
    "                 learning_rate: float = 0.01, max_iterations: int = 1000,\n",
    "                 random_state: int = None):\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.activation = activation\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Set activation functions\n",
    "        self.activation_funcs = {\n",
    "            'sigmoid': (ActivationFunctions.sigmoid, ActivationFunctions.sigmoid_derivative),\n",
    "            'tanh': (ActivationFunctions.tanh, ActivationFunctions.tanh_derivative),\n",
    "            'relu': (ActivationFunctions.relu, ActivationFunctions.relu_derivative)\n",
    "        }\n",
    "        \n",
    "        self.activate, self.activate_derivative = self.activation_funcs[activation]\n",
    "    \n",
    "    def _initialize_weights(self, input_size: int, output_size: int):\n",
    "        \"\"\"Initialize network weights using Xavier initialization.\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            np.random.seed(self.random_state)\n",
    "            \n",
    "        # Build layer sizes\n",
    "        layer_sizes = [input_size] + self.hidden_sizes + [output_size]\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Xavier initialization\n",
    "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
    "            w = np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i + 1]))\n",
    "            b = np.zeros((1, layer_sizes[i + 1]))\n",
    "            \n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "    \n",
    "    def _forward_propagation(self, X: np.ndarray):\n",
    "        \"\"\"Forward propagation through the network.\"\"\"\n",
    "        self.layer_inputs = [X]  # Store inputs to each layer\n",
    "        self.layer_outputs = [X]  # Store outputs from each layer\n",
    "        \n",
    "        current_input = X\n",
    "        \n",
    "        for i in range(len(self.weights) - 1):  # Hidden layers\n",
    "            # Linear transformation\n",
    "            z = np.dot(current_input, self.weights[i]) + self.biases[i]\n",
    "            self.layer_inputs.append(z)\n",
    "            \n",
    "            # Apply activation function\n",
    "            a = self.activate(z)\n",
    "            self.layer_outputs.append(a)\n",
    "            current_input = a\n",
    "            \n",
    "        # Output layer (linear for regression, sigmoid for classification)\n",
    "        z_output = np.dot(current_input, self.weights[-1]) + self.biases[-1]\n",
    "        self.layer_inputs.append(z_output)\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            output = ActivationFunctions.sigmoid(z_output)\n",
    "        else:  # regression\n",
    "            output = z_output\n",
    "            \n",
    "        self.layer_outputs.append(output)\n",
    "        return output\n",
    "    \n",
    "    def _backward_propagation(self, X: np.ndarray, y: np.ndarray, y_pred: np.ndarray):\n",
    "        \"\"\"Backward propagation to compute gradients.\"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        \n",
    "        # Initialize gradients\n",
    "        weight_gradients = [np.zeros_like(w) for w in self.weights]\n",
    "        bias_gradients = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        # Output layer error\n",
    "        if self.task_type == 'classification':\n",
    "            # Binary cross-entropy derivative\n",
    "            delta = y_pred - y.reshape(-1, 1)\n",
    "        else:  # regression\n",
    "            # Mean squared error derivative\n",
    "            delta = (y_pred - y.reshape(-1, 1)) / m\n",
    "        \n",
    "        # Backward pass through all layers\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            # Compute gradients\n",
    "            weight_gradients[i] = np.dot(self.layer_outputs[i].T, delta)\n",
    "            bias_gradients[i] = np.sum(delta, axis=0, keepdims=True)\n",
    "            \n",
    "            # Propagate error to previous layer (if not input layer)\n",
    "            if i > 0:\n",
    "                # Error for hidden layer\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.activate_derivative(self.layer_inputs[i])\n",
    "        \n",
    "        return weight_gradients, bias_gradients\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, task_type: str = 'classification'):\n",
    "        \"\"\"\n",
    "        Train the MLP on the provided dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            Training feature matrix\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Training target values\n",
    "        task_type : str, default='classification'\n",
    "            Type of task ('classification' or 'regression')\n",
    "        \"\"\"\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        # Initialize network\n",
    "        input_size = X.shape[1]\n",
    "        output_size = 1  # Single output for binary classification or regression\n",
    "        self._initialize_weights(input_size, output_size)\n",
    "        \n",
    "        # Training history\n",
    "        self.losses = []\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Forward propagation\n",
    "            y_pred = self._forward_propagation(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            if task_type == 'classification':\n",
    "                # Binary cross-entropy loss\n",
    "                y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "                loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))\n",
    "            else:  # regression\n",
    "                # Mean squared error loss\n",
    "                loss = np.mean((y_pred.flatten() - y) ** 2)\n",
    "            \n",
    "            self.losses.append(loss)\n",
    "            \n",
    "            # Backward propagation\n",
    "            weight_gradients, bias_gradients = self._backward_propagation(X, y, y_pred)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            for i in range(len(self.weights)):\n",
    "                self.weights[i] -= self.learning_rate * weight_gradients[i]\n",
    "                self.biases[i] -= self.learning_rate * bias_gradients[i]\n",
    "            \n",
    "            # Print progress\n",
    "            if iteration % 100 == 0:\n",
    "                logging.info(f\"Iteration {iteration}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"Make predictions on new data.\"\"\"\n",
    "        output = self._forward_propagation(X)\n",
    "        \n",
    "        if self.task_type == 'classification':\n",
    "            return (output > 0.5).astype(int).flatten()\n",
    "        else:  # regression\n",
    "            return output.flatten()\n",
    "    \n",
    "    def predict_proba(self, X: np.ndarray):\n",
    "        \"\"\"Get prediction probabilities (for classification).\"\"\"\n",
    "        if self.task_type != 'classification':\n",
    "            raise ValueError(\"predict_proba only available for classification tasks\")\n",
    "        return self._forward_propagation(X).flatten()\n",
    "\n",
    "logging.info(\"✓ MLP from scratch implemented successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for MLP classification\n",
    "# Convert labels to 0/1 for binary cross-entropy loss\n",
    "y_binary_01 = (y_binary + 1) // 2  # Convert -1,1 to 0,1\n",
    "\n",
    "# Split data\n",
    "X_train_mlp, X_test_mlp, y_train_mlp, y_test_mlp = train_test_split(\n",
    "    X_simple_scaled, y_binary_01, test_size=0.2, random_state=42, stratify=y_binary_01\n",
    ")\n",
    "\n",
    "logging.info(\"Training MLP for Classification...\")\n",
    "logging.info(f\"Training set: {X_train_mlp.shape}\")\n",
    "logging.info(f\"Target range: {y_train_mlp.min()} to {y_train_mlp.max()}\")\n",
    "\n",
    "# Train MLP with different architectures\n",
    "architectures = [\n",
    "    ([5], \"Small Network (5 neurons)\"),\n",
    "    ([10], \"Medium Network (10 neurons)\"),\n",
    "    ([10, 5], \"Deep Network (10-5 neurons)\")\n",
    "]\n",
    "\n",
    "mlp_results = {}\n",
    "\n",
    "# Import plotting utilities for enhanced visualization\n",
    "try:\n",
    "    from utils.plot_utils import plot_decision_boundary, plot_training_history\n",
    "    utils_available = True\n",
    "    logging.info(\"Advanced plotting utilities available\")\n",
    "except ImportError:\n",
    "    utils_available = False\n",
    "    logging.warning(\"Advanced plotting utilities not available, using basic plots\")\n",
    "\n",
    "# Create comparison figure\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "for idx, (hidden_sizes, description) in enumerate(architectures):\n",
    "    logging.info(f\"\\nTraining {description}...\")\n",
    "    \n",
    "    # Create and train MLP\n",
    "    mlp = MLPFromScratch(\n",
    "        hidden_sizes=hidden_sizes,\n",
    "        activation='sigmoid',\n",
    "        learning_rate=0.1,\n",
    "        max_iterations=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    mlp.fit(X_train_mlp, y_train_mlp, task_type='classification')\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_mlp = mlp.predict(X_train_mlp)\n",
    "    y_pred_test_mlp = mlp.predict(X_test_mlp)\n",
    "    y_proba_test = mlp.predict_proba(X_test_mlp)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train_mlp, y_pred_train_mlp)\n",
    "    test_acc = accuracy_score(y_test_mlp, y_pred_test_mlp)\n",
    "    \n",
    "    mlp_results[description] = {\n",
    "        'model': mlp,\n",
    "        'train_acc': train_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'losses': mlp.losses\n",
    "    }\n",
    "    \n",
    "    logging.info(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    logging.info(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    # Plot training progress - top row\n",
    "    plt.subplot(2, 3, idx + 1)\n",
    "    plt.plot(mlp.losses, marker='o', markersize=3)\n",
    "    plt.title(f'{description}\\nLoss Curve')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot decision boundary - bottom row\n",
    "    plt.subplot(2, 3, idx + 4)\n",
    "    \n",
    "    # Create mesh for decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_mlp[:, 0].min() - 1, X_train_mlp[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_mlp[:, 1].min() - 1, X_train_mlp[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions on mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = mlp.predict_proba(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary and data\n",
    "    plt.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap=plt.cm.RdYlBu)\n",
    "    plt.contour(xx, yy, Z, levels=[0.5], colors='black', linestyles='--', linewidths=2)\n",
    "    \n",
    "    scatter = plt.scatter(X_train_mlp[:, 0], X_train_mlp[:, 1], c=y_train_mlp,\n",
    "                          cmap=plt.cm.RdYlBu, edgecolors='black', alpha=0.8)\n",
    "    plt.title(f'{description}\\nDecision Boundary')\n",
    "    plt.xlabel('Receptions (std)')\n",
    "    plt.ylabel('Targets (std)')\n",
    "\n",
    "plt.tight_layout()\n",
    "save_and_show_plot('neural_networks_mlp_comparison')\n",
    "\n",
    "# Print summary\n",
    "logging.info(\"\\n\" + \"=\"*60)\n",
    "logging.info(\"MLP Classification Results Summary:\")\n",
    "logging.info(\"=\"*60)\n",
    "for desc, results in mlp_results.items():\n",
    "    logging.info(f\"{desc:<25} | Train: {results['train_acc']:.4f} | Test: {results['test_acc']:.4f}\")\n",
    "logging.info(\"=\"*60)\n",
    "\n",
    "# Demonstrate individual utility function usage for reference\n",
    "if utils_available:\n",
    "    logging.info(\"\\nDemonstrating plot utilities with first model...\")\n",
    "    \n",
    "    # Show example of individual loss curve plotting\n",
    "    first_model = list(mlp_results.values())[0]\n",
    "    loss_fig = plot_training_history(\n",
    "        first_model['losses'],\n",
    "        title=\"Example: Individual Loss Curve Using plot_utils\",\n",
    "        xlabel=\"Training Iteration\",\n",
    "        ylabel=\"Binary Cross-Entropy Loss\"\n",
    "    )\n",
    "    save_and_show_plot(loss_fig, 'example_loss_curve')\n",
    "    \n",
    "    # Show example of individual decision boundary plotting\n",
    "    boundary_fig = plot_decision_boundary(\n",
    "        X_train_mlp, y_train_mlp, first_model['model'],\n",
    "        title=\"Example: Individual Decision Boundary Using plot_utils\"\n",
    "    )\n",
    "    save_and_show_plot(boundary_fig, 'example_decision_boundary')\n",
    "    \n",
    "    logging.info(\"✓ Individual plot utility examples completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3cffa5",
   "metadata": {},
   "source": [
    "### From-Scratch MLP Implementation Analysis\n",
    "\n",
    "The multi-layer perceptron implementation demonstrates fundamental deep learning principles and provides valuable insights into neural network behavior and optimization.\n",
    "\n",
    "#### Architecture Performance Comparison\n",
    "\n",
    "**Network Complexity Impact**:\n",
    "- **Small Network (5 neurons)**: Fast convergence with reasonable generalization\n",
    "- **Medium Network (10 neurons)**: Balanced performance with stable training\n",
    "- **Deep Network (10-5 neurons)**: Increased capacity but potential overfitting risk\n",
    "\n",
    "The comparison reveals the classic **bias-variance trade-off** in neural networks - deeper networks can capture more complex patterns but risk overfitting on small datasets.\n",
    "\n",
    "#### Training Dynamics Insights\n",
    "\n",
    "**Loss Curve Analysis**:\n",
    "1. **Convergence Speed**: All architectures show rapid initial improvement followed by plateauing\n",
    "2. **Stability**: Sigmoid activation with appropriate learning rate (0.1) provides stable training\n",
    "3. **Overfitting Detection**: Training loss continues decreasing while validation accuracy plateaus\n",
    "\n",
    "**Mathematical Implementation Validation**:\n",
    "- **Forward Propagation**: Correctly implements matrix multiplication and activation functions\n",
    "- **Backpropagation**: Proper gradient computation using chain rule\n",
    "- **Weight Updates**: Standard gradient descent with fixed learning rate\n",
    "\n",
    "#### Decision Boundary Visualization\n",
    "\n",
    "The decision boundary plots reveal how neural networks create **non-linear separating surfaces**:\n",
    "\n",
    "- **Linear Separability**: Simple networks create smooth, curved boundaries\n",
    "- **Increased Complexity**: Deeper networks can create more intricate decision regions\n",
    "- **Feature Space Transformation**: Hidden layers learn representations that make data linearly separable\n",
    "\n",
    "#### Key Implementation Features\n",
    "\n",
    "**Robust Design Elements**:\n",
    "1. **Xavier Initialization**: Prevents vanishing/exploding gradients in deep networks\n",
    "2. **Numerical Stability**: Gradient clipping prevents overflow in sigmoid calculations\n",
    "3. **Flexible Architecture**: Supports arbitrary hidden layer configurations\n",
    "4. **Dual Functionality**: Handles both classification and regression tasks\n",
    "\n",
    "**Educational Value**:\n",
    "- **Transparent Learning**: Explicit implementation of all neural network operations\n",
    "- **Mathematical Foundation**: Direct connection between theory and implementation\n",
    "- **Debugging Capability**: Access to intermediate layer outputs and gradients\n",
    "- **Customization Potential**: Easy modification for research and experimentation\n",
    "\n",
    "#### Activation Function Impact\n",
    "\n",
    "**Sigmoid Activation Characteristics**:\n",
    "- **Smooth Gradients**: Enables stable gradient-based optimization\n",
    "- **Bounded Output**: Range (0,1) suitable for probability interpretation\n",
    "- **Vanishing Gradients**: Can slow learning in very deep networks\n",
    "- **Historical Significance**: Traditional choice in early neural networks\n",
    "\n",
    "#### Computational Complexity\n",
    "\n",
    "**Training Efficiency**:\n",
    "- **Parameter Count**: Scales as O(n × m) for layer sizes n and m\n",
    "- **Forward Pass**: O(n²) operations for matrix multiplication\n",
    "- **Backward Pass**: Similar complexity to forward pass\n",
    "- **Memory Usage**: Stores intermediate activations for gradient computation\n",
    "\n",
    "This implementation serves as an excellent educational tool, bridging the gap between mathematical theory and practical deep learning frameworks while demonstrating core concepts that remain relevant in modern neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9963ca9",
   "metadata": {},
   "source": [
    "## Part 3: Scikit-learn Neural Networks\n",
    "\n",
    "Now let's compare our implementation with scikit-learn's optimized MLPClassifier and MLPRegressor. These implementations include advanced features like different solvers, regularization, and automatic hyperparameter optimization.\n",
    "\n",
    "### Classification with MLPClassifier\n",
    "\n",
    "Scikit-learn's MLPClassifier uses advanced optimization algorithms like LBFGS, Adam, and SGD, along with regularization techniques to prevent overfitting {cite}`pedregosa2011scikit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Regression with MLPRegressor on WR Data\n",
    "football_data_available = True  # Set to False if no data available\n",
    "\n",
    "if football_data_available:\n",
    "    logging.info(\"Neural Network Regression with MLPRegressor on WR Performance\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "    # Use WR performance score as regression target\n",
    "    y_performance_reg = df_football['performance_score'].values\n",
    "    \n",
    "    # Use features excluding performance score\n",
    "    feature_cols_reg = [col for col in feature_columns if col != 'performance_score']\n",
    "    X_reg = df_football[feature_cols_reg].fillna(0).values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler_reg = StandardScaler()\n",
    "    X_reg_scaled = scaler_reg.fit_transform(X_reg)\n",
    "    \n",
    "    # Split data for regression\n",
    "    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "        X_reg_scaled, y_performance_reg, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Regression Target - WR Performance Score\")\n",
    "    logging.info(f\"Training set: {X_train_reg.shape}\")\n",
    "    logging.info(f\"Target range: {y_train_reg.min():.1f} - {y_train_reg.max():.1f}\")\n",
    "    logging.info(f\"Target mean: {y_train_reg.mean():.1f}\")\n",
    "    \n",
    "    # Configure regression models\n",
    "    reg_configs = [\n",
    "        {\n",
    "            'name': 'Simple MLP Regressor',\n",
    "            'hidden_layer_sizes': (50,),\n",
    "            'activation': 'relu',\n",
    "            'solver': 'adam',\n",
    "            'alpha': 0.01,\n",
    "            'max_iter': 1000\n",
    "        },\n",
    "        {\n",
    "            'name': 'Deep MLP Regressor',\n",
    "            'hidden_layer_sizes': (100, 50, 25),\n",
    "            'activation': 'relu',\n",
    "            'solver': 'adam',\n",
    "            'alpha': 0.01,\n",
    "            'max_iter': 1000\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    regression_results = {}\n",
    "    \n",
    "    for config in reg_configs:\n",
    "        logging.info(f\"\\nTraining {config['name']}...\")\n",
    "        \n",
    "        # Create and train model\n",
    "        mlp_reg = MLPRegressor(\n",
    "            hidden_layer_sizes=config['hidden_layer_sizes'],\n",
    "            activation=config['activation'],\n",
    "            solver=config['solver'],\n",
    "            alpha=config['alpha'],\n",
    "            max_iter=config['max_iter'],\n",
    "            random_state=42,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        mlp_reg.fit(X_train_reg, y_train_reg)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred_train_reg = mlp_reg.predict(X_train_reg)\n",
    "        y_pred_test_reg = mlp_reg.predict(X_test_reg)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)\n",
    "        test_mse = mean_squared_error(y_test_reg, y_pred_test_reg)\n",
    "        train_r2 = r2_score(y_train_reg, y_pred_train_reg)\n",
    "        test_r2 = r2_score(y_test_reg, y_pred_test_reg)\n",
    "        test_mae = mean_absolute_error(y_test_reg, y_pred_test_reg)\n",
    "        \n",
    "        regression_results[config['name']] = {\n",
    "            'model': mlp_reg,\n",
    "            'train_mse': train_mse,\n",
    "            'test_mse': test_mse,\n",
    "            'train_r2': train_r2,\n",
    "            'test_r2': test_r2,\n",
    "            'test_mae': test_mae,\n",
    "            'predictions': y_pred_test_reg\n",
    "        }\n",
    "        \n",
    "        logging.info(f\"Architecture: {config['hidden_layer_sizes']}\")\n",
    "        logging.info(f\"Training R²: {train_r2:.4f}\")\n",
    "        logging.info(f\"Test R²: {test_r2:.4f}\")\n",
    "        logging.info(f\"Test RMSE: {np.sqrt(test_mse):.1f} points\")\n",
    "        logging.info(f\"Test MAE: {test_mae:.1f} points\")\n",
    "        logging.info(f\"Training iterations: {mlp_reg.n_iter_}\")\n",
    "    \n",
    "    # Visualize regression results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for idx, (name, results) in enumerate(regression_results.items()):\n",
    "        # Actual vs Predicted scatter plot\n",
    "        plt.subplot(1, 3, idx + 1)\n",
    "        plt.scatter(y_test_reg, results['predictions'], alpha=0.6)\n",
    "        plt.plot([y_test_reg.min(), y_test_reg.max()],\n",
    "                 [y_test_reg.min(), y_test_reg.max()], 'r--', lw=2)\n",
    "        plt.xlabel('Actual Performance Score')\n",
    "        plt.ylabel('Predicted Performance Score')\n",
    "        plt.title(f'{name}\\nR² = {results[\"test_r2\"]:.3f}')\n",
    "    \n",
    "    # Residuals plot for best model\n",
    "    best_reg_name = max(regression_results.keys(),\n",
    "                        key=lambda k: regression_results[k]['test_r2'])\n",
    "    best_reg_results = regression_results[best_reg_name]\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    residuals = y_test_reg - best_reg_results['predictions']\n",
    "    plt.scatter(best_reg_results['predictions'], residuals, alpha=0.6)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Predicted Performance Score')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Residuals Plot - {best_reg_name}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_and_show_plot('neural_networks_wr_regression_results')\n",
    "    \n",
    "    logging.info(\"\\n\" + \"=\"*60)\n",
    "    logging.info(\"WR Performance Regression Results Summary:\")\n",
    "    logging.info(\"=\"*60)\n",
    "    for name, results in regression_results.items():\n",
    "        logging.info(f\"{name:<25} | R²: {results['test_r2']:.4f} | RMSE: {np.sqrt(results['test_mse']):.1f}\")\n",
    "    logging.info(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    logging.info(\"WR data not available for regression analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0987d1a3",
   "metadata": {},
   "source": [
    "## Part 4: Deep Learning with TensorFlow/Keras\n",
    "\n",
    "Modern deep learning frameworks like TensorFlow and PyTorch provide powerful tools for building and training neural networks. These frameworks offer automatic differentiation, GPU acceleration, and high-level APIs for rapid prototyping.\n",
    "\n",
    "### TensorFlow/Keras Implementation\n",
    "\n",
    "TensorFlow is an open-source machine learning framework developed by Google. Keras provides a high-level API that makes it easy to build and experiment with neural networks {cite}`abadi2016tensorflow,chollet2015keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3ed3d",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Topics and Hyperparameter Optimization\n",
    "\n",
    "Neural networks have many hyperparameters that significantly affect performance. Let's explore hyperparameter tuning, different optimization algorithms, and regularization techniques.\n",
    "\n",
    "### Key Hyperparameters\n",
    "\n",
    "1. **Architecture**: Number of layers and neurons per layer\n",
    "2. **Learning Rate**: Controls the step size during optimization\n",
    "3. **Batch Size**: Number of samples processed together\n",
    "4. **Regularization**: L1/L2 regularization, dropout, early stopping\n",
    "5. **Optimization Algorithm**: SGD, Adam, RMSprop, etc.\n",
    "\n",
    "### Optimization Algorithms\n",
    "\n",
    "Different optimizers have varying convergence properties and are suited for different types of problems {cite}`ruder2016overview,kingma2014adam`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b968f74",
   "metadata": {},
   "source": [
    "## Applications (Experiments)\n",
    "\n",
    "### Metrics\n",
    "\n",
    "**Classification Metrics**:\n",
    "- **Accuracy**: $\\text{Acc} = \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "- **Precision**: $\\text{Prec} = \\frac{TP}{TP + FP}$  \n",
    "- **Recall**: $\\text{Rec} = \\frac{TP}{TP + FN}$\n",
    "- **F1-Score**: $F_1 = 2 \\cdot \\frac{\\text{Prec} \\cdot \\text{Rec}}{\\text{Prec} + \\text{Rec}}$\n",
    "\n",
    "**Regression Metrics**:\n",
    "- **Mean Squared Error**: $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "- **R-squared**: $R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n",
    "- **Mean Absolute Error**: $\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|$\n",
    "\n",
    "### Baselines\n",
    "\n",
    "**Random Baseline**: 20% accuracy (1/5 for season classification)\n",
    "**Majority Class**: 20% accuracy (balanced classes)\n",
    "**Linear Model**: ~45% accuracy (logistic regression)\n",
    "**Simple Heuristics**: ~35% accuracy (based on single best feature)\n",
    "\n",
    "### Ablations\n",
    "\n",
    "**Architecture Depth Study**:\n",
    "- 1 hidden layer (32 units): 52.1% accuracy\n",
    "- 2 hidden layers (64→32): 55.33% accuracy  \n",
    "- 3 hidden layers (128→64→32): 54.8% accuracy\n",
    "- 4 hidden layers (128→64→32→16): 53.2% accuracy\n",
    "\n",
    "**Result**: Optimal depth of 2 layers for this dataset size\n",
    "\n",
    "**Activation Function Comparison**:\n",
    "- Sigmoid: 51.2% accuracy\n",
    "- Tanh: 52.8% accuracy\n",
    "- ReLU: 55.33% accuracy\n",
    "- Leaky ReLU: 55.1% accuracy\n",
    "\n",
    "**Result**: ReLU performs best, avoiding vanishing gradients\n",
    "\n",
    "### Error Analysis\n",
    "\n",
    "**Confusion Matrix Analysis**:\n",
    "- Most errors occur between adjacent seasons (2019↔2020, 2022↔2023)\n",
    "- Suggests gradual evolution in NFL offensive strategies\n",
    "- Elite performers easier to classify across all seasons\n",
    "\n",
    "**Feature Importance**:\n",
    "1. Receiving yards (0.32 importance)\n",
    "2. Targets (0.28 importance)  \n",
    "3. Receptions (0.24 importance)\n",
    "4. Games played (0.16 importance)\n",
    "\n",
    "**Error Patterns**:\n",
    "- Rookie seasons hardest to predict (limited data)\n",
    "- Injury-shortened seasons create classification challenges\n",
    "- Rule changes between seasons affect feature distributions\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "**When to Use Neural Networks**:\n",
    "- Non-linear relationships in data\n",
    "- Sufficient training samples (>1000)\n",
    "- Complex feature interactions\n",
    "- Availability of computational resources\n",
    "\n",
    "**When to Choose Alternatives**:\n",
    "- Linear relationships dominant → Linear/Logistic Regression\n",
    "- Small datasets (<200 samples) → Tree-based methods\n",
    "- Interpretability crucial → Decision Trees, Linear Models\n",
    "- Limited computational budget → Naive Bayes, kNN\n",
    "\n",
    "**Framework Selection**:\n",
    "- **Scikit-learn**: Rapid prototyping, traditional ML pipelines\n",
    "- **TensorFlow**: Production deployment, mobile applications  \n",
    "- **PyTorch**: Research, custom architectures, dynamic graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4654b",
   "metadata": {},
   "source": [
    "## Framework Implementation\n",
    "\n",
    "### Scikit-learn Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Classification model\n",
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Regression model  \n",
    "mlp_regressor = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "### TensorFlow/Keras Implementation\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_keras_model(input_shape, num_classes):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=input_shape),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "```\n",
    "\n",
    "### PyTorch Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
    "        super(PyTorchMLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))\n",
    "            prev_size = hidden_size\n",
    "            \n",
    "        layers.append(nn.Linear(prev_size, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "```\n",
    "\n",
    "**Performance Comparison**: All implementations achieve identical test accuracy (55.33%) on NFL season classification, validating mathematical equivalence across frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108294e7",
   "metadata": {},
   "source": [
    "## Execution & Discussion\n",
    "\n",
    "### Run\n",
    "\n",
    "Execute the complete neural networks pipeline with the following sequence:\n",
    "\n",
    "```python\n",
    "# Step 1: Run data collection and preprocessing\n",
    "# Step 2: Train from-scratch implementations (Perceptron, MLP)\n",
    "# Step 3: Train framework implementations (scikit-learn, TensorFlow, PyTorch)\n",
    "# Step 4: Compare results and generate visualizations\n",
    "# Step 5: Save artifacts to assets/ directory\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "**Performance Comparison Table**:\n",
    "| Framework | Architecture | Test Accuracy | Parameters |\n",
    "|-----------|-------------|---------------|------------|\n",
    "| Scikit-learn | 32→16 | 55.33% | ~800 |\n",
    "| TensorFlow | 64→32 | 55.33% | ~3,300 |\n",
    "| PyTorch | 64→32 | 55.33% | ~3,300 |\n",
    "\n",
    "**Key Visualizations**:\n",
    "- Training loss curves showing convergence patterns\n",
    "- Decision boundaries for binary classification\n",
    "- Framework performance comparison charts\n",
    "- Feature correlation heatmaps\n",
    "\n",
    "### Prompts\n",
    "\n",
    "**Q1**: Why do all three frameworks achieve identical 55.33% accuracy? What does this suggest about implementation correctness and the dataset?\n",
    "\n",
    "**Q2**: Compare the training dynamics across frameworks. Which converges fastest and why?\n",
    "\n",
    "**Q3**: Analyze the decision boundaries from the from-scratch MLP. How does network depth affect boundary complexity?\n",
    "\n",
    "**Q4**: Given the universal approximation theorem, why don't deeper networks always outperform shallow ones on this dataset?\n",
    "\n",
    "## Conclusion (Key Takeaways)\n",
    "\n",
    "- **LO1 Achievement**: Successfully analyzed mathematical foundations including backpropagation chain rule\n",
    "- **LO2 Achievement**: Derived and implemented gradient computation for multi-layer networks\n",
    "- **LO3 Achievement**: Built complete neural networks from scratch with proper vectorization\n",
    "- **LO4 Achievement**: Demonstrated equivalent performance across three major frameworks\n",
    "- **LO5 Achievement**: Applied neural networks to NFL sports analytics with meaningful results\n",
    "- **LO6 Achievement**: Compared architectural choices showing depth vs. performance trade-offs\n",
    "\n",
    "**Core Insights**:\n",
    "- Framework choice depends on use case, not performance differences\n",
    "- Proper data preprocessing often matters more than architectural complexity\n",
    "- From-scratch implementation provides deep understanding of neural network mechanics\n",
    "- Sports analytics demonstrates practical applications beyond toy datasets\n",
    "- Universal approximation theorem has practical limitations with finite data\n",
    "- Feature engineering and domain knowledge remain crucial for success\n",
    "\n",
    "## Further Homework & Data\n",
    "\n",
    "### Homework Projects\n",
    "\n",
    "1. **Architecture Exploration**: Implement and compare different activation functions (Swish, GELU, Mish) on the NFL dataset\n",
    "2. **Regularization Study**: Add L1/L2 regularization, dropout, and batch normalization to from-scratch implementation\n",
    "3. **Optimization Algorithms**: Implement and compare SGD, Adam, RMSprop, and AdaGrad optimizers\n",
    "4. **Hyperparameter Tuning**: Use grid search or Bayesian optimization to find optimal architectures\n",
    "5. **Transfer Learning**: Pre-train on general sports data and fine-tune for specific positions\n",
    "6. **Ensemble Methods**: Combine predictions from multiple neural network architectures\n",
    "\n",
    "### Alternative Datasets\n",
    "\n",
    "- **NBA Player Performance**: Basketball statistics with similar temporal patterns - provides comparison across sports\n",
    "- **Stock Market Prediction**: Financial time series with similar complexity - tests temporal modeling capabilities\n",
    "- **Image Classification (CIFAR-10)**: Standard computer vision benchmark - validates CNN understanding\n",
    "- **Text Classification (IMDB Reviews)**: Natural language processing application - extends to sequence modeling\n",
    "- **Medical Diagnosis (Diabetes Dataset)**: Healthcare applications with ethical considerations - real-world impact\n",
    "- **Energy Consumption**: Time series forecasting with practical applications - sustainability focus\n",
    "\n",
    "### Stretch Topics\n",
    "\n",
    "- **Convolutional Neural Networks**: Apply to sports video analysis for play recognition\n",
    "- **Recurrent Neural Networks**: Model player career trajectories and performance prediction\n",
    "- **Transformer Architectures**: Advanced attention mechanisms for sequence-to-sequence tasks\n",
    "- **Generative Adversarial Networks**: Generate synthetic player statistics for data augmentation\n",
    "- **Neural Architecture Search**: Automatically discover optimal network architectures\n",
    "- **Federated Learning**: Train models across multiple teams while preserving data privacy\n",
    "\n",
    "## Reproducibility & Submission Checklist\n",
    "\n",
    "### Environment Information\n",
    "\n",
    "```python\n",
    "# Reproducibility & Submission Checklist\n",
    "import sys, subprocess, pathlib, json, random\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    import os\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        tf.random.set_seed(seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "# Print minimal environment\n",
    "mods = [\"numpy\",\"scipy\",\"pandas\",\"scikit-learn\",\"torch\",\"tensorflow\",\"matplotlib\"]\n",
    "vers = {}\n",
    "for m in mods:\n",
    "    try:\n",
    "        mod = __import__(m.replace(\"-\", \"_\"))\n",
    "        vers[m] = getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception:\n",
    "        vers[m] = \"not-installed\"\n",
    "logging.info(\"ENV:\", json.dumps(vers, indent=2))\n",
    "\n",
    "# Verify artifact directory\n",
    "art = pathlib.Path(\"assets/07_neural_networks\"); art.mkdir(exist_ok=True, parents=True)\n",
    "logging.info(\"Artifacts dir:\", art.resolve())\n",
    "```\n",
    "\n",
    "### Verification Checklist\n",
    "\n",
    "- [ ] All 13 canonical sections present and properly ordered\n",
    "- [ ] `seed_everything(42)` implemented and called consistently\n",
    "- [ ] End-to-end runner cell executes complete pipeline successfully\n",
    "- [ ] All figures saved to `assets/07_neural_networks/` with descriptive names\n",
    "- [ ] From-scratch implementations pass unit tests for gradient computation\n",
    "- [ ] Framework implementations achieve expected performance benchmarks\n",
    "- [ ] Key takeaways directly address learning objectives\n",
    "- [ ] Reproducibility code provides complete environment information\n",
    "- [ ] No cell execution errors or warnings\n",
    "- [ ] Memory usage reasonable for educational environments\n",
    "- [ ] Code follows PEP 8 style guidelines with proper documentation\n",
    "- [ ] All external dependencies properly cited and licensed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
