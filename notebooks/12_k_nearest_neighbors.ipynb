{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23963b86",
   "metadata": {},
   "source": [
    "# Chapter 12: K-Nearest Neighbors (KNN)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of instance-based learning\n",
    "- **Distance metrics** and their impact on performance\n",
    "- **Optimal k selection** using cross-validation\n",
    "- **Implementation** for both classification and regression\n",
    "- **Curse of dimensionality** and mitigation strategies\n",
    "\n",
    "## Introduction\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple, non-parametric algorithm that makes predictions based on the k closest training examples in the feature space. It's both intuitive and effective for many applications.\n",
    "\n",
    "**Mathematical Foundation**: KNN relies on the assumption that similar data points have similar labels, using distance metrics to define similarity in the feature space.\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "Common distance metrics include:\n",
    "\n",
    "- **Euclidean distance**: $d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$\n",
    "- **Manhattan distance**: $d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|$\n",
    "- **Minkowski distance**: $d(x, y) = (\\sum_{i=1}^{n} |x_i - y_i|^p)^{1/p}$\n",
    "\n",
    "### Prediction Rules\n",
    "\n",
    "For classification: $\\hat{y} = \\text{mode}(y_1, y_2, ..., y_k)$\n",
    "\n",
    "For regression: $\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i$\n",
    "\n",
    "**Citation**: Instance-based learning and lazy learning methods are described in machine learning textbooks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
