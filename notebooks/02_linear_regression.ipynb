{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214d54b4",
   "metadata": {},
   "source": [
    "# Chapter 2: Linear Regression with Real Estate Data\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of linear regression with proper derivations\n",
    "- **Data collection** from real estate websites for regression analysis  \n",
    "- **Implementation** from scratch using NumPy\n",
    "- **Scikit-learn** implementation for comparison\n",
    "- **Model evaluation** and interpretation techniques\n",
    "- **Assumptions** and diagnostics for linear regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear regression is the foundation of predictive modeling and one of the most important algorithms in machine learning. It models the relationship between a dependent variable and independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "**Mathematical Foundation**: Linear regression finds the best-fitting line through data points by minimizing the sum of squared residuals, as originally formulated by Legendre (1805) and Gauss (1809) (Stigler, 1981).\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### The Linear Model\n",
    "\n",
    "For a single predictor variable, the linear regression model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable (response)\n",
    "- $x$ is the independent variable (predictor)  \n",
    "- $\\beta_0$ is the y-intercept\n",
    "- $\\beta_1$ is the slope coefficient\n",
    "- $\\epsilon$ is the error term\n",
    "\n",
    "For multiple predictors:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$$\n",
    "\n",
    "Or in matrix form: $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n",
    "\n",
    "**Citation**: The mathematical foundations are detailed in Hastie et al. (2009) and James et al. (2013).\n",
    "\n",
    "### Least Squares Estimation\n",
    "\n",
    "The ordinary least squares (OLS) estimator minimizes the residual sum of squares:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2$$\n",
    "\n",
    "The closed-form solution is:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "**Citation**: The derivation and properties of OLS are covered in Greene (2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e6a23f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utility functions imported successfully\n",
      "Libraries imported successfully!\n",
      "NumPy version: 2.2.6\n",
      "Pandas version: 2.3.1\n",
      "Scikit-learn available: True\n",
      "Matplotlib backend: Agg\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add utils directory to path for imports\n",
    "utils_path = os.path.abspath(os.path.join('..', 'utils'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend for building\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "\n",
    "# Import utility functions\n",
    "try:\n",
    "    from data_utils import save_data, load_data, check_data_quality\n",
    "    print(\"‚úì Utility functions imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö† Could not import utilities: {e}\")\n",
    "    # Define basic save_data function as fallback\n",
    "    def save_data(data, filepath, format='csv'):\n",
    "        import json\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        if format == 'csv' and hasattr(data, 'to_csv'):\n",
    "            data.to_csv(filepath, index=False)\n",
    "        elif format == 'json':\n",
    "            if hasattr(data, 'to_json'):\n",
    "                data.to_json(filepath, orient='records', date_format='iso')\n",
    "            else:\n",
    "                with open(filepath, 'w') as f:\n",
    "                    json.dump(data, f, indent=2, default=str)\n",
    "        return filepath\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for inline plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn available: True\")\n",
    "print(f\"Matplotlib backend: {matplotlib.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74f50e",
   "metadata": {},
   "source": [
    "## Data Collection: Real Estate Prices\n",
    "\n",
    "For our linear regression analysis, we'll collect real estate data that naturally exhibits linear relationships between features like square footage, number of bedrooms, and price.\n",
    "\n",
    "**Real-World Application**: Housing price prediction is a classic application of linear regression in economics and real estate, as documented by Case & Shiller (1989).\n",
    "\n",
    "Since we need to be ethical about web scraping, we'll simulate realistic real estate data based on actual market patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92df1712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating realistic real estate dataset...\n",
      "Dataset created with 1000 properties\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   sqft                1000 non-null   int64  \n",
      " 1   bedrooms            1000 non-null   int64  \n",
      " 2   bathrooms           1000 non-null   float64\n",
      " 3   age                 1000 non-null   float64\n",
      " 4   location_score      1000 non-null   float64\n",
      " 5   distance_to_center  1000 non-null   float64\n",
      " 6   garage              1000 non-null   int32  \n",
      " 7   school_rating       1000 non-null   float64\n",
      " 8   price               1000 non-null   int64  \n",
      "dtypes: float64(5), int32(1), int64(3)\n",
      "memory usage: 66.5 KB\n",
      "None\n",
      "\n",
      "First 10 properties:\n",
      "   sqft  bedrooms  bathrooms   age  location_score  distance_to_center  \\\n",
      "0  1634         3        2.0  25.6             3.9                 1.7   \n",
      "1  1267         3        2.0  50.0             7.1                 5.2   \n",
      "2  1736         3        2.0   4.5             6.1                 1.3   \n",
      "3  2463         4        3.0  50.0             4.9                 3.4   \n",
      "4  1220         2        1.0   8.5             6.1                 6.2   \n",
      "5  1220         2        1.5   6.4             3.1                 1.1   \n",
      "6  2519         5        4.0   0.8             5.3                 7.9   \n",
      "7  1821         3        2.0   0.8             3.7                 5.4   \n",
      "8  1110         2        1.5  17.7             5.0                 8.7   \n",
      "9  1664         3        2.5   9.2             4.3                 8.6   \n",
      "\n",
      "   garage  school_rating   price  \n",
      "0       0            7.0  322290  \n",
      "1       1            5.1  339683  \n",
      "2       0            6.8  445151  \n",
      "3       1            2.9  416614  \n",
      "4       0            4.2  277682  \n",
      "5       1            9.1  315216  \n",
      "6       1            5.1  611828  \n",
      "7       0            5.4  322345  \n",
      "8       0            7.9  302823  \n",
      "9       0            7.0  312293  \n",
      "\n",
      "Basic Statistics:\n",
      "              sqft     bedrooms    bathrooms         age  location_score  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.00000      1000.00000   \n",
      "mean   1466.617000     2.487000     1.898500    13.99210         5.64320   \n",
      "std     581.422043     1.083977     0.806907    12.84787         2.03969   \n",
      "min     800.000000     1.000000     1.000000     0.00000         1.20000   \n",
      "25%    1033.750000     2.000000     1.500000     4.17500         4.10000   \n",
      "50%    1353.000000     2.000000     1.500000     9.90000         5.70000   \n",
      "75%    1736.000000     3.000000     2.500000    19.70000         7.30000   \n",
      "max    4000.000000     6.000000     4.000000    50.00000         9.90000   \n",
      "\n",
      "       distance_to_center       garage  school_rating          price  \n",
      "count         1000.000000  1000.000000    1000.000000    1000.000000  \n",
      "mean             6.153700     0.374000       6.524500  334688.611000  \n",
      "std              4.273288     0.484106       1.735916  105324.787016  \n",
      "min              1.000000     0.000000       1.300000   98485.000000  \n",
      "25%              3.100000     0.000000       5.300000  265469.750000  \n",
      "50%              5.100000     0.000000       6.700000  327192.000000  \n",
      "75%              8.200000     1.000000       7.900000  392589.250000  \n",
      "max             27.900000     1.000000      10.000000  832879.000000  \n"
     ]
    }
   ],
   "source": [
    "def simulate_real_estate_data(n_samples: int = 1000, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate realistic real estate data for linear regression analysis.\n",
    "    \n",
    "    This function creates data that mimics patterns found in real housing markets,\n",
    "    with realistic relationships between features and prices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, default=1000\n",
    "        Number of properties to simulate\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Simulated real estate dataset with features and target price\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate base features with realistic distributions\n",
    "    # Square footage (log-normal distribution, typical range 800-4000 sqft)\n",
    "    sqft = np.random.lognormal(mean=7.2, sigma=0.4, size=n_samples)\n",
    "    sqft = np.clip(sqft, 800, 4000)\n",
    "    \n",
    "    # Number of bedrooms (correlated with square footage)\n",
    "    # Typical relationship: 1 bedroom per ~600 sqft, with some variation\n",
    "    bedrooms_base = sqft / 600 + np.random.normal(0, 0.5, n_samples)\n",
    "    bedrooms = np.clip(np.round(bedrooms_base), 1, 6).astype(int)\n",
    "    \n",
    "    # Number of bathrooms (correlated with bedrooms, typically 0.5-1 per bedroom)\n",
    "    bathrooms_base = bedrooms * 0.75 + np.random.normal(0, 0.3, n_samples)\n",
    "    bathrooms = np.clip(np.round(bathrooms_base * 2) / 2, 1, 4)  # Round to nearest 0.5\n",
    "    \n",
    "    # Property age (0-50 years, exponential decay in frequency)\n",
    "    age = np.random.exponential(scale=15, size=n_samples)\n",
    "    age = np.clip(age, 0, 50)\n",
    "    \n",
    "    # Location score (1-10, representing neighborhood desirability)\n",
    "    location_score = np.random.beta(2, 2, n_samples) * 9 + 1\n",
    "    \n",
    "    # Distance to city center (miles, affects price)\n",
    "    distance_to_center = np.random.gamma(2, 3, n_samples)\n",
    "    distance_to_center = np.clip(distance_to_center, 1, 30)\n",
    "    \n",
    "    # Garage (binary: 0 or 1)\n",
    "    garage_prob = 0.3 + 0.4 * (sqft - 800) / (4000 - 800)  # Larger homes more likely to have garage\n",
    "    garage = np.random.binomial(1, garage_prob, n_samples)\n",
    "    \n",
    "    # School rating (1-10, affects desirability)\n",
    "    school_rating = np.random.beta(3, 2, n_samples) * 9 + 1\n",
    "    \n",
    "    # Generate realistic price based on these features\n",
    "    # Base price equation with realistic coefficients\n",
    "    price_base = (\n",
    "        100 * sqft +                           # $100 per sqft\n",
    "        15000 * bedrooms +                     # $15k per bedroom\n",
    "        8000 * bathrooms +                     # $8k per bathroom  \n",
    "        -2000 * age +                          # -$2k per year of age\n",
    "        25000 * location_score +               # $25k per location point\n",
    "        -3000 * distance_to_center +           # -$3k per mile from center\n",
    "        20000 * garage +                       # $20k for garage\n",
    "        5000 * school_rating                   # $5k per school rating point\n",
    "    )\n",
    "    \n",
    "    # Add realistic noise (market fluctuations, unmeasured factors)\n",
    "    noise = np.random.normal(0, 0.1 * price_base, n_samples)\n",
    "    price = price_base + noise\n",
    "    \n",
    "    # Ensure prices are reasonable (minimum $50k)\n",
    "    price = np.maximum(price, 50000)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'sqft': np.round(sqft).astype(int),\n",
    "        'bedrooms': bedrooms,\n",
    "        'bathrooms': bathrooms,\n",
    "        'age': np.round(age, 1),\n",
    "        'location_score': np.round(location_score, 1),\n",
    "        'distance_to_center': np.round(distance_to_center, 1),\n",
    "        'garage': garage,\n",
    "        'school_rating': np.round(school_rating, 1),\n",
    "        'price': np.round(price).astype(int)\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"Generating realistic real estate dataset...\")\n",
    "real_estate_data = simulate_real_estate_data(n_samples=1000, random_state=42)\n",
    "\n",
    "print(f\"Dataset created with {len(real_estate_data)} properties\")\n",
    "print(\"\\nDataset Info:\")\n",
    "print(real_estate_data.info())\n",
    "print(\"\\nFirst 10 properties:\")\n",
    "print(real_estate_data.head(10))\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(real_estate_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed15dd1",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Before building our regression model, we need to understand the data distribution, relationships, and potential issues.\n",
    "\n",
    "**Citation**: The importance of exploratory data analysis is emphasized by Tukey (1977) and Cleveland (1993)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225a2a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations created successfully!\n",
      "\n",
      "=== KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS ===\n",
      "Average home price: $334,689\n",
      "Price range: $98,485 - $832,879\n",
      "Average square footage: 1467 sqft\n",
      "Most common bedroom count: 2 bedrooms\n",
      "\n",
      "Strongest correlations with price:\n",
      "  sqft: 0.732\n",
      "  bedrooms: 0.663\n",
      "  bathrooms: 0.620\n",
      "  location_score: 0.516\n",
      "  age: 0.232\n",
      "\n",
      "Garage effect:\n",
      "  Homes without garage: $316,274 (average)\n",
      "  Homes with garage: $365,511 (average)\n",
      "  Price premium for garage: $49,237 (15.6%)\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive visualizations\n",
    "try:\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "    fig.suptitle('Real Estate Data: Exploratory Data Analysis', fontsize=20, fontweight='bold')\n",
    "\n",
    "    # 1. Price distribution\n",
    "    axes[0, 0].hist(real_estate_data['price'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Price Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Price ($)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add statistics\n",
    "    mean_price = real_estate_data['price'].mean()\n",
    "    median_price = real_estate_data['price'].median()\n",
    "    axes[0, 0].axvline(mean_price, color='red', linestyle='--', label=f'Mean: ${mean_price:,.0f}')\n",
    "    axes[0, 0].axvline(median_price, color='orange', linestyle='--', label=f'Median: ${median_price:,.0f}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # 2. Square footage vs Price\n",
    "    axes[0, 1].scatter(real_estate_data['sqft'], real_estate_data['price'], alpha=0.6, s=50, color='green')\n",
    "    axes[0, 1].set_title('Square Footage vs Price', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Square Feet')\n",
    "    axes[0, 1].set_ylabel('Price ($)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add correlation coefficient\n",
    "    corr_sqft_price = real_estate_data['sqft'].corr(real_estate_data['price'])\n",
    "    axes[0, 1].text(0.05, 0.95, f'Correlation: {corr_sqft_price:.3f}', \n",
    "                    transform=axes[0, 1].transAxes, fontsize=12,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # 3. Number of bedrooms distribution\n",
    "    bedroom_counts = real_estate_data['bedrooms'].value_counts().sort_index()\n",
    "    axes[0, 2].bar(bedroom_counts.index, bedroom_counts.values, alpha=0.7, color='lightcoral')\n",
    "    axes[0, 2].set_title('Number of Bedrooms Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Number of Bedrooms')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Age vs Price\n",
    "    axes[1, 0].scatter(real_estate_data['age'], real_estate_data['price'], alpha=0.6, s=50, color='purple')\n",
    "    axes[1, 0].set_title('Property Age vs Price', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Age (years)')\n",
    "    axes[1, 0].set_ylabel('Price ($)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add correlation coefficient\n",
    "    corr_age_price = real_estate_data['age'].corr(real_estate_data['price'])\n",
    "    axes[1, 0].text(0.05, 0.95, f'Correlation: {corr_age_price:.3f}', \n",
    "                    transform=axes[1, 0].transAxes, fontsize=12,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # 5. Location score vs Price\n",
    "    axes[1, 1].scatter(real_estate_data['location_score'], real_estate_data['price'], alpha=0.6, s=50, color='orange')\n",
    "    axes[1, 1].set_title('Location Score vs Price', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Location Score (1-10)')\n",
    "    axes[1, 1].set_ylabel('Price ($)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add correlation coefficient\n",
    "    corr_location_price = real_estate_data['location_score'].corr(real_estate_data['price'])\n",
    "    axes[1, 1].text(0.05, 0.95, f'Correlation: {corr_location_price:.3f}', \n",
    "                    transform=axes[1, 1].transAxes, fontsize=12,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # 6. Garage effect on price\n",
    "    garage_prices = [real_estate_data[real_estate_data['garage'] == 0]['price'],\n",
    "                     real_estate_data[real_estate_data['garage'] == 1]['price']]\n",
    "    axes[1, 2].boxplot(garage_prices, labels=['No Garage', 'Has Garage'], patch_artist=True)\n",
    "    axes[1, 2].set_title('Garage Effect on Price', fontsize=14, fontweight='bold')\n",
    "    axes[1, 2].set_ylabel('Price ($)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add mean prices (fixed text formatting)\n",
    "    no_garage_mean = real_estate_data[real_estate_data['garage'] == 0]['price'].mean()\n",
    "    garage_mean = real_estate_data[real_estate_data['garage'] == 1]['price'].mean()\n",
    "    price_text = f'No Garage: ${no_garage_mean:,.0f} | Has Garage: ${garage_mean:,.0f}'\n",
    "    axes[1, 2].text(0.5, 0.95, price_text, \n",
    "                    transform=axes[1, 2].transAxes, fontsize=10, ha='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # 7. Correlation heatmap\n",
    "    numeric_cols = ['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                    'distance_to_center', 'garage', 'school_rating', 'price']\n",
    "    correlation_matrix = real_estate_data[numeric_cols].corr()\n",
    "\n",
    "    im = axes[2, 0].imshow(correlation_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "    axes[2, 0].set_xticks(range(len(correlation_matrix.columns)))\n",
    "    axes[2, 0].set_yticks(range(len(correlation_matrix.columns)))\n",
    "    axes[2, 0].set_xticklabels(correlation_matrix.columns, rotation=45, ha='right')\n",
    "    axes[2, 0].set_yticklabels(correlation_matrix.columns)\n",
    "    axes[2, 0].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Add correlation values as text\n",
    "    for i in range(len(correlation_matrix)):\n",
    "        for j in range(len(correlation_matrix)):\n",
    "            text = axes[2, 0].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                                  ha=\"center\", va=\"center\", color=\"white\" if abs(correlation_matrix.iloc[i, j]) > 0.5 else \"black\",\n",
    "                                  fontweight='bold', fontsize=10)\n",
    "\n",
    "    # 8. Distance to center vs Price\n",
    "    axes[2, 1].scatter(real_estate_data['distance_to_center'], real_estate_data['price'], alpha=0.6, s=50, color='brown')\n",
    "    axes[2, 1].set_title('Distance to Center vs Price', fontsize=14, fontweight='bold')\n",
    "    axes[2, 1].set_xlabel('Distance to Center (miles)')\n",
    "    axes[2, 1].set_ylabel('Price ($)')\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add correlation coefficient\n",
    "    corr_distance_price = real_estate_data['distance_to_center'].corr(real_estate_data['price'])\n",
    "    axes[2, 1].text(0.05, 0.95, f'Correlation: {corr_distance_price:.3f}', \n",
    "                    transform=axes[2, 1].transAxes, fontsize=12,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # 9. School rating vs Price\n",
    "    axes[2, 2].scatter(real_estate_data['school_rating'], real_estate_data['price'], alpha=0.6, s=50, color='teal')\n",
    "    axes[2, 2].set_title('School Rating vs Price', fontsize=14, fontweight='bold')\n",
    "    axes[2, 2].set_xlabel('School Rating (1-10)')\n",
    "    axes[2, 2].set_ylabel('Price ($)')\n",
    "    axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add correlation coefficient\n",
    "    corr_school_price = real_estate_data['school_rating'].corr(real_estate_data['price'])\n",
    "    axes[2, 2].text(0.05, 0.95, f'Correlation: {corr_school_price:.3f}', \n",
    "                    transform=axes[2, 2].transAxes, fontsize=12,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Visualizations created successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error (non-critical): {e}\")\n",
    "    print(\"Continuing without visualizations...\")\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\n=== KEY INSIGHTS FROM EXPLORATORY DATA ANALYSIS ===\")\n",
    "print(f\"Average home price: ${real_estate_data['price'].mean():,.0f}\")\n",
    "print(f\"Price range: ${real_estate_data['price'].min():,.0f} - ${real_estate_data['price'].max():,.0f}\")\n",
    "print(f\"Average square footage: {real_estate_data['sqft'].mean():.0f} sqft\")\n",
    "print(f\"Most common bedroom count: {real_estate_data['bedrooms'].mode().iloc[0]} bedrooms\")\n",
    "\n",
    "print(\"\\nStrongest correlations with price:\")\n",
    "price_correlations = real_estate_data.corr()['price'].abs().sort_values(ascending=False)[1:]  # Exclude price-price correlation\n",
    "for feature, corr in price_correlations.head(5).items():\n",
    "    print(f\"  {feature}: {corr:.3f}\")\n",
    "\n",
    "print(f\"\\nGarage effect:\")\n",
    "print(f\"  Homes without garage: ${no_garage_mean:,.0f} (average)\")\n",
    "print(f\"  Homes with garage: ${garage_mean:,.0f} (average)\")\n",
    "print(f\"  Price premium for garage: ${garage_mean - no_garage_mean:,.0f} ({((garage_mean - no_garage_mean) / no_garage_mean * 100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b498f71",
   "metadata": {},
   "source": [
    "## Linear Regression Implementation from Scratch\n",
    "\n",
    "Now we'll implement linear regression from scratch using the mathematical formulas we discussed. This helps understand what's happening under the hood.\n",
    "\n",
    "**Mathematical Implementation**: We'll implement both the analytical solution and gradient descent for comparison, following the approaches described in Murphy (2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdd9b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Linear Regression Implementation\n",
      "==================================================\n",
      "Analytical Solution:\n",
      "Intercept: $136,998.85\n",
      "Coefficient (sqft): $135.62 per sqft\n",
      "\\nGradient Descent Solution:\n",
      "Intercept: $19.10\n",
      "Coefficient (sqft): $215.79 per sqft\n",
      "\\nR¬≤ Score (Analytical): 0.4089\n",
      "R¬≤ Score (Gradient Descent): 0.1078\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionFromScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation from scratch using NumPy.\n",
    "    \n",
    "    This class implements both analytical (closed-form) and gradient descent solutions\n",
    "    for educational purposes.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    coefficients_ : np.ndarray\n",
    "        Fitted coefficients (Œ≤‚ÇÅ, Œ≤‚ÇÇ, ..., Œ≤‚Çö)\n",
    "    intercept_ : float\n",
    "        Fitted intercept (Œ≤‚ÇÄ)\n",
    "    cost_history_ : list\n",
    "        Cost function values during gradient descent (if used)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='analytical', learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str, default='analytical'\n",
    "            Method to use: 'analytical' or 'gradient_descent'\n",
    "        learning_rate : float, default=0.01\n",
    "            Learning rate for gradient descent\n",
    "        max_iterations : int, default=1000\n",
    "            Maximum iterations for gradient descent\n",
    "        tolerance : float, default=1e-6\n",
    "            Convergence tolerance for gradient descent\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.coefficients_ = None\n",
    "        self.intercept_ = None\n",
    "        self.cost_history_ = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : np.ndarray, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Add bias term (intercept) to X\n",
    "        X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        if self.method == 'analytical':\n",
    "            self._fit_analytical(X_with_bias, y)\n",
    "        elif self.method == 'gradient_descent':\n",
    "            self._fit_gradient_descent(X_with_bias, y)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'analytical' or 'gradient_descent'\")\n",
    "    \n",
    "    def _fit_analytical(self, X_with_bias, y):\n",
    "        \"\"\"\n",
    "        Fit using analytical solution: Œ≤ = (X^T X)^(-1) X^T y\n",
    "        \"\"\"\n",
    "        # Calculate (X^T X)^(-1) X^T y\n",
    "        XtX = X_with_bias.T @ X_with_bias\n",
    "        XtX_inv = np.linalg.inv(XtX)\n",
    "        Xty = X_with_bias.T @ y\n",
    "        \n",
    "        theta = XtX_inv @ Xty\n",
    "        \n",
    "        self.intercept_ = theta[0]\n",
    "        self.coefficients_ = theta[1:]\n",
    "    \n",
    "    def _fit_gradient_descent(self, X_with_bias, y):\n",
    "        \"\"\"\n",
    "        Fit using gradient descent optimization.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X_with_bias.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        theta = np.zeros(n_features)\n",
    "        \n",
    "        self.cost_history_ = []\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward pass: calculate predictions\n",
    "            y_pred = X_with_bias @ theta\n",
    "            \n",
    "            # Calculate cost (Mean Squared Error)\n",
    "            cost = np.mean((y_pred - y) ** 2)\n",
    "            self.cost_history_.append(cost)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = (2 / n_samples) * X_with_bias.T @ (y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            theta_new = theta - self.learning_rate * gradients\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(theta, theta_new, atol=self.tolerance):\n",
    "                print(f\"Converged after {i+1} iterations\")\n",
    "                break\n",
    "            \n",
    "            theta = theta_new\n",
    "        \n",
    "        self.intercept_ = theta[0]\n",
    "        self.coefficients_ = theta[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the fitted model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (n_samples, n_features)\n",
    "            Input data\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray, shape (n_samples,)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        if self.coefficients_ is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        X = np.array(X)\n",
    "        return self.intercept_ + X @ self.coefficients_\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R¬≤ score.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input data\n",
    "        y : np.ndarray\n",
    "            True values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            R¬≤ score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test our implementation with a simple example\n",
    "print(\"Testing Linear Regression Implementation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start with square footage as single predictor\n",
    "X_simple = real_estate_data[['sqft']].values\n",
    "y = real_estate_data['price'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Test analytical method\n",
    "model_analytical = LinearRegressionFromScratch(method='analytical')\n",
    "model_analytical.fit(X_train, y_train)\n",
    "\n",
    "print(\"Analytical Solution:\")\n",
    "print(f\"Intercept: ${model_analytical.intercept_:,.2f}\")\n",
    "print(f\"Coefficient (sqft): ${model_analytical.coefficients_[0]:.2f} per sqft\")\n",
    "\n",
    "# Test gradient descent method  \n",
    "model_gd = LinearRegressionFromScratch(method='gradient_descent', learning_rate=0.0000001, max_iterations=5000)\n",
    "model_gd.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\\\nGradient Descent Solution:\")\n",
    "print(f\"Intercept: ${model_gd.intercept_:,.2f}\")\n",
    "print(f\"Coefficient (sqft): ${model_gd.coefficients_[0]:.2f} per sqft\")\n",
    "\n",
    "# Compare predictions\n",
    "y_pred_analytical = model_analytical.predict(X_test)\n",
    "y_pred_gd = model_gd.predict(X_test)\n",
    "\n",
    "print(f\"\\\\nR¬≤ Score (Analytical): {model_analytical.score(X_test, y_test):.4f}\")\n",
    "print(f\"R¬≤ Score (Gradient Descent): {model_gd.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Visualize gradient descent convergence\n",
    "if len(model_gd.cost_history_) > 1:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model_gd.cost_history_, linewidth=2, color='blue')\n",
    "    plt.title('Gradient Descent Convergence', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost (MSE)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_test, y_test, alpha=0.6, label='Actual', s=50)\n",
    "    plt.plot(X_test, y_pred_analytical, color='red', linewidth=2, label='Analytical')\n",
    "    plt.plot(X_test, y_pred_gd, color='green', linewidth=2, linestyle='--', label='Gradient Descent')\n",
    "    plt.title('Predictions Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Square Footage')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927faf8f",
   "metadata": {},
   "source": [
    "## Data Storage and Management\n",
    "\n",
    "Following best practices, we'll save our processed real estate data to the appropriate directories for future use.\n",
    "\n",
    "**Citation**: Data management best practices are outlined by Wilkinson et al. (2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6273d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA SAVED SUCCESSFULLY ===\n",
      "Timestamp: 2025-08-12\n",
      "\\nFiles saved:\n",
      "  Raw data: ../data/raw/2025-08-12_real_estate_raw.csv\n",
      "  Features: ../data/features/2025-08-12_real_estate_features.csv\n",
      "  Training: ../data/processed/2025-08-12_real_estate_train.csv\n",
      "  Testing:  ../data/processed/2025-08-12_real_estate_test.csv\n",
      "  Metadata: ../data/processed/2025-08-12_real_estate_metadata.json\n",
      "\\nDataset summary:\n",
      "  Total samples: 1,000\n",
      "  Training samples: 800\n",
      "  Testing samples: 200\n",
      "  Features: 14\n",
      "  Price range: $98,485 - $832,879\n",
      "\\nüìÅ Data organization follows best practices:\n",
      "  ‚úÖ Raw data preserved in /raw directory\n",
      "  ‚úÖ Processed data in /processed directory\n",
      "  ‚úÖ Feature-engineered data in /features directory\n",
      "  ‚úÖ Comprehensive metadata documentation\n",
      "  ‚úÖ Reproducible train/test splits\n",
      "  ‚úÖ Timestamped for version control\n"
     ]
    }
   ],
   "source": [
    "# Save processed data following the established directory structure\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamp for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Ensure data directories exist\n",
    "data_dirs = ['../data/processed', '../data/raw', '../data/features']\n",
    "for dir_path in data_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Save raw simulated data\n",
    "save_data(real_estate_data, f'../data/raw/{timestamp}_real_estate_raw.csv', format='csv')\n",
    "save_data(real_estate_data, f'../data/raw/{timestamp}_real_estate_raw.json', format='json')\n",
    "\n",
    "# Create feature-engineered version for future use\n",
    "real_estate_features = real_estate_data.copy()\n",
    "\n",
    "# Add derived features that might be useful for other algorithms\n",
    "real_estate_features['price_per_sqft'] = real_estate_features['price'] / real_estate_features['sqft']\n",
    "real_estate_features['total_rooms'] = real_estate_features['bedrooms'] + real_estate_features['bathrooms']\n",
    "real_estate_features['luxury_score'] = (\n",
    "    real_estate_features['location_score'] * 0.4 + \n",
    "    real_estate_features['school_rating'] * 0.3 + \n",
    "    (real_estate_features['sqft'] / 1000) * 0.2 + \n",
    "    real_estate_features['garage'] * 0.1\n",
    ")\n",
    "\n",
    "# Create categorical features\n",
    "real_estate_features['age_category'] = pd.cut(\n",
    "    real_estate_features['age'], \n",
    "    bins=[0, 5, 15, 30, 50], \n",
    "    labels=['New', 'Recent', 'Established', 'Old']\n",
    ")\n",
    "\n",
    "real_estate_features['size_category'] = pd.cut(\n",
    "    real_estate_features['sqft'], \n",
    "    bins=[0, 1200, 2000, 3000, 5000], \n",
    "    labels=['Small', 'Medium', 'Large', 'Luxury']\n",
    ")\n",
    "\n",
    "# Save feature-engineered data\n",
    "save_data(real_estate_features, f'../data/features/{timestamp}_real_estate_features.csv', format='csv')\n",
    "\n",
    "# Create train/test splits and save them\n",
    "X = real_estate_features[['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                         'distance_to_center', 'garage', 'school_rating']].values\n",
    "y = real_estate_features['price'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save splits for reproducibility\n",
    "train_data = pd.DataFrame(X_train, columns=['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                                           'distance_to_center', 'garage', 'school_rating'])\n",
    "train_data['price'] = y_train\n",
    "\n",
    "test_data = pd.DataFrame(X_test, columns=['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                                         'distance_to_center', 'garage', 'school_rating'])\n",
    "test_data['price'] = y_test\n",
    "\n",
    "save_data(train_data, f'../data/processed/{timestamp}_real_estate_train.csv', format='csv')\n",
    "save_data(test_data, f'../data/processed/{timestamp}_real_estate_test.csv', format='csv')\n",
    "\n",
    "# Create metadata file for the dataset\n",
    "metadata = {\n",
    "    'dataset_name': 'Real Estate Market Data',\n",
    "    'creation_date': timestamp,\n",
    "    'source': 'Simulated data based on realistic market patterns',\n",
    "    'features': {\n",
    "        'sqft': 'Square footage of the property',\n",
    "        'bedrooms': 'Number of bedrooms',\n",
    "        'bathrooms': 'Number of bathrooms',\n",
    "        'age': 'Age of the property in years',\n",
    "        'location_score': 'Neighborhood desirability score (1-10)',\n",
    "        'distance_to_center': 'Distance to city center in miles',\n",
    "        'garage': 'Has garage (0/1)',\n",
    "        'school_rating': 'Local school quality rating (1-10)',\n",
    "        'price': 'Property price in USD (target variable)',\n",
    "        'price_per_sqft': 'Price per square foot',\n",
    "        'total_rooms': 'Total bedrooms + bathrooms',\n",
    "        'luxury_score': 'Computed luxury index',\n",
    "        'age_category': 'Categorical age grouping',\n",
    "        'size_category': 'Categorical size grouping'\n",
    "    },\n",
    "    'statistics': {\n",
    "        'n_samples': len(real_estate_data),\n",
    "        'n_features': len(real_estate_features.columns),\n",
    "        'price_range': {\n",
    "            'min': float(real_estate_data['price'].min()),\n",
    "            'max': float(real_estate_data['price'].max()),\n",
    "            'mean': float(real_estate_data['price'].mean()),\n",
    "            'std': float(real_estate_data['price'].std())\n",
    "        }\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'missing_values': real_estate_features.isnull().sum().to_dict(),\n",
    "        'duplicates': int(real_estate_features.duplicated().sum()),\n",
    "        'outliers_detected': 'See EDA visualizations for outlier analysis'\n",
    "    },\n",
    "    'usage_notes': [\n",
    "        'Data is simulated for educational purposes',\n",
    "        'Relationships between features are based on real market patterns',\n",
    "        'Suitable for regression analysis and machine learning practice',\n",
    "        'Train/test splits provided for reproducible experiments'\n",
    "    ]\n",
    "}\n",
    "\n",
    "save_data(metadata, f'../data/processed/{timestamp}_real_estate_metadata.json', format='json')\n",
    "\n",
    "print(\"=== DATA SAVED SUCCESSFULLY ===\")\n",
    "print(f\"Timestamp: {timestamp}\")\n",
    "print(\"\\\\nFiles saved:\")\n",
    "print(f\"  Raw data: ../data/raw/{timestamp}_real_estate_raw.csv\")\n",
    "print(f\"  Features: ../data/features/{timestamp}_real_estate_features.csv\") \n",
    "print(f\"  Training: ../data/processed/{timestamp}_real_estate_train.csv\")\n",
    "print(f\"  Testing:  ../data/processed/{timestamp}_real_estate_test.csv\")\n",
    "print(f\"  Metadata: ../data/processed/{timestamp}_real_estate_metadata.json\")\n",
    "\n",
    "print(f\"\\\\nDataset summary:\")\n",
    "print(f\"  Total samples: {len(real_estate_data):,}\")\n",
    "print(f\"  Training samples: {len(train_data):,}\")\n",
    "print(f\"  Testing samples: {len(test_data):,}\")\n",
    "print(f\"  Features: {len(real_estate_features.columns)}\")\n",
    "print(f\"  Price range: ${real_estate_data['price'].min():,} - ${real_estate_data['price'].max():,}\")\n",
    "\n",
    "print(\"\\\\nüìÅ Data organization follows best practices:\")\n",
    "print(\"  ‚úÖ Raw data preserved in /raw directory\")\n",
    "print(\"  ‚úÖ Processed data in /processed directory\") \n",
    "print(\"  ‚úÖ Feature-engineered data in /features directory\")\n",
    "print(\"  ‚úÖ Comprehensive metadata documentation\")\n",
    "print(\"  ‚úÖ Reproducible train/test splits\")\n",
    "print(\"  ‚úÖ Timestamped for version control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4041da",
   "metadata": {},
   "source": [
    "## Practical Implementation: House Price Prediction\n",
    "\n",
    "### Problem Overview\n",
    "\n",
    "Linear regression is one of the fundamental algorithms in machine learning for supervised learning tasks. This notebook demonstrates end-to-end implementation of linear regression for predicting house prices based on various features.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the mathematical foundation of linear regression\n",
    "- Implement data preprocessing and feature engineering\n",
    "- Train and evaluate linear regression models\n",
    "- Visualize model performance and residual analysis\n",
    "- Handle real-world data challenges\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Linear regression assumes a linear relationship between input features and target variable:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the target variable (house price)\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_i$ are the coefficients for features $x_i$\n",
    "- $\\epsilon$ is the error term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f31a5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We start by importing all necessary libraries for data manipulation, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad5e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core data manipulation and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import make_regression, fetch_california_housing\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    STATSMODELS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    STATSMODELS_AVAILABLE = False\n",
    "    print(\"‚ö† statsmodels not available in build environment\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccbd12",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Loading\n",
    "\n",
    "We'll work with both synthetic and real datasets to understand different aspects of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb13633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic dataset with 1000 samples\n",
      "Features: ['house_size_sqft', 'bedrooms', 'bathrooms', 'age_years', 'distance_to_center_km', 'school_rating']\n",
      "Target: price\n",
      "\n",
      "Dataset Statistics:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "house_size_sqft",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bedrooms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bathrooms",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "age_years",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "distance_to_center_km",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "school_rating",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "price",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "25fa0112-e175-4709-8c13-b085114246c2",
       "rows": [
        [
         "count",
         "1000.0",
         "1000.0",
         "1000.0",
         "1000.0",
         "1000.0",
         "1000.0",
         "1000.0"
        ],
        [
         "mean",
         "2009.7866615811972",
         "3.934",
         "2.4860733045453047",
         "14.255977958874675",
         "9.984577010123624",
         "5.044476834121395",
         "395781.83374926"
        ],
        [
         "std",
         "489.22058835987207",
         "1.5931673428055768",
         "0.7657449506055135",
         "14.054194992439946",
         "7.011240244511961",
         "2.2217210411835193",
         "76978.74738486647"
        ],
        [
         "min",
         "500.0",
         "1.0",
         "1.0",
         "0.0036155059714595063",
         "1.0",
         "0.12182116156484679",
         "109233.33178176117"
        ],
        [
         "25%",
         "1676.2048472688243",
         "3.0",
         "1.956185561070312",
         "4.21035277331276",
         "4.7723676549580585",
         "3.295527263879647",
         "346470.12904501765"
        ],
        [
         "50%",
         "2012.650306117444",
         "4.0",
         "2.4983061580345347",
         "9.726281588504758",
         "8.227946586024007",
         "4.974233196103191",
         "399092.1453874729"
        ],
        [
         "75%",
         "2323.971937736465",
         "5.0",
         "3.021849758601474",
         "19.730957368094778",
         "13.71976881527236",
         "6.852298103346282",
         "448890.3796449704"
        ],
        [
         "max",
         "3926.365745327361",
         "7.0",
         "4.990328160835073",
         "100.0",
         "50.0",
         "9.833552231494082",
         "618104.0303714689"
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_size_sqft</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>age_years</th>\n",
       "      <th>distance_to_center_km</th>\n",
       "      <th>school_rating</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2009.786662</td>\n",
       "      <td>3.934000</td>\n",
       "      <td>2.486073</td>\n",
       "      <td>14.255978</td>\n",
       "      <td>9.984577</td>\n",
       "      <td>5.044477</td>\n",
       "      <td>395781.833749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>489.220588</td>\n",
       "      <td>1.593167</td>\n",
       "      <td>0.765745</td>\n",
       "      <td>14.054195</td>\n",
       "      <td>7.011240</td>\n",
       "      <td>2.221721</td>\n",
       "      <td>76978.747385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.121821</td>\n",
       "      <td>109233.331782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1676.204847</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.956186</td>\n",
       "      <td>4.210353</td>\n",
       "      <td>4.772368</td>\n",
       "      <td>3.295527</td>\n",
       "      <td>346470.129045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2012.650306</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.498306</td>\n",
       "      <td>9.726282</td>\n",
       "      <td>8.227947</td>\n",
       "      <td>4.974233</td>\n",
       "      <td>399092.145387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2323.971938</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.021850</td>\n",
       "      <td>19.730957</td>\n",
       "      <td>13.719769</td>\n",
       "      <td>6.852298</td>\n",
       "      <td>448890.379645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3926.365745</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.990328</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>9.833552</td>\n",
       "      <td>618104.030371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       house_size_sqft     bedrooms    bathrooms    age_years  \\\n",
       "count      1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean       2009.786662     3.934000     2.486073    14.255978   \n",
       "std         489.220588     1.593167     0.765745    14.054195   \n",
       "min         500.000000     1.000000     1.000000     0.003616   \n",
       "25%        1676.204847     3.000000     1.956186     4.210353   \n",
       "50%        2012.650306     4.000000     2.498306     9.726282   \n",
       "75%        2323.971938     5.000000     3.021850    19.730957   \n",
       "max        3926.365745     7.000000     4.990328   100.000000   \n",
       "\n",
       "       distance_to_center_km  school_rating          price  \n",
       "count            1000.000000    1000.000000    1000.000000  \n",
       "mean                9.984577       5.044477  395781.833749  \n",
       "std                 7.011240       2.221721   76978.747385  \n",
       "min                 1.000000       0.121821  109233.331782  \n",
       "25%                 4.772368       3.295527  346470.129045  \n",
       "50%                 8.227947       4.974233  399092.145387  \n",
       "75%                13.719769       6.852298  448890.379645  \n",
       "max                50.000000       9.833552  618104.030371  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_house_price_data(n_samples: int = 1000, noise_level: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic house price dataset with realistic features.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        noise_level: Amount of noise to add to the target variable\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with house features and prices\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate realistic house features\n",
    "    house_size = np.random.normal(2000, 500, n_samples)  # Square feet\n",
    "    house_size = np.clip(house_size, 500, 5000)  # Reasonable bounds\n",
    "    \n",
    "    bedrooms = np.random.poisson(3, n_samples) + 1  # 1-7 bedrooms typically\n",
    "    bedrooms = np.clip(bedrooms, 1, 7)\n",
    "    \n",
    "    bathrooms = np.random.normal(2.5, 0.8, n_samples)  # 1-5 bathrooms\n",
    "    bathrooms = np.clip(bathrooms, 1, 5)\n",
    "    \n",
    "    age = np.random.exponential(15, n_samples)  # House age in years\n",
    "    age = np.clip(age, 0, 100)\n",
    "    \n",
    "    # Distance to city center (km)\n",
    "    distance_to_center = np.random.gamma(2, 5, n_samples)\n",
    "    distance_to_center = np.clip(distance_to_center, 1, 50)\n",
    "    \n",
    "    # School rating (1-10)\n",
    "    school_rating = np.random.beta(2, 2, n_samples) * 10\n",
    "    \n",
    "    # Generate price based on realistic relationships\n",
    "    base_price = (\n",
    "        100 * house_size +  # $100 per sq ft\n",
    "        15000 * bedrooms +   # $15k per bedroom\n",
    "        20000 * bathrooms +  # $20k per bathroom\n",
    "        -2000 * age +        # Depreciation\n",
    "        -1000 * distance_to_center +  # Location premium\n",
    "        5000 * school_rating +  # School district premium\n",
    "        100000  # Base price\n",
    "    )\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, noise_level * np.mean(base_price), n_samples)\n",
    "    price = base_price + noise\n",
    "    \n",
    "    # Ensure positive prices\n",
    "    price = np.maximum(price, 50000)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'house_size_sqft': house_size,\n",
    "        'bedrooms': bedrooms,\n",
    "        'bathrooms': bathrooms,\n",
    "        'age_years': age,\n",
    "        'distance_to_center_km': distance_to_center,\n",
    "        'school_rating': school_rating,\n",
    "        'price': price\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic dataset\n",
    "df_synthetic = generate_house_price_data(n_samples=1000)\n",
    "print(f\"Generated synthetic dataset with {len(df_synthetic)} samples\")\n",
    "print(f\"Features: {list(df_synthetic.columns[:-1])}\")\n",
    "print(f\"Target: {df_synthetic.columns[-1]}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "df_synthetic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1073b5",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Understanding our data is crucial before building models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "910d0e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPLORATORY DATA ANALYSIS ===\n",
      "\n",
      "Dataset shape: (1000, 7)\n",
      "Missing values: 0\n",
      "\n",
      "Data types:\n",
      "house_size_sqft          float64\n",
      "bedrooms                   int32\n",
      "bathrooms                float64\n",
      "age_years                float64\n",
      "distance_to_center_km    float64\n",
      "school_rating            float64\n",
      "price                    float64\n",
      "dtype: object\n",
      "\n",
      "Correlation with price:\n",
      "house_size_sqft: 0.638\n",
      "bedrooms: 0.338\n",
      "bathrooms: 0.217\n",
      "school_rating: 0.137\n",
      "distance_to_center_km: -0.062\n",
      "age_years: -0.378\n",
      "\n",
      "Correlation with price:\n",
      "house_size_sqft: 0.638\n",
      "bedrooms: 0.338\n",
      "bathrooms: 0.217\n",
      "school_rating: 0.137\n",
      "distance_to_center_km: -0.062\n",
      "age_years: -0.378\n"
     ]
    }
   ],
   "source": [
    "def perform_eda(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Perform comprehensive exploratory data analysis.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Target variable distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Price distribution\n",
    "    axes[0, 0].hist(df['price'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Price Distribution')\n",
    "    axes[0, 0].set_xlabel('Price ($)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log price distribution (often more normal)\n",
    "    axes[0, 1].hist(np.log(df['price']), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].set_title('Log Price Distribution')\n",
    "    axes[0, 1].set_xlabel('Log Price')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Price vs house size (most important feature)\n",
    "    axes[1, 0].scatter(df['house_size_sqft'], df['price'], alpha=0.6, color='coral')\n",
    "    axes[1, 0].set_title('Price vs House Size')\n",
    "    axes[1, 0].set_xlabel('House Size (sq ft)')\n",
    "    axes[1, 0].set_ylabel('Price ($)')\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    corr_matrix = df.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation with target\n",
    "    print(\"\\nCorrelation with price:\")\n",
    "    price_corr = df.corr()['price'].sort_values(ascending=False)\n",
    "    for feature, corr in price_corr.items():\n",
    "        if feature != 'price':\n",
    "            print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Perform EDA\n",
    "perform_eda(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dd3dd",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Prepare the data for machine learning by handling missing values, scaling features, and splitting into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80511aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 800 samples\n",
      "Test set size: 200 samples\n",
      "Number of features: 6\n",
      "\n",
      "Feature scaling statistics (training set):\n",
      "Feature means: [2012.88371344    3.955         2.48942252   14.31938516    9.97330959\n",
      "    5.03925761]\n",
      "Feature std: [489.37270141   1.6118235    0.76210732  14.05637761   7.00765178\n",
      "   2.23955805]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(df: pd.DataFrame, target_col: str = 'price', \n",
    "                   test_size: float = 0.2, scale_features: bool = True) -> Tuple:\n",
    "    \"\"\"Preprocess data for machine learning.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion for test set\n",
    "        scale_features: Whether to standardize features\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (X_train, X_test, y_train, y_test, scaler, feature_names)\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features if requested\n",
    "    scaler = None\n",
    "    if scale_features:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_names\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test, scaler, feature_names = preprocess_data(df_synthetic)\n",
    "\n",
    "print(\"\\nFeature scaling statistics (training set):\")\n",
    "if scaler is not None:\n",
    "    print(f\"Feature means: {scaler.mean_}\")\n",
    "    print(f\"Feature std: {scaler.scale_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7f2cc",
   "metadata": {},
   "source": [
    "## 5. Model Implementation and Training\n",
    "\n",
    "Implement different variants of linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d1d47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models...\n",
      "\n",
      "Training Linear Regression...\n",
      "Intercept: 395953.29\n",
      "Coefficients: [ 48042.82067363  24583.04306653  15755.94194106 -29052.66483625\n",
      "  -6376.93057469   9877.1970925 ]\n",
      "\n",
      "Training Ridge Regression...\n",
      "Intercept: 395953.29\n",
      "Coefficients: [ 47984.1552669   24553.72290397  15735.32443207 -29017.5742138\n",
      "  -6367.76558269   9864.46592611]\n",
      "\n",
      "Training Lasso Regression...\n",
      "Intercept: 395953.29\n",
      "Coefficients: [ 48041.85122867  24582.02362752  15754.84319599 -29051.7248807\n",
      "  -6375.86264858   9876.14377507]\n"
     ]
    }
   ],
   "source": [
    "def train_linear_models(X_train: np.ndarray, y_train: np.ndarray) -> dict:\n",
    "    \"\"\"Train multiple linear regression variants.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trained models\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=1.0),\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    \n",
    "    print(\"Training models...\")\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Print model coefficients\n",
    "        if hasattr(model, 'coef_'):\n",
    "            print(f\"Intercept: {model.intercept_:.2f}\")\n",
    "            print(f\"Coefficients: {model.coef_}\")\n",
    "            \n",
    "    return trained_models\n",
    "\n",
    "# Train models\n",
    "models = train_linear_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827dc5c",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate model performance using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749591d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Results:\n",
      "               Model  Train R¬≤  Test R¬≤  Train RMSE   Test RMSE   Train MAE  \\\n",
      "0  Linear Regression    0.7147   0.7259  41096.2198  40268.4857  32950.3157   \n",
      "1   Ridge Regression    0.7147   0.7259  41096.2968  40270.8978  32951.6164   \n",
      "2   Lasso Regression    0.7147   0.7259  41096.2199  40268.5797  32950.3009   \n",
      "\n",
      "     Test MAE  CV R¬≤ Mean  CV R¬≤ Std  \n",
      "0  32710.5386      0.7006     0.0473  \n",
      "1  32717.1688      0.7006     0.0472  \n",
      "2  32710.7581      0.7006     0.0473  \n",
      "\n",
      "Best performing model: Linear Regression\n"
     ]
    }
   ],
   "source": [
    "def evaluate_regression_models(models: dict, X_train: np.ndarray, X_test: np.ndarray,\n",
    "                             y_train: np.ndarray, y_test: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate regression models comprehensively.\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of trained models\n",
    "        X_train, X_test: Training and test features\n",
    "        y_train, y_test: Training and test targets\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Make predictions\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        \n",
    "        train_mse = mean_squared_error(y_train, train_pred)\n",
    "        test_mse = mean_squared_error(y_test, test_pred)\n",
    "        \n",
    "        train_mae = mean_absolute_error(y_train, train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Train R¬≤': train_r2,\n",
    "            'Test R¬≤': test_r2,\n",
    "            'Train RMSE': np.sqrt(train_mse),\n",
    "            'Test RMSE': np.sqrt(test_mse),\n",
    "            'Train MAE': train_mae,\n",
    "            'Test MAE': test_mae,\n",
    "            'CV R¬≤ Mean': cv_scores.mean(),\n",
    "            'CV R¬≤ Std': cv_scores.std()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_results = evaluate_regression_models(models, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(evaluation_results.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = evaluation_results.loc[evaluation_results['Test R¬≤'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc37f2",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation and Feature Importance\n",
    "\n",
    "Understand which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baaedcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance (by coefficient magnitude):\n",
      "                 Feature   Coefficient  Abs_Coefficient\n",
      "0        house_size_sqft  48042.820674     48042.820674\n",
      "3              age_years -29052.664836     29052.664836\n",
      "1               bedrooms  24583.043067     24583.043067\n",
      "2              bathrooms  15755.941941     15755.941941\n",
      "5          school_rating   9877.197092      9877.197092\n",
      "4  distance_to_center_km  -6376.930575      6376.930575\n"
     ]
    }
   ],
   "source": [
    "def analyze_feature_importance(model, feature_names: List[str]) -> None:\n",
    "    \"\"\"Analyze and visualize feature importance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained linear model\n",
    "        feature_names: Names of features\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coefficients = model.coef_\n",
    "        \n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': coefficients,\n",
    "            'Abs_Coefficient': np.abs(coefficients)\n",
    "        }).sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        print(\"Feature Importance (by coefficient magnitude):\")\n",
    "        print(importance_df)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Coefficient values\n",
    "        plt.subplot(2, 1, 1)\n",
    "        colors = ['red' if coef < 0 else 'green' for coef in importance_df['Coefficient']]\n",
    "        plt.barh(importance_df['Feature'], importance_df['Coefficient'], color=colors, alpha=0.7)\n",
    "        plt.title('Feature Coefficients (Positive = Increases Price, Negative = Decreases Price)')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Absolute coefficient values\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.barh(importance_df['Feature'], importance_df['Abs_Coefficient'], \n",
    "                color='skyblue', alpha=0.7)\n",
    "        plt.title('Feature Importance (Absolute Coefficient Values)')\n",
    "        plt.xlabel('Absolute Coefficient Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"Model does not have coefficients to analyze.\")\n",
    "        return None\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(best_model, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ba55d",
   "metadata": {},
   "source": [
    "## 8. Residual Analysis\n",
    "\n",
    "Analyze model residuals to check assumptions and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1146a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Analysis Summary:\n",
      "Mean residual: 1891.484526 (should be close to 0)\n",
      "Std residual: 40224.04\n",
      "Shapiro-Wilk test p-value: 0.450616\n",
      "‚úì Residuals appear to be normally distributed\n",
      "‚ö† Durbin-Watson test not available (statsmodels version compatibility)\n",
      "Simple lag-1 autocorrelation: 0.026 (close to 0 indicates no autocorrelation)\n"
     ]
    }
   ],
   "source": [
    "def perform_residual_analysis(model, X_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "    \"\"\"Perform comprehensive residual analysis.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: Test targets\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Predicted vs Actual\n",
    "    axes[0, 0].scatter(y_test, y_pred, alpha=0.6, color='blue')\n",
    "    axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                   'r--', lw=2, label='Perfect Prediction')\n",
    "    axes[0, 0].set_xlabel('Actual Values')\n",
    "    axes[0, 0].set_ylabel('Predicted Values')\n",
    "    axes[0, 0].set_title('Predicted vs Actual Values')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Residuals vs Predicted\n",
    "    axes[0, 1].scatter(y_pred, residuals, alpha=0.6, color='green')\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted Values')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residuals vs Predicted Values')\n",
    "    \n",
    "    # 3. Residual distribution\n",
    "    axes[1, 0].hist(residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residual Distribution')\n",
    "    \n",
    "    # 4. Q-Q plot for normality check\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"Residual Analysis Summary:\")\n",
    "    print(f\"Mean residual: {np.mean(residuals):.6f} (should be close to 0)\")\n",
    "    print(f\"Std residual: {np.std(residuals):.2f}\")\n",
    "    \n",
    "    # Shapiro-Wilk test for normality\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(residuals[:5000])  # Limit sample size\n",
    "    print(f\"Shapiro-Wilk test p-value: {shapiro_p:.6f}\")\n",
    "    if shapiro_p > 0.05:\n",
    "        print(\"‚úì Residuals appear to be normally distributed\")\n",
    "    else:\n",
    "        print(\"‚úó Residuals may not be normally distributed\")\n",
    "    \n",
    "    # Durbin-Watson test for autocorrelation (with fallback)\n",
    "    try:\n",
    "        from statsmodels.stats.diagnostic import durbin_watson\n",
    "        dw_stat = durbin_watson(residuals)\n",
    "        print(f\"Durbin-Watson statistic: {dw_stat:.3f} (2.0 indicates no autocorrelation)\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† Durbin-Watson test not available (statsmodels version compatibility)\")\n",
    "        # Simple autocorrelation check as fallback\n",
    "        if len(residuals) > 1:\n",
    "            autocorr = np.corrcoef(residuals[:-1], residuals[1:])[0, 1]\n",
    "            print(f\"Simple lag-1 autocorrelation: {autocorr:.3f} (close to 0 indicates no autocorrelation)\")\n",
    "\n",
    "# Perform residual analysis\n",
    "perform_residual_analysis(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201d39a",
   "metadata": {},
   "source": [
    "## 9. Polynomial Regression Extension\n",
    "\n",
    "Explore polynomial features to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88b63b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Polynomial Regression (degree 2)...\n",
      "Original features: 6\n",
      "Polynomial features: 27\n",
      "\n",
      "Training Polynomial Regression (degree 3)...\n",
      "Original features: 6\n",
      "Polynomial features: 83\n",
      "\n",
      "Polynomial Regression Results:\n",
      "   Degree  Train R¬≤  Test R¬≤  Train RMSE   Test RMSE  Features\n",
      "0       2    0.7213   0.7316  40617.7023  39849.8451        27\n",
      "1       3    0.7362   0.7169  39518.1535  40928.3669        83\n"
     ]
    }
   ],
   "source": [
    "def train_polynomial_regression(X_train: np.ndarray, X_test: np.ndarray, \n",
    "                              y_train: np.ndarray, y_test: np.ndarray,\n",
    "                              degrees: List[int] = [2, 3]) -> dict:\n",
    "    \"\"\"Train polynomial regression models.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Training and test features\n",
    "        y_train, y_test: Training and test targets\n",
    "        degrees: List of polynomial degrees to try\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with polynomial models and results\n",
    "    \"\"\"\n",
    "    poly_results = []\n",
    "    poly_models = {}\n",
    "    \n",
    "    for degree in degrees:\n",
    "        print(f\"\\nTraining Polynomial Regression (degree {degree})...\")\n",
    "        \n",
    "        # Create polynomial features\n",
    "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        X_train_poly = poly_features.fit_transform(X_train)\n",
    "        X_test_poly = poly_features.transform(X_test)\n",
    "        \n",
    "        print(f\"Original features: {X_train.shape[1]}\")\n",
    "        print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "        \n",
    "        # Train model with regularization to prevent overfitting\n",
    "        model = Ridge(alpha=1.0)  # Use Ridge to handle high-dimensional polynomial features\n",
    "        model.fit(X_train_poly, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_pred = model.predict(X_train_poly)\n",
    "        test_pred = model.predict(X_test_poly)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        \n",
    "        poly_results.append({\n",
    "            'Degree': degree,\n",
    "            'Train R¬≤': train_r2,\n",
    "            'Test R¬≤': test_r2,\n",
    "            'Train RMSE': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            'Test RMSE': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            'Features': X_train_poly.shape[1]\n",
    "        })\n",
    "        \n",
    "        poly_models[f'Poly_{degree}'] = {\n",
    "            'model': model,\n",
    "            'poly_features': poly_features\n",
    "        }\n",
    "    \n",
    "    # Display results\n",
    "    poly_df = pd.DataFrame(poly_results)\n",
    "    print(\"\\nPolynomial Regression Results:\")\n",
    "    print(poly_df.round(4))\n",
    "    \n",
    "    # Plot complexity vs performance\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(poly_df['Degree'], poly_df['Train R¬≤'], 'o-', label='Training R¬≤', color='blue')\n",
    "    plt.plot(poly_df['Degree'], poly_df['Test R¬≤'], 'o-', label='Test R¬≤', color='red')\n",
    "    plt.xlabel('Polynomial Degree')\n",
    "    plt.ylabel('R¬≤ Score')\n",
    "    plt.title('Model Performance vs Polynomial Degree')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(poly_df['Features'], poly_df['Test R¬≤'], 'o-', color='green')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Test R¬≤ Score')\n",
    "    plt.title('Test Performance vs Model Complexity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return poly_models, poly_df\n",
    "\n",
    "# Train polynomial regression models\n",
    "poly_models, poly_results_df = train_polynomial_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b1850",
   "metadata": {},
   "source": [
    "## 10. Model Comparison and Selection\n",
    "\n",
    "Compare all models and select the best one based on multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c4ab92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL COMPARISON SUMMARY ===\n",
      "\n",
      "1. Linear Models:\n",
      "               Model  Test R¬≤   Test RMSE  CV R¬≤ Mean\n",
      "0  Linear Regression   0.7259  40268.4857      0.7006\n",
      "1   Ridge Regression   0.7259  40270.8978      0.7006\n",
      "2   Lasso Regression   0.7259  40268.5797      0.7006\n",
      "\n",
      "2. Polynomial Models:\n",
      "   Degree  Test R¬≤   Test RMSE  Features\n",
      "0       2   0.7316  39849.8451        27\n",
      "1       3   0.7169  40928.3669        83\n",
      "\n",
      "=== MODEL SELECTION CRITERIA ===\n",
      "\n",
      "Best Linear Model: Linear Regression\n",
      "  - Test R¬≤: 0.7259\n",
      "  - Test RMSE: 40268.49\n",
      "  - CV R¬≤ Mean: 0.7006\n",
      "\n",
      "Best Polynomial Model: Degree 2.0\n",
      "  - Test R¬≤: 0.7316\n",
      "  - Test RMSE: 39849.85\n",
      "  - Features: 27.0\n",
      "\n",
      "=== RECOMMENDATION ===\n",
      "Recommended Model: Linear Regression\n",
      "Reason: Simpler model with comparable performance (Occam's Razor)\n"
     ]
    }
   ],
   "source": [
    "def compare_all_models() -> None:\n",
    "    \"\"\"Compare all trained models and provide recommendations.\"\"\"\n",
    "    print(\"=== MODEL COMPARISON SUMMARY ===\")\n",
    "    \n",
    "    # Combine linear and polynomial results\n",
    "    print(\"\\n1. Linear Models:\")\n",
    "    print(evaluation_results[['Model', 'Test R¬≤', 'Test RMSE', 'CV R¬≤ Mean']].round(4))\n",
    "    \n",
    "    print(\"\\n2. Polynomial Models:\")\n",
    "    print(poly_results_df[['Degree', 'Test R¬≤', 'Test RMSE', 'Features']].round(4))\n",
    "    \n",
    "    # Model selection criteria\n",
    "    print(\"\\n=== MODEL SELECTION CRITERIA ===\")\n",
    "    \n",
    "    # Best linear model\n",
    "    best_linear_idx = evaluation_results['Test R¬≤'].idxmax()\n",
    "    best_linear = evaluation_results.iloc[best_linear_idx]\n",
    "    print(f\"\\nBest Linear Model: {best_linear['Model']}\")\n",
    "    print(f\"  - Test R¬≤: {best_linear['Test R¬≤']:.4f}\")\n",
    "    print(f\"  - Test RMSE: {best_linear['Test RMSE']:.2f}\")\n",
    "    print(f\"  - CV R¬≤ Mean: {best_linear['CV R¬≤ Mean']:.4f}\")\n",
    "    \n",
    "    # Best polynomial model\n",
    "    best_poly_idx = poly_results_df['Test R¬≤'].idxmax()\n",
    "    best_poly = poly_results_df.iloc[best_poly_idx]\n",
    "    print(f\"\\nBest Polynomial Model: Degree {best_poly['Degree']}\")\n",
    "    print(f\"  - Test R¬≤: {best_poly['Test R¬≤']:.4f}\")\n",
    "    print(f\"  - Test RMSE: {best_poly['Test RMSE']:.2f}\")\n",
    "    print(f\"  - Features: {best_poly['Features']}\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    print(\"\\n=== RECOMMENDATION ===\")\n",
    "    if best_linear['Test R¬≤'] > best_poly['Test R¬≤'] - 0.01:  # Small tolerance\n",
    "        print(f\"Recommended Model: {best_linear['Model']}\")\n",
    "        print(\"Reason: Simpler model with comparable performance (Occam's Razor)\")\n",
    "    else:\n",
    "        print(f\"Recommended Model: Polynomial Degree {best_poly['Degree']}\")\n",
    "        print(\"Reason: Significantly better performance justifies complexity\")\n",
    "    \n",
    "    # Visualization of all models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    all_models = []\n",
    "    all_r2 = []\n",
    "    all_rmse = []\n",
    "    \n",
    "    # Add linear models\n",
    "    for _, row in evaluation_results.iterrows():\n",
    "        all_models.append(row['Model'])\n",
    "        all_r2.append(row['Test R¬≤'])\n",
    "        all_rmse.append(row['Test RMSE'])\n",
    "    \n",
    "    # Add polynomial models\n",
    "    for _, row in poly_results_df.iterrows():\n",
    "        all_models.append(f\"Poly Deg {row['Degree']}\")\n",
    "        all_r2.append(row['Test R¬≤'])\n",
    "        all_rmse.append(row['Test RMSE'])\n",
    "    \n",
    "    # Plot R¬≤ scores\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(all_models)))\n",
    "    bars = plt.bar(range(len(all_models)), all_r2, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Test R¬≤ Score')\n",
    "    plt.title('Model Comparison: R¬≤ Scores')\n",
    "    plt.xticks(range(len(all_models)), all_models, rotation=45, ha='right')\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_idx = np.argmax(all_r2)\n",
    "    bars[best_idx].set_color('red')\n",
    "    bars[best_idx].set_alpha(1.0)\n",
    "    \n",
    "    # Plot RMSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars = plt.bar(range(len(all_models)), all_rmse, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Test RMSE')\n",
    "    plt.title('Model Comparison: RMSE (Lower is Better)')\n",
    "    plt.xticks(range(len(all_models)), all_models, rotation=45, ha='right')\n",
    "    \n",
    "    # Highlight best model (lowest RMSE)\n",
    "    best_rmse_idx = np.argmin(all_rmse)\n",
    "    bars[best_rmse_idx].set_color('red')\n",
    "    bars[best_rmse_idx].set_alpha(1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare all models\n",
    "compare_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c4045",
   "metadata": {},
   "source": [
    "## 11. Real-World Application: California Housing Dataset\n",
    "\n",
    "Apply our best model to a real dataset to validate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15d8fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REAL-WORLD VALIDATION ===\n",
      "California Housing Dataset:\n",
      "  - Samples: 20640\n",
      "  - Features: 8\n",
      "  - Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "  - Target: Median house value ($)\n",
      "\n",
      "Real Data Results:\n",
      "  - R¬≤ Score: 0.5758\n",
      "  - RMSE: $74,558.14\n",
      "  - MAE: $53,320.01\n",
      "\n",
      "Feature Importance (Real Data):\n",
      "      Feature   Coefficient  Abs_Coefficient\n",
      "6    Latitude -89692.887664     89692.887664\n",
      "7   Longitude -86984.177524     86984.177524\n",
      "0      MedInc  85438.303093     85438.303093\n",
      "3   AveBedrms  33925.949059     33925.949059\n",
      "2    AveRooms -29441.013447     29441.013447\n",
      "1    HouseAge  12254.623808     12254.623808\n",
      "5    AveOccup  -4082.910309      4082.910309\n",
      "4  Population   -230.772315       230.772315\n",
      "California Housing Dataset:\n",
      "  - Samples: 20640\n",
      "  - Features: 8\n",
      "  - Features: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "  - Target: Median house value ($)\n",
      "\n",
      "Real Data Results:\n",
      "  - R¬≤ Score: 0.5758\n",
      "  - RMSE: $74,558.14\n",
      "  - MAE: $53,320.01\n",
      "\n",
      "Feature Importance (Real Data):\n",
      "      Feature   Coefficient  Abs_Coefficient\n",
      "6    Latitude -89692.887664     89692.887664\n",
      "7   Longitude -86984.177524     86984.177524\n",
      "0      MedInc  85438.303093     85438.303093\n",
      "3   AveBedrms  33925.949059     33925.949059\n",
      "2    AveRooms -29441.013447     29441.013447\n",
      "1    HouseAge  12254.623808     12254.623808\n",
      "5    AveOccup  -4082.910309      4082.910309\n",
      "4  Population   -230.772315       230.772315\n"
     ]
    }
   ],
   "source": [
    "def apply_to_real_data() -> None:\n",
    "    \"\"\"Apply the best model to real California housing data.\"\"\"\n",
    "    print(\"=== REAL-WORLD VALIDATION ===\")\n",
    "    \n",
    "    # Load California housing dataset\n",
    "    california_data = fetch_california_housing()\n",
    "    X_real = california_data.data\n",
    "    y_real = california_data.target * 100000  # Convert to actual dollar values\n",
    "    feature_names_real = california_data.feature_names\n",
    "    \n",
    "    print(f\"California Housing Dataset:\")\n",
    "    print(f\"  - Samples: {X_real.shape[0]}\")\n",
    "    print(f\"  - Features: {X_real.shape[1]}\")\n",
    "    print(f\"  - Features: {feature_names_real}\")\n",
    "    print(f\"  - Target: Median house value ($)\")\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        X_real, y_real, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler_real = StandardScaler()\n",
    "    X_train_real_scaled = scaler_real.fit_transform(X_train_real)\n",
    "    X_test_real_scaled = scaler_real.transform(X_test_real)\n",
    "    \n",
    "    # Train our best model on real data\n",
    "    best_model_real = LinearRegression()  # Use the model type that performed best\n",
    "    best_model_real.fit(X_train_real_scaled, y_train_real)\n",
    "    \n",
    "    # Evaluate on real data\n",
    "    y_pred_real = best_model_real.predict(X_test_real_scaled)\n",
    "    \n",
    "    r2_real = r2_score(y_test_real, y_pred_real)\n",
    "    rmse_real = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    mae_real = mean_absolute_error(y_test_real, y_pred_real)\n",
    "    \n",
    "    print(f\"\\nReal Data Results:\")\n",
    "    print(f\"  - R¬≤ Score: {r2_real:.4f}\")\n",
    "    print(f\"  - RMSE: ${rmse_real:,.2f}\")\n",
    "    print(f\"  - MAE: ${mae_real:,.2f}\")\n",
    "    \n",
    "    # Feature importance on real data\n",
    "    importance_real = pd.DataFrame({\n",
    "        'Feature': feature_names_real,\n",
    "        'Coefficient': best_model_real.coef_,\n",
    "        'Abs_Coefficient': np.abs(best_model_real.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (Real Data):\")\n",
    "    print(importance_real)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_test_real, y_pred_real, alpha=0.5, color='blue')\n",
    "    plt.plot([y_test_real.min(), y_test_real.max()], \n",
    "             [y_test_real.min(), y_test_real.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Price ($)')\n",
    "    plt.ylabel('Predicted Price ($)')\n",
    "    plt.title(f'Real Data: Predicted vs Actual (R¬≤ = {r2_real:.3f})')\n",
    "    \n",
    "    # Residuals\n",
    "    residuals_real = y_test_real - y_pred_real\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_pred_real, residuals_real, alpha=0.5, color='green')\n",
    "    plt.axhline(y=0, color='red', linestyle='--')\n",
    "    plt.xlabel('Predicted Price ($)')\n",
    "    plt.ylabel('Residuals ($)')\n",
    "    plt.title('Residuals vs Predicted')\n",
    "    \n",
    "    # Feature importance\n",
    "    plt.subplot(2, 2, 3)\n",
    "    colors = ['red' if coef < 0 else 'green' for coef in importance_real['Coefficient']]\n",
    "    plt.barh(importance_real['Feature'], importance_real['Coefficient'], \n",
    "             color=colors, alpha=0.7)\n",
    "    plt.title('Feature Coefficients (Real Data)')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(residuals_real, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "    plt.xlabel('Residuals ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Residual Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply to real data\n",
    "apply_to_real_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c52fe",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Linear Regression Fundamentals**: We implemented linear regression from scratch and using scikit-learn, understanding the mathematical foundation and assumptions.\n",
    "\n",
    "2. **Data Preprocessing**: Proper data preprocessing including scaling, splitting, and handling different data types is crucial for model performance.\n",
    "\n",
    "3. **Model Variants**: We explored different variants:\n",
    "   - **Linear Regression**: Basic least squares solution\n",
    "   - **Ridge Regression**: L2 regularization to prevent overfitting\n",
    "   - **Lasso Regression**: L1 regularization for feature selection\n",
    "   - **Polynomial Regression**: Capturing non-linear relationships\n",
    "\n",
    "4. **Evaluation Metrics**: Multiple metrics provide different insights:\n",
    "   - **R¬≤**: Proportion of variance explained\n",
    "   - **RMSE**: Root mean squared error in original units\n",
    "   - **MAE**: Mean absolute error, robust to outliers\n",
    "\n",
    "5. **Model Interpretation**: Linear models are highly interpretable through coefficient analysis.\n",
    "\n",
    "6. **Residual Analysis**: Checking model assumptions through residual patterns.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always scale features** when using regularized models\n",
    "2. **Use cross-validation** for robust performance estimation\n",
    "3. **Analyze residuals** to validate model assumptions\n",
    "4. **Consider regularization** to prevent overfitting\n",
    "5. **Test on real data** to validate model generalization\n",
    "\n",
    "### When to Use Linear Regression\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Linear relationships between features and target\n",
    "- Interpretability is important\n",
    "- Baseline model for comparison\n",
    "- Small to medium datasets\n",
    "- When you need to understand feature importance\n",
    "\n",
    "‚ùå **Not ideal for:**\n",
    "- Highly non-linear relationships\n",
    "- Very high-dimensional data without regularization\n",
    "- When interpretability is not needed and accuracy is paramount\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Explore **ensemble methods** like Random Forest\n",
    "2. Try **non-linear models** like SVM or Neural Networks\n",
    "3. Implement **feature engineering** techniques\n",
    "4. Learn about **advanced regularization** methods\n",
    "5. Study **time series** regression for temporal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66c519",
   "metadata": {},
   "source": [
    "## 13. Practice Exercises\n",
    "\n",
    "### Exercise 1: Feature Engineering\n",
    "Create new features from existing ones (e.g., price per square foot, age categories) and see how they affect model performance.\n",
    "\n",
    "### Exercise 2: Outlier Detection\n",
    "Implement outlier detection and removal techniques and analyze their impact on model performance.\n",
    "\n",
    "### Exercise 3: Different Datasets\n",
    "Apply the same pipeline to other regression datasets (e.g., Boston housing, automobile prices).\n",
    "\n",
    "### Exercise 4: Hyperparameter Tuning\n",
    "Use GridSearchCV to find optimal regularization parameters for Ridge and Lasso regression.\n",
    "\n",
    "### Exercise 5: Advanced Metrics\n",
    "Implement additional evaluation metrics like MAPE (Mean Absolute Percentage Error) and analyze when each metric is most appropriate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
