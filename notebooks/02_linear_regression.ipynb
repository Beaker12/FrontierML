{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214d54b4",
   "metadata": {},
   "source": [
    "# Chapter 2: Linear Regression with Real Estate Data\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of linear regression with proper derivations\n",
    "- **Data collection** from real estate websites for regression analysis  \n",
    "- **Implementation** from scratch using NumPy\n",
    "- **Scikit-learn** implementation for comparison\n",
    "- **Model evaluation** and interpretation techniques\n",
    "- **Assumptions** and diagnostics for linear regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Linear regression is the foundation of predictive modeling and one of the most important algorithms in machine learning. It models the relationship between a dependent variable and independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "**Mathematical Foundation**: Linear regression finds the best-fitting line through data points by minimizing the sum of squared residuals, as originally formulated by Legendre (1805) and Gauss (1809) \\citep{stigler1981gauss}.\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### The Linear Model\n",
    "\n",
    "For a single predictor variable, the linear regression model is:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the dependent variable (response)\n",
    "- $x$ is the independent variable (predictor)  \n",
    "- $\\beta_0$ is the y-intercept\n",
    "- $\\beta_1$ is the slope coefficient\n",
    "- $\\epsilon$ is the error term\n",
    "\n",
    "For multiple predictors:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon$$\n",
    "\n",
    "Or in matrix form: $\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$\n",
    "\n",
    "**Citation**: The mathematical foundations are detailed in Hastie et al. (2009) \\citep{hastie2009elements} and James et al. (2013) \\citep{james2013introduction}.\n",
    "\n",
    "### Least Squares Estimation\n",
    "\n",
    "The ordinary least squares (OLS) estimator minimizes the residual sum of squares:\n",
    "\n",
    "$$RSS = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 x_i)^2$$\n",
    "\n",
    "The closed-form solution is:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "**Citation**: The derivation and properties of OLS are covered in Greene (2003) \\citep{greene2003econometric}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'utils'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom utilities\n",
    "from scraping_utils import WebScraper, clean_numeric_data, save_data\n",
    "from data_utils import load_data\n",
    "from plot_utils import create_regression_plots\n",
    "from evaluation_utils import regression_metrics\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74f50e",
   "metadata": {},
   "source": [
    "## Data Collection: Real Estate Prices\n",
    "\n",
    "For our linear regression analysis, we'll collect real estate data that naturally exhibits linear relationships between features like square footage, number of bedrooms, and price.\n",
    "\n",
    "**Real-World Application**: Housing price prediction is a classic application of linear regression in economics and real estate, as documented by Case & Shiller (1989) \\citep{case1989efficiency}.\n",
    "\n",
    "Since we need to be ethical about web scraping, we'll simulate realistic real estate data based on actual market patterns and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_real_estate_data(n_samples: int = 1000, random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate realistic real estate data for linear regression analysis.\n",
    "    \n",
    "    This function creates data that mimics patterns found in real housing markets,\n",
    "    with realistic relationships between features and prices.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int, default=1000\n",
    "        Number of properties to simulate\n",
    "    random_state : int, default=42\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Simulated real estate dataset with features and target price\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Generate base features with realistic distributions\n",
    "    # Square footage (log-normal distribution, typical range 800-4000 sqft)\n",
    "    sqft = np.random.lognormal(mean=7.2, sigma=0.4, size=n_samples)\n",
    "    sqft = np.clip(sqft, 800, 4000)\n",
    "    \n",
    "    # Number of bedrooms (correlated with square footage)\n",
    "    # Typical relationship: 1 bedroom per ~600 sqft, with some variation\n",
    "    bedrooms_base = sqft / 600 + np.random.normal(0, 0.5, n_samples)\n",
    "    bedrooms = np.clip(np.round(bedrooms_base), 1, 6).astype(int)\n",
    "    \n",
    "    # Number of bathrooms (correlated with bedrooms, typically 0.5-1 per bedroom)\n",
    "    bathrooms_base = bedrooms * 0.75 + np.random.normal(0, 0.3, n_samples)\n",
    "    bathrooms = np.clip(np.round(bathrooms_base * 2) / 2, 1, 4)  # Round to nearest 0.5\n",
    "    \n",
    "    # Property age (0-50 years, exponential decay in frequency)\n",
    "    age = np.random.exponential(scale=15, size=n_samples)\n",
    "    age = np.clip(age, 0, 50)\n",
    "    \n",
    "    # Location score (1-10, representing neighborhood desirability)\n",
    "    location_score = np.random.beta(2, 2, n_samples) * 9 + 1\n",
    "    \n",
    "    # Distance to city center (miles, affects price)\n",
    "    distance_to_center = np.random.gamma(2, 3, n_samples)\n",
    "    distance_to_center = np.clip(distance_to_center, 1, 30)\n",
    "    \n",
    "    # Garage (binary: 0 or 1)\n",
    "    garage_prob = 0.3 + 0.4 * (sqft - 800) / (4000 - 800)  # Larger homes more likely to have garage\n",
    "    garage = np.random.binomial(1, garage_prob, n_samples)\n",
    "    \n",
    "    # School rating (1-10, affects desirability)\n",
    "    school_rating = np.random.beta(3, 2, n_samples) * 9 + 1\n",
    "    \n",
    "    # Generate realistic price based on these features\n",
    "    # Base price equation with realistic coefficients\n",
    "    price_base = (\n",
    "        100 * sqft +                           # $100 per sqft\n",
    "        15000 * bedrooms +                     # $15k per bedroom\n",
    "        8000 * bathrooms +                     # $8k per bathroom  \n",
    "        -2000 * age +                          # -$2k per year of age\n",
    "        25000 * location_score +               # $25k per location point\n",
    "        -3000 * distance_to_center +           # -$3k per mile from center\n",
    "        20000 * garage +                       # $20k for garage\n",
    "        5000 * school_rating                   # $5k per school rating point\n",
    "    )\n",
    "    \n",
    "    # Add realistic noise (market fluctuations, unmeasured factors)\n",
    "    noise = np.random.normal(0, 0.1 * price_base, n_samples)\n",
    "    price = price_base + noise\n",
    "    \n",
    "    # Ensure prices are reasonable (minimum $50k)\n",
    "    price = np.maximum(price, 50000)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'sqft': np.round(sqft).astype(int),\n",
    "        'bedrooms': bedrooms,\n",
    "        'bathrooms': bathrooms,\n",
    "        'age': np.round(age, 1),\n",
    "        'location_score': np.round(location_score, 1),\n",
    "        'distance_to_center': np.round(distance_to_center, 1),\n",
    "        'garage': garage,\n",
    "        'school_rating': np.round(school_rating, 1),\n",
    "        'price': np.round(price).astype(int)\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"Generating realistic real estate dataset...\")\n",
    "real_estate_data = simulate_real_estate_data(n_samples=1000, random_state=42)\n",
    "\n",
    "print(f\"Dataset created with {len(real_estate_data)} properties\")\n",
    "print(\"\\nDataset Info:\")\n",
    "print(real_estate_data.info())\n",
    "print(\"\\nFirst 10 properties:\")\n",
    "print(real_estate_data.head(10))\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(real_estate_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed15dd1",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Before building our regression model, we need to understand the data distribution, relationships, and potential issues.\n",
    "\n",
    "**Citation**: The importance of exploratory data analysis is emphasized by Tukey (1977) \\citep{tukey1977exploratory} and Cleveland (1993) \\citep{cleveland1993visualizing}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive EDA visualizations\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "fig.suptitle('Real Estate Data: Exploratory Data Analysis', fontsize=20, fontweight='bold')\n",
    "\n",
    "# 1. Price distribution\n",
    "axes[0, 0].hist(real_estate_data['price'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Price Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Price ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics to the plot\n",
    "mean_price = real_estate_data['price'].mean()\n",
    "median_price = real_estate_data['price'].median()\n",
    "axes[0, 0].axvline(mean_price, color='red', linestyle='--', label=f'Mean: ${mean_price:,.0f}')\n",
    "axes[0, 0].axvline(median_price, color='green', linestyle='--', label=f'Median: ${median_price:,.0f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Square footage vs Price\n",
    "axes[0, 1].scatter(real_estate_data['sqft'], real_estate_data['price'], alpha=0.6, s=50, color='coral')\n",
    "axes[0, 1].set_title('Square Footage vs Price', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Square Footage')\n",
    "axes[0, 1].set_ylabel('Price ($)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_sqft_price = real_estate_data['sqft'].corr(real_estate_data['price'])\n",
    "axes[0, 1].text(0.05, 0.95, f'Correlation: {corr_sqft_price:.3f}', \n",
    "                transform=axes[0, 1].transAxes, fontsize=12, \n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# 3. Bedrooms distribution\n",
    "bedroom_counts = real_estate_data['bedrooms'].value_counts().sort_index()\n",
    "axes[0, 2].bar(bedroom_counts.index, bedroom_counts.values, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 2].set_title('Number of Bedrooms Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Number of Bedrooms')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Age vs Price\n",
    "axes[1, 0].scatter(real_estate_data['age'], real_estate_data['price'], alpha=0.6, s=50, color='purple')\n",
    "axes[1, 0].set_title('Property Age vs Price', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Age (years)')\n",
    "axes[1, 0].set_ylabel('Price ($)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_age_price = real_estate_data['age'].corr(real_estate_data['price'])\n",
    "axes[1, 0].text(0.05, 0.95, f'Correlation: {corr_age_price:.3f}', \n",
    "                transform=axes[1, 0].transAxes, fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# 5. Location score vs Price\n",
    "axes[1, 1].scatter(real_estate_data['location_score'], real_estate_data['price'], alpha=0.6, s=50, color='orange')\n",
    "axes[1, 1].set_title('Location Score vs Price', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Location Score (1-10)')\n",
    "axes[1, 1].set_ylabel('Price ($)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_location_price = real_estate_data['location_score'].corr(real_estate_data['price'])\n",
    "axes[1, 1].text(0.05, 0.95, f'Correlation: {corr_location_price:.3f}', \n",
    "                transform=axes[1, 1].transAxes, fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# 6. Garage effect on price\n",
    "garage_prices = [real_estate_data[real_estate_data['garage'] == 0]['price'],\n",
    "                 real_estate_data[real_estate_data['garage'] == 1]['price']]\n",
    "axes[1, 2].boxplot(garage_prices, labels=['No Garage', 'Has Garage'], patch_artist=True)\n",
    "axes[1, 2].set_title('Garage Effect on Price', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_ylabel('Price ($)')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add mean prices\n",
    "no_garage_mean = real_estate_data[real_estate_data['garage'] == 0]['price'].mean()\n",
    "garage_mean = real_estate_data[real_estate_data['garage'] == 1]['price'].mean()\n",
    "axes[1, 2].text(0.5, 0.95, f'No Garage: ${no_garage_mean:,.0f}\\\\nHas Garage: ${garage_mean:,.0f}', \n",
    "                transform=axes[1, 2].transAxes, fontsize=10, ha='center',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# 7. Correlation heatmap\n",
    "numeric_cols = ['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                'distance_to_center', 'garage', 'school_rating', 'price']\n",
    "correlation_matrix = real_estate_data[numeric_cols].corr()\n",
    "\n",
    "im = axes[2, 0].imshow(correlation_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[2, 0].set_xticks(range(len(correlation_matrix.columns)))\n",
    "axes[2, 0].set_yticks(range(len(correlation_matrix.columns)))\n",
    "axes[2, 0].set_xticklabels(correlation_matrix.columns, rotation=45, ha='right')\n",
    "axes[2, 0].set_yticklabels(correlation_matrix.columns)\n",
    "axes[2, 0].set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add correlation values as text\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix)):\n",
    "        text = axes[2, 0].text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontweight='bold', fontsize=9)\n",
    "\n",
    "# 8. Distance to center vs Price\n",
    "axes[2, 1].scatter(real_estate_data['distance_to_center'], real_estate_data['price'], \n",
    "                   alpha=0.6, s=50, color='brown')\n",
    "axes[2, 1].set_title('Distance to Center vs Price', fontsize=14, fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Distance to Center (miles)')\n",
    "axes[2, 1].set_ylabel('Price ($)')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_distance_price = real_estate_data['distance_to_center'].corr(real_estate_data['price'])\n",
    "axes[2, 1].text(0.05, 0.95, f'Correlation: {corr_distance_price:.3f}', \n",
    "                transform=axes[2, 1].transAxes, fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# 9. School rating vs Price\n",
    "axes[2, 2].scatter(real_estate_data['school_rating'], real_estate_data['price'], \n",
    "                   alpha=0.6, s=50, color='darkgreen')\n",
    "axes[2, 2].set_title('School Rating vs Price', fontsize=14, fontweight='bold')\n",
    "axes[2, 2].set_xlabel('School Rating (1-10)')\n",
    "axes[2, 2].set_ylabel('Price ($)')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr_school_price = real_estate_data['school_rating'].corr(real_estate_data['price'])\n",
    "axes[2, 2].text(0.05, 0.95, f'Correlation: {corr_school_price:.3f}', \n",
    "                transform=axes[2, 2].transAxes, fontsize=12,\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"=== KEY INSIGHTS FROM EDA ===\")\n",
    "print(f\"1. Price Range: ${real_estate_data['price'].min():,} - ${real_estate_data['price'].max():,}\")\n",
    "print(f\"2. Average Price: ${real_estate_data['price'].mean():,.0f}\")\n",
    "print(f\"3. Price Standard Deviation: ${real_estate_data['price'].std():,.0f}\")\n",
    "print(f\"4. Strongest positive correlation with price: Square footage ({corr_sqft_price:.3f})\")\n",
    "print(f\"5. Strongest negative correlation with price: Distance to center ({corr_distance_price:.3f})\")\n",
    "print(f\"6. Garage premium: ${garage_mean - no_garage_mean:,.0f} on average\")\n",
    "print(f\"7. Age depreciation: {corr_age_price:.3f} correlation\")\n",
    "print(f\"8. Location importance: {corr_location_price:.3f} correlation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b498f71",
   "metadata": {},
   "source": [
    "## Linear Regression Implementation from Scratch\n",
    "\n",
    "Now we'll implement linear regression from scratch using the mathematical formulas we discussed. This helps understand what's happening under the hood.\n",
    "\n",
    "**Mathematical Implementation**: We'll implement both the analytical solution and gradient descent for comparison, following the approaches described in Murphy (2012) \\citep{murphy2012machine}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd9b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionFromScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation from scratch using NumPy.\n",
    "    \n",
    "    This class implements both analytical (closed-form) and gradient descent solutions\n",
    "    for educational purposes.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    coefficients_ : np.ndarray\n",
    "        Fitted coefficients (β₁, β₂, ..., βₚ)\n",
    "    intercept_ : float\n",
    "        Fitted intercept (β₀)\n",
    "    cost_history_ : list\n",
    "        Cost function values during gradient descent (if used)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='analytical', learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str, default='analytical'\n",
    "            Method to use: 'analytical' or 'gradient_descent'\n",
    "        learning_rate : float, default=0.01\n",
    "            Learning rate for gradient descent\n",
    "        max_iterations : int, default=1000\n",
    "            Maximum iterations for gradient descent\n",
    "        tolerance : float, default=1e-6\n",
    "            Convergence tolerance for gradient descent\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.coefficients_ = None\n",
    "        self.intercept_ = None\n",
    "        self.cost_history_ = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : np.ndarray, shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Add bias term (intercept) to X\n",
    "        X_with_bias = np.column_stack([np.ones(X.shape[0]), X])\n",
    "        \n",
    "        if self.method == 'analytical':\n",
    "            self._fit_analytical(X_with_bias, y)\n",
    "        elif self.method == 'gradient_descent':\n",
    "            self._fit_gradient_descent(X_with_bias, y)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'analytical' or 'gradient_descent'\")\n",
    "    \n",
    "    def _fit_analytical(self, X_with_bias, y):\n",
    "        \"\"\"\n",
    "        Fit using analytical solution: β = (X^T X)^(-1) X^T y\n",
    "        \"\"\"\n",
    "        # Calculate (X^T X)^(-1) X^T y\n",
    "        XtX = X_with_bias.T @ X_with_bias\n",
    "        XtX_inv = np.linalg.inv(XtX)\n",
    "        Xty = X_with_bias.T @ y\n",
    "        \n",
    "        theta = XtX_inv @ Xty\n",
    "        \n",
    "        self.intercept_ = theta[0]\n",
    "        self.coefficients_ = theta[1:]\n",
    "    \n",
    "    def _fit_gradient_descent(self, X_with_bias, y):\n",
    "        \"\"\"\n",
    "        Fit using gradient descent optimization.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X_with_bias.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        theta = np.zeros(n_features)\n",
    "        \n",
    "        self.cost_history_ = []\n",
    "        \n",
    "        for i in range(self.max_iterations):\n",
    "            # Forward pass: calculate predictions\n",
    "            y_pred = X_with_bias @ theta\n",
    "            \n",
    "            # Calculate cost (Mean Squared Error)\n",
    "            cost = np.mean((y_pred - y) ** 2)\n",
    "            self.cost_history_.append(cost)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            gradients = (2 / n_samples) * X_with_bias.T @ (y_pred - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            theta_new = theta - self.learning_rate * gradients\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.allclose(theta, theta_new, atol=self.tolerance):\n",
    "                print(f\"Converged after {i+1} iterations\")\n",
    "                break\n",
    "            \n",
    "            theta = theta_new\n",
    "        \n",
    "        self.intercept_ = theta[0]\n",
    "        self.coefficients_ = theta[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the fitted model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray, shape (n_samples, n_features)\n",
    "            Input data\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray, shape (n_samples,)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        if self.coefficients_ is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        X = np.array(X)\n",
    "        return self.intercept_ + X @ self.coefficients_\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² score.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Input data\n",
    "        y : np.ndarray\n",
    "            True values\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            R² score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Test our implementation with a simple example\n",
    "print(\"Testing Linear Regression Implementation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start with square footage as single predictor\n",
    "X_simple = real_estate_data[['sqft']].values\n",
    "y = real_estate_data['price'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_simple, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Test analytical method\n",
    "model_analytical = LinearRegressionFromScratch(method='analytical')\n",
    "model_analytical.fit(X_train, y_train)\n",
    "\n",
    "print(\"Analytical Solution:\")\n",
    "print(f\"Intercept: ${model_analytical.intercept_:,.2f}\")\n",
    "print(f\"Coefficient (sqft): ${model_analytical.coefficients_[0]:.2f} per sqft\")\n",
    "\n",
    "# Test gradient descent method  \n",
    "model_gd = LinearRegressionFromScratch(method='gradient_descent', learning_rate=0.0000001, max_iterations=5000)\n",
    "model_gd.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\\\nGradient Descent Solution:\")\n",
    "print(f\"Intercept: ${model_gd.intercept_:,.2f}\")\n",
    "print(f\"Coefficient (sqft): ${model_gd.coefficients_[0]:.2f} per sqft\")\n",
    "\n",
    "# Compare predictions\n",
    "y_pred_analytical = model_analytical.predict(X_test)\n",
    "y_pred_gd = model_gd.predict(X_test)\n",
    "\n",
    "print(f\"\\\\nR² Score (Analytical): {model_analytical.score(X_test, y_test):.4f}\")\n",
    "print(f\"R² Score (Gradient Descent): {model_gd.score(X_test, y_test):.4f}\")\n",
    "\n",
    "# Visualize gradient descent convergence\n",
    "if len(model_gd.cost_history_) > 1:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(model_gd.cost_history_, linewidth=2, color='blue')\n",
    "    plt.title('Gradient Descent Convergence', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost (MSE)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_test, y_test, alpha=0.6, label='Actual', s=50)\n",
    "    plt.plot(X_test, y_pred_analytical, color='red', linewidth=2, label='Analytical')\n",
    "    plt.plot(X_test, y_pred_gd, color='green', linewidth=2, linestyle='--', label='Gradient Descent')\n",
    "    plt.title('Predictions Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Square Footage')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927faf8f",
   "metadata": {},
   "source": [
    "## Data Storage and Management\n",
    "\n",
    "Following best practices, we'll save our processed real estate data to the appropriate directories for future use.\n",
    "\n",
    "**Citation**: Data management best practices are outlined by Wilkinson et al. (2016) \\citep{wilkinson2016fair}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6273d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data following the established directory structure\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create timestamp for versioning\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Ensure data directories exist\n",
    "data_dirs = ['../data/processed', '../data/raw', '../data/features']\n",
    "for dir_path in data_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "# Save raw simulated data\n",
    "save_data(real_estate_data, f'../data/raw/{timestamp}_real_estate_raw.csv', format='csv')\n",
    "save_data(real_estate_data, f'../data/raw/{timestamp}_real_estate_raw.json', format='json')\n",
    "\n",
    "# Create feature-engineered version for future use\n",
    "real_estate_features = real_estate_data.copy()\n",
    "\n",
    "# Add derived features that might be useful for other algorithms\n",
    "real_estate_features['price_per_sqft'] = real_estate_features['price'] / real_estate_features['sqft']\n",
    "real_estate_features['total_rooms'] = real_estate_features['bedrooms'] + real_estate_features['bathrooms']\n",
    "real_estate_features['luxury_score'] = (\n",
    "    real_estate_features['location_score'] * 0.4 + \n",
    "    real_estate_features['school_rating'] * 0.3 + \n",
    "    (real_estate_features['sqft'] / 1000) * 0.2 + \n",
    "    real_estate_features['garage'] * 0.1\n",
    ")\n",
    "\n",
    "# Create categorical features\n",
    "real_estate_features['age_category'] = pd.cut(\n",
    "    real_estate_features['age'], \n",
    "    bins=[0, 5, 15, 30, 50], \n",
    "    labels=['New', 'Recent', 'Established', 'Old']\n",
    ")\n",
    "\n",
    "real_estate_features['size_category'] = pd.cut(\n",
    "    real_estate_features['sqft'], \n",
    "    bins=[0, 1200, 2000, 3000, 5000], \n",
    "    labels=['Small', 'Medium', 'Large', 'Luxury']\n",
    ")\n",
    "\n",
    "# Save feature-engineered data\n",
    "save_data(real_estate_features, f'../data/features/{timestamp}_real_estate_features.csv', format='csv')\n",
    "\n",
    "# Create train/test splits and save them\n",
    "X = real_estate_features[['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                         'distance_to_center', 'garage', 'school_rating']].values\n",
    "y = real_estate_features['price'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save splits for reproducibility\n",
    "train_data = pd.DataFrame(X_train, columns=['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                                           'distance_to_center', 'garage', 'school_rating'])\n",
    "train_data['price'] = y_train\n",
    "\n",
    "test_data = pd.DataFrame(X_test, columns=['sqft', 'bedrooms', 'bathrooms', 'age', 'location_score', \n",
    "                                         'distance_to_center', 'garage', 'school_rating'])\n",
    "test_data['price'] = y_test\n",
    "\n",
    "save_data(train_data, f'../data/processed/{timestamp}_real_estate_train.csv', format='csv')\n",
    "save_data(test_data, f'../data/processed/{timestamp}_real_estate_test.csv', format='csv')\n",
    "\n",
    "# Create metadata file for the dataset\n",
    "metadata = {\n",
    "    'dataset_name': 'Real Estate Market Data',\n",
    "    'creation_date': timestamp,\n",
    "    'source': 'Simulated data based on realistic market patterns',\n",
    "    'features': {\n",
    "        'sqft': 'Square footage of the property',\n",
    "        'bedrooms': 'Number of bedrooms',\n",
    "        'bathrooms': 'Number of bathrooms',\n",
    "        'age': 'Age of the property in years',\n",
    "        'location_score': 'Neighborhood desirability score (1-10)',\n",
    "        'distance_to_center': 'Distance to city center in miles',\n",
    "        'garage': 'Has garage (0/1)',\n",
    "        'school_rating': 'Local school quality rating (1-10)',\n",
    "        'price': 'Property price in USD (target variable)',\n",
    "        'price_per_sqft': 'Price per square foot',\n",
    "        'total_rooms': 'Total bedrooms + bathrooms',\n",
    "        'luxury_score': 'Computed luxury index',\n",
    "        'age_category': 'Categorical age grouping',\n",
    "        'size_category': 'Categorical size grouping'\n",
    "    },\n",
    "    'statistics': {\n",
    "        'n_samples': len(real_estate_data),\n",
    "        'n_features': len(real_estate_features.columns),\n",
    "        'price_range': {\n",
    "            'min': float(real_estate_data['price'].min()),\n",
    "            'max': float(real_estate_data['price'].max()),\n",
    "            'mean': float(real_estate_data['price'].mean()),\n",
    "            'std': float(real_estate_data['price'].std())\n",
    "        }\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'missing_values': real_estate_features.isnull().sum().to_dict(),\n",
    "        'duplicates': int(real_estate_features.duplicated().sum()),\n",
    "        'outliers_detected': 'See EDA visualizations for outlier analysis'\n",
    "    },\n",
    "    'usage_notes': [\n",
    "        'Data is simulated for educational purposes',\n",
    "        'Relationships between features are based on real market patterns',\n",
    "        'Suitable for regression analysis and machine learning practice',\n",
    "        'Train/test splits provided for reproducible experiments'\n",
    "    ]\n",
    "}\n",
    "\n",
    "save_data(metadata, f'../data/processed/{timestamp}_real_estate_metadata.json', format='json')\n",
    "\n",
    "print(\"=== DATA SAVED SUCCESSFULLY ===\")\n",
    "print(f\"Timestamp: {timestamp}\")\n",
    "print(\"\\\\nFiles saved:\")\n",
    "print(f\"  Raw data: ../data/raw/{timestamp}_real_estate_raw.csv\")\n",
    "print(f\"  Features: ../data/features/{timestamp}_real_estate_features.csv\") \n",
    "print(f\"  Training: ../data/processed/{timestamp}_real_estate_train.csv\")\n",
    "print(f\"  Testing:  ../data/processed/{timestamp}_real_estate_test.csv\")\n",
    "print(f\"  Metadata: ../data/processed/{timestamp}_real_estate_metadata.json\")\n",
    "\n",
    "print(f\"\\\\nDataset summary:\")\n",
    "print(f\"  Total samples: {len(real_estate_data):,}\")\n",
    "print(f\"  Training samples: {len(train_data):,}\")\n",
    "print(f\"  Testing samples: {len(test_data):,}\")\n",
    "print(f\"  Features: {len(real_estate_features.columns)}\")\n",
    "print(f\"  Price range: ${real_estate_data['price'].min():,} - ${real_estate_data['price'].max():,}\")\n",
    "\n",
    "print(\"\\\\n📁 Data organization follows best practices:\")\n",
    "print(\"  ✅ Raw data preserved in /raw directory\")\n",
    "print(\"  ✅ Processed data in /processed directory\") \n",
    "print(\"  ✅ Feature-engineered data in /features directory\")\n",
    "print(\"  ✅ Comprehensive metadata documentation\")\n",
    "print(\"  ✅ Reproducible train/test splits\")\n",
    "print(\"  ✅ Timestamped for version control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4041da",
   "metadata": {},
   "source": [
    "# Linear Regression: House Price Prediction\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "Linear regression is one of the fundamental algorithms in machine learning for supervised learning tasks. This notebook demonstrates end-to-end implementation of linear regression for predicting house prices based on various features.\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the mathematical foundation of linear regression\n",
    "- Implement data preprocessing and feature engineering\n",
    "- Train and evaluate linear regression models\n",
    "- Visualize model performance and residual analysis\n",
    "- Handle real-world data challenges\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "Linear regression assumes a linear relationship between input features and target variable:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the target variable (house price)\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_i$ are the coefficients for features $x_i$\n",
    "- $\\epsilon$ is the error term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862f31a5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We start by importing all necessary libraries for data manipulation, modeling, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and numerical computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import make_regression, fetch_california_housing\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeccbd12",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Loading\n",
    "\n",
    "We'll work with both synthetic and real datasets to understand different aspects of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb13633",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_house_price_data(n_samples: int = 1000, noise_level: float = 0.1) -> pd.DataFrame:\n",
    "    \"\"\"Generate synthetic house price dataset with realistic features.\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        noise_level: Amount of noise to add to the target variable\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with house features and prices\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate realistic house features\n",
    "    house_size = np.random.normal(2000, 500, n_samples)  # Square feet\n",
    "    house_size = np.clip(house_size, 500, 5000)  # Reasonable bounds\n",
    "    \n",
    "    bedrooms = np.random.poisson(3, n_samples) + 1  # 1-7 bedrooms typically\n",
    "    bedrooms = np.clip(bedrooms, 1, 7)\n",
    "    \n",
    "    bathrooms = np.random.normal(2.5, 0.8, n_samples)  # 1-5 bathrooms\n",
    "    bathrooms = np.clip(bathrooms, 1, 5)\n",
    "    \n",
    "    age = np.random.exponential(15, n_samples)  # House age in years\n",
    "    age = np.clip(age, 0, 100)\n",
    "    \n",
    "    # Distance to city center (km)\n",
    "    distance_to_center = np.random.gamma(2, 5, n_samples)\n",
    "    distance_to_center = np.clip(distance_to_center, 1, 50)\n",
    "    \n",
    "    # School rating (1-10)\n",
    "    school_rating = np.random.beta(2, 2, n_samples) * 10\n",
    "    \n",
    "    # Generate price based on realistic relationships\n",
    "    base_price = (\n",
    "        100 * house_size +  # $100 per sq ft\n",
    "        15000 * bedrooms +   # $15k per bedroom\n",
    "        20000 * bathrooms +  # $20k per bathroom\n",
    "        -2000 * age +        # Depreciation\n",
    "        -1000 * distance_to_center +  # Location premium\n",
    "        5000 * school_rating +  # School district premium\n",
    "        100000  # Base price\n",
    "    )\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, noise_level * np.mean(base_price), n_samples)\n",
    "    price = base_price + noise\n",
    "    \n",
    "    # Ensure positive prices\n",
    "    price = np.maximum(price, 50000)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'house_size_sqft': house_size,\n",
    "        'bedrooms': bedrooms,\n",
    "        'bathrooms': bathrooms,\n",
    "        'age_years': age,\n",
    "        'distance_to_center_km': distance_to_center,\n",
    "        'school_rating': school_rating,\n",
    "        'price': price\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic dataset\n",
    "df_synthetic = generate_house_price_data(n_samples=1000)\n",
    "print(f\"Generated synthetic dataset with {len(df_synthetic)} samples\")\n",
    "print(f\"Features: {list(df_synthetic.columns[:-1])}\")\n",
    "print(f\"Target: {df_synthetic.columns[-1]}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "df_synthetic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1073b5",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Understanding our data is crucial before building models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d0e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Perform comprehensive exploratory data analysis.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "    \"\"\"\n",
    "    print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Target variable distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Price distribution\n",
    "    axes[0, 0].hist(df['price'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Price Distribution')\n",
    "    axes[0, 0].set_xlabel('Price ($)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Log price distribution (often more normal)\n",
    "    axes[0, 1].hist(np.log(df['price']), bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].set_title('Log Price Distribution')\n",
    "    axes[0, 1].set_xlabel('Log Price')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Price vs house size (most important feature)\n",
    "    axes[1, 0].scatter(df['house_size_sqft'], df['price'], alpha=0.6, color='coral')\n",
    "    axes[1, 0].set_title('Price vs House Size')\n",
    "    axes[1, 0].set_xlabel('House Size (sq ft)')\n",
    "    axes[1, 0].set_ylabel('Price ($)')\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    corr_matrix = df.corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation with target\n",
    "    print(\"\\nCorrelation with price:\")\n",
    "    price_corr = df.corr()['price'].sort_values(ascending=False)\n",
    "    for feature, corr in price_corr.items():\n",
    "        if feature != 'price':\n",
    "            print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# Perform EDA\n",
    "perform_eda(df_synthetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52dd3dd",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Prepare the data for machine learning by handling missing values, scaling features, and splitting into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80511aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame, target_col: str = 'price', \n",
    "                   test_size: float = 0.2, scale_features: bool = True) -> Tuple:\n",
    "    \"\"\"Preprocess data for machine learning.\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        target_col: Name of target column\n",
    "        test_size: Proportion for test set\n",
    "        scale_features: Whether to standardize features\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (X_train, X_test, y_train, y_test, scaler, feature_names)\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale features if requested\n",
    "    scaler = None\n",
    "    if scale_features:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_names\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test, scaler, feature_names = preprocess_data(df_synthetic)\n",
    "\n",
    "print(\"\\nFeature scaling statistics (training set):\")\n",
    "if scaler is not None:\n",
    "    print(f\"Feature means: {scaler.mean_}\")\n",
    "    print(f\"Feature std: {scaler.scale_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7f2cc",
   "metadata": {},
   "source": [
    "## 5. Model Implementation and Training\n",
    "\n",
    "Implement different variants of linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d1d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_models(X_train: np.ndarray, y_train: np.ndarray) -> dict:\n",
    "    \"\"\"Train multiple linear regression variants.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trained models\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=1.0),\n",
    "    }\n",
    "    \n",
    "    trained_models = {}\n",
    "    \n",
    "    print(\"Training models...\")\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        # Print model coefficients\n",
    "        if hasattr(model, 'coef_'):\n",
    "            print(f\"Intercept: {model.intercept_:.2f}\")\n",
    "            print(f\"Coefficients: {model.coef_}\")\n",
    "            \n",
    "    return trained_models\n",
    "\n",
    "# Train models\n",
    "models = train_linear_models(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5827dc5c",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate model performance using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749591d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression_models(models: dict, X_train: np.ndarray, X_test: np.ndarray,\n",
    "                             y_train: np.ndarray, y_test: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate regression models comprehensively.\n",
    "    \n",
    "    Args:\n",
    "        models: Dictionary of trained models\n",
    "        X_train, X_test: Training and test features\n",
    "        y_train, y_test: Training and test targets\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation metrics\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Make predictions\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        \n",
    "        train_mse = mean_squared_error(y_train, train_pred)\n",
    "        test_mse = mean_squared_error(y_test, test_pred)\n",
    "        \n",
    "        train_mae = mean_absolute_error(y_train, train_pred)\n",
    "        test_mae = mean_absolute_error(y_test, test_pred)\n",
    "        \n",
    "        # Cross-validation score\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Train R²': train_r2,\n",
    "            'Test R²': test_r2,\n",
    "            'Train RMSE': np.sqrt(train_mse),\n",
    "            'Test RMSE': np.sqrt(test_mse),\n",
    "            'Train MAE': train_mae,\n",
    "            'Test MAE': test_mae,\n",
    "            'CV R² Mean': cv_scores.mean(),\n",
    "            'CV R² Std': cv_scores.std()\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_results = evaluate_regression_models(models, X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(evaluation_results.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = evaluation_results.loc[evaluation_results['Test R²'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest performing model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc37f2",
   "metadata": {},
   "source": [
    "## 7. Model Interpretation and Feature Importance\n",
    "\n",
    "Understand which features are most important for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaedcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_names: List[str]) -> None:\n",
    "    \"\"\"Analyze and visualize feature importance.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained linear model\n",
    "        feature_names: Names of features\n",
    "    \"\"\"\n",
    "    if hasattr(model, 'coef_'):\n",
    "        coefficients = model.coef_\n",
    "        \n",
    "        # Create feature importance DataFrame\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': coefficients,\n",
    "            'Abs_Coefficient': np.abs(coefficients)\n",
    "        }).sort_values('Abs_Coefficient', ascending=False)\n",
    "        \n",
    "        print(\"Feature Importance (by coefficient magnitude):\")\n",
    "        print(importance_df)\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Coefficient values\n",
    "        plt.subplot(2, 1, 1)\n",
    "        colors = ['red' if coef < 0 else 'green' for coef in importance_df['Coefficient']]\n",
    "        plt.barh(importance_df['Feature'], importance_df['Coefficient'], color=colors, alpha=0.7)\n",
    "        plt.title('Feature Coefficients (Positive = Increases Price, Negative = Decreases Price)')\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Absolute coefficient values\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.barh(importance_df['Feature'], importance_df['Abs_Coefficient'], \n",
    "                color='skyblue', alpha=0.7)\n",
    "        plt.title('Feature Importance (Absolute Coefficient Values)')\n",
    "        plt.xlabel('Absolute Coefficient Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return importance_df\n",
    "    else:\n",
    "        print(\"Model does not have coefficients to analyze.\")\n",
    "        return None\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(best_model, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6ba55d",
   "metadata": {},
   "source": [
    "## 8. Residual Analysis\n",
    "\n",
    "Analyze model residuals to check assumptions and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146a9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_residual_analysis(model, X_test: np.ndarray, y_test: np.ndarray) -> None:\n",
    "    \"\"\"Perform comprehensive residual analysis.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: Test targets\n",
    "    \"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    residuals = y_test - y_pred\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Predicted vs Actual\n",
    "    axes[0, 0].scatter(y_test, y_pred, alpha=0.6, color='blue')\n",
    "    axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                   'r--', lw=2, label='Perfect Prediction')\n",
    "    axes[0, 0].set_xlabel('Actual Values')\n",
    "    axes[0, 0].set_ylabel('Predicted Values')\n",
    "    axes[0, 0].set_title('Predicted vs Actual Values')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # 2. Residuals vs Predicted\n",
    "    axes[0, 1].scatter(y_pred, residuals, alpha=0.6, color='green')\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted Values')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residuals vs Predicted Values')\n",
    "    \n",
    "    # 3. Residual distribution\n",
    "    axes[1, 0].hist(residuals, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Residuals')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Residual Distribution')\n",
    "    \n",
    "    # 4. Q-Q plot for normality check\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot (Normality Check)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical tests\n",
    "    print(\"Residual Analysis Summary:\")\n",
    "    print(f\"Mean residual: {np.mean(residuals):.6f} (should be close to 0)\")\n",
    "    print(f\"Std residual: {np.std(residuals):.2f}\")\n",
    "    \n",
    "    # Shapiro-Wilk test for normality\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(residuals[:5000])  # Limit sample size\n",
    "    print(f\"Shapiro-Wilk test p-value: {shapiro_p:.6f}\")\n",
    "    if shapiro_p > 0.05:\n",
    "        print(\"✓ Residuals appear to be normally distributed\")\n",
    "    else:\n",
    "        print(\"✗ Residuals may not be normally distributed\")\n",
    "    \n",
    "    # Durbin-Watson test for autocorrelation\n",
    "    from statsmodels.stats.diagnostic import durbin_watson\n",
    "    dw_stat = durbin_watson(residuals)\n",
    "    print(f\"Durbin-Watson statistic: {dw_stat:.3f} (2.0 indicates no autocorrelation)\")\n",
    "\n",
    "# Perform residual analysis\n",
    "perform_residual_analysis(best_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201d39a",
   "metadata": {},
   "source": [
    "## 9. Polynomial Regression Extension\n",
    "\n",
    "Explore polynomial features to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b63b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_polynomial_regression(X_train: np.ndarray, X_test: np.ndarray, \n",
    "                              y_train: np.ndarray, y_test: np.ndarray,\n",
    "                              degrees: List[int] = [2, 3]) -> dict:\n",
    "    \"\"\"Train polynomial regression models.\n",
    "    \n",
    "    Args:\n",
    "        X_train, X_test: Training and test features\n",
    "        y_train, y_test: Training and test targets\n",
    "        degrees: List of polynomial degrees to try\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with polynomial models and results\n",
    "    \"\"\"\n",
    "    poly_results = []\n",
    "    poly_models = {}\n",
    "    \n",
    "    for degree in degrees:\n",
    "        print(f\"\\nTraining Polynomial Regression (degree {degree})...\")\n",
    "        \n",
    "        # Create polynomial features\n",
    "        poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        X_train_poly = poly_features.fit_transform(X_train)\n",
    "        X_test_poly = poly_features.transform(X_test)\n",
    "        \n",
    "        print(f\"Original features: {X_train.shape[1]}\")\n",
    "        print(f\"Polynomial features: {X_train_poly.shape[1]}\")\n",
    "        \n",
    "        # Train model with regularization to prevent overfitting\n",
    "        model = Ridge(alpha=1.0)  # Use Ridge to handle high-dimensional polynomial features\n",
    "        model.fit(X_train_poly, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_pred = model.predict(X_train_poly)\n",
    "        test_pred = model.predict(X_test_poly)\n",
    "        \n",
    "        train_r2 = r2_score(y_train, train_pred)\n",
    "        test_r2 = r2_score(y_test, test_pred)\n",
    "        \n",
    "        poly_results.append({\n",
    "            'Degree': degree,\n",
    "            'Train R²': train_r2,\n",
    "            'Test R²': test_r2,\n",
    "            'Train RMSE': np.sqrt(mean_squared_error(y_train, train_pred)),\n",
    "            'Test RMSE': np.sqrt(mean_squared_error(y_test, test_pred)),\n",
    "            'Features': X_train_poly.shape[1]\n",
    "        })\n",
    "        \n",
    "        poly_models[f'Poly_{degree}'] = {\n",
    "            'model': model,\n",
    "            'poly_features': poly_features\n",
    "        }\n",
    "    \n",
    "    # Display results\n",
    "    poly_df = pd.DataFrame(poly_results)\n",
    "    print(\"\\nPolynomial Regression Results:\")\n",
    "    print(poly_df.round(4))\n",
    "    \n",
    "    # Plot complexity vs performance\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(poly_df['Degree'], poly_df['Train R²'], 'o-', label='Training R²', color='blue')\n",
    "    plt.plot(poly_df['Degree'], poly_df['Test R²'], 'o-', label='Test R²', color='red')\n",
    "    plt.xlabel('Polynomial Degree')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Model Performance vs Polynomial Degree')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(poly_df['Features'], poly_df['Test R²'], 'o-', color='green')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Test R² Score')\n",
    "    plt.title('Test Performance vs Model Complexity')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return poly_models, poly_df\n",
    "\n",
    "# Train polynomial regression models\n",
    "poly_models, poly_results_df = train_polynomial_regression(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b1850",
   "metadata": {},
   "source": [
    "## 10. Model Comparison and Selection\n",
    "\n",
    "Compare all models and select the best one based on multiple criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ab92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_all_models() -> None:\n",
    "    \"\"\"Compare all trained models and provide recommendations.\"\"\"\n",
    "    print(\"=== MODEL COMPARISON SUMMARY ===\")\n",
    "    \n",
    "    # Combine linear and polynomial results\n",
    "    print(\"\\n1. Linear Models:\")\n",
    "    print(evaluation_results[['Model', 'Test R²', 'Test RMSE', 'CV R² Mean']].round(4))\n",
    "    \n",
    "    print(\"\\n2. Polynomial Models:\")\n",
    "    print(poly_results_df[['Degree', 'Test R²', 'Test RMSE', 'Features']].round(4))\n",
    "    \n",
    "    # Model selection criteria\n",
    "    print(\"\\n=== MODEL SELECTION CRITERIA ===\")\n",
    "    \n",
    "    # Best linear model\n",
    "    best_linear_idx = evaluation_results['Test R²'].idxmax()\n",
    "    best_linear = evaluation_results.iloc[best_linear_idx]\n",
    "    print(f\"\\nBest Linear Model: {best_linear['Model']}\")\n",
    "    print(f\"  - Test R²: {best_linear['Test R²']:.4f}\")\n",
    "    print(f\"  - Test RMSE: {best_linear['Test RMSE']:.2f}\")\n",
    "    print(f\"  - CV R² Mean: {best_linear['CV R² Mean']:.4f}\")\n",
    "    \n",
    "    # Best polynomial model\n",
    "    best_poly_idx = poly_results_df['Test R²'].idxmax()\n",
    "    best_poly = poly_results_df.iloc[best_poly_idx]\n",
    "    print(f\"\\nBest Polynomial Model: Degree {best_poly['Degree']}\")\n",
    "    print(f\"  - Test R²: {best_poly['Test R²']:.4f}\")\n",
    "    print(f\"  - Test RMSE: {best_poly['Test RMSE']:.2f}\")\n",
    "    print(f\"  - Features: {best_poly['Features']}\")\n",
    "    \n",
    "    # Overall recommendation\n",
    "    print(\"\\n=== RECOMMENDATION ===\")\n",
    "    if best_linear['Test R²'] > best_poly['Test R²'] - 0.01:  # Small tolerance\n",
    "        print(f\"Recommended Model: {best_linear['Model']}\")\n",
    "        print(\"Reason: Simpler model with comparable performance (Occam's Razor)\")\n",
    "    else:\n",
    "        print(f\"Recommended Model: Polynomial Degree {best_poly['Degree']}\")\n",
    "        print(\"Reason: Significantly better performance justifies complexity\")\n",
    "    \n",
    "    # Visualization of all models\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    all_models = []\n",
    "    all_r2 = []\n",
    "    all_rmse = []\n",
    "    \n",
    "    # Add linear models\n",
    "    for _, row in evaluation_results.iterrows():\n",
    "        all_models.append(row['Model'])\n",
    "        all_r2.append(row['Test R²'])\n",
    "        all_rmse.append(row['Test RMSE'])\n",
    "    \n",
    "    # Add polynomial models\n",
    "    for _, row in poly_results_df.iterrows():\n",
    "        all_models.append(f\"Poly Deg {row['Degree']}\")\n",
    "        all_r2.append(row['Test R²'])\n",
    "        all_rmse.append(row['Test RMSE'])\n",
    "    \n",
    "    # Plot R² scores\n",
    "    plt.subplot(1, 2, 1)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(all_models)))\n",
    "    bars = plt.bar(range(len(all_models)), all_r2, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Test R² Score')\n",
    "    plt.title('Model Comparison: R² Scores')\n",
    "    plt.xticks(range(len(all_models)), all_models, rotation=45, ha='right')\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_idx = np.argmax(all_r2)\n",
    "    bars[best_idx].set_color('red')\n",
    "    bars[best_idx].set_alpha(1.0)\n",
    "    \n",
    "    # Plot RMSE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars = plt.bar(range(len(all_models)), all_rmse, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Test RMSE')\n",
    "    plt.title('Model Comparison: RMSE (Lower is Better)')\n",
    "    plt.xticks(range(len(all_models)), all_models, rotation=45, ha='right')\n",
    "    \n",
    "    # Highlight best model (lowest RMSE)\n",
    "    best_rmse_idx = np.argmin(all_rmse)\n",
    "    bars[best_rmse_idx].set_color('red')\n",
    "    bars[best_rmse_idx].set_alpha(1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare all models\n",
    "compare_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c4045",
   "metadata": {},
   "source": [
    "## 11. Real-World Application: California Housing Dataset\n",
    "\n",
    "Apply our best model to a real dataset to validate its effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8fa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_to_real_data() -> None:\n",
    "    \"\"\"Apply the best model to real California housing data.\"\"\"\n",
    "    print(\"=== REAL-WORLD VALIDATION ===\")\n",
    "    \n",
    "    # Load California housing dataset\n",
    "    california_data = fetch_california_housing()\n",
    "    X_real = california_data.data\n",
    "    y_real = california_data.target * 100000  # Convert to actual dollar values\n",
    "    feature_names_real = california_data.feature_names\n",
    "    \n",
    "    print(f\"California Housing Dataset:\")\n",
    "    print(f\"  - Samples: {X_real.shape[0]}\")\n",
    "    print(f\"  - Features: {X_real.shape[1]}\")\n",
    "    print(f\"  - Features: {feature_names_real}\")\n",
    "    print(f\"  - Target: Median house value ($)\")\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train_real, X_test_real, y_train_real, y_test_real = train_test_split(\n",
    "        X_real, y_real, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    scaler_real = StandardScaler()\n",
    "    X_train_real_scaled = scaler_real.fit_transform(X_train_real)\n",
    "    X_test_real_scaled = scaler_real.transform(X_test_real)\n",
    "    \n",
    "    # Train our best model on real data\n",
    "    best_model_real = LinearRegression()  # Use the model type that performed best\n",
    "    best_model_real.fit(X_train_real_scaled, y_train_real)\n",
    "    \n",
    "    # Evaluate on real data\n",
    "    y_pred_real = best_model_real.predict(X_test_real_scaled)\n",
    "    \n",
    "    r2_real = r2_score(y_test_real, y_pred_real)\n",
    "    rmse_real = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "    mae_real = mean_absolute_error(y_test_real, y_pred_real)\n",
    "    \n",
    "    print(f\"\\nReal Data Results:\")\n",
    "    print(f\"  - R² Score: {r2_real:.4f}\")\n",
    "    print(f\"  - RMSE: ${rmse_real:,.2f}\")\n",
    "    print(f\"  - MAE: ${mae_real:,.2f}\")\n",
    "    \n",
    "    # Feature importance on real data\n",
    "    importance_real = pd.DataFrame({\n",
    "        'Feature': feature_names_real,\n",
    "        'Coefficient': best_model_real.coef_,\n",
    "        'Abs_Coefficient': np.abs(best_model_real.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (Real Data):\")\n",
    "    print(importance_real)\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Predicted vs Actual\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_test_real, y_pred_real, alpha=0.5, color='blue')\n",
    "    plt.plot([y_test_real.min(), y_test_real.max()], \n",
    "             [y_test_real.min(), y_test_real.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Price ($)')\n",
    "    plt.ylabel('Predicted Price ($)')\n",
    "    plt.title(f'Real Data: Predicted vs Actual (R² = {r2_real:.3f})')\n",
    "    \n",
    "    # Residuals\n",
    "    residuals_real = y_test_real - y_pred_real\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(y_pred_real, residuals_real, alpha=0.5, color='green')\n",
    "    plt.axhline(y=0, color='red', linestyle='--')\n",
    "    plt.xlabel('Predicted Price ($)')\n",
    "    plt.ylabel('Residuals ($)')\n",
    "    plt.title('Residuals vs Predicted')\n",
    "    \n",
    "    # Feature importance\n",
    "    plt.subplot(2, 2, 3)\n",
    "    colors = ['red' if coef < 0 else 'green' for coef in importance_real['Coefficient']]\n",
    "    plt.barh(importance_real['Feature'], importance_real['Coefficient'], \n",
    "             color=colors, alpha=0.7)\n",
    "    plt.title('Feature Coefficients (Real Data)')\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Error distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(residuals_real, bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "    plt.xlabel('Residuals ($)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Residual Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Apply to real data\n",
    "apply_to_real_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c52fe",
   "metadata": {},
   "source": [
    "## 12. Summary and Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Linear Regression Fundamentals**: We implemented linear regression from scratch and using scikit-learn, understanding the mathematical foundation and assumptions.\n",
    "\n",
    "2. **Data Preprocessing**: Proper data preprocessing including scaling, splitting, and handling different data types is crucial for model performance.\n",
    "\n",
    "3. **Model Variants**: We explored different variants:\n",
    "   - **Linear Regression**: Basic least squares solution\n",
    "   - **Ridge Regression**: L2 regularization to prevent overfitting\n",
    "   - **Lasso Regression**: L1 regularization for feature selection\n",
    "   - **Polynomial Regression**: Capturing non-linear relationships\n",
    "\n",
    "4. **Evaluation Metrics**: Multiple metrics provide different insights:\n",
    "   - **R²**: Proportion of variance explained\n",
    "   - **RMSE**: Root mean squared error in original units\n",
    "   - **MAE**: Mean absolute error, robust to outliers\n",
    "\n",
    "5. **Model Interpretation**: Linear models are highly interpretable through coefficient analysis.\n",
    "\n",
    "6. **Residual Analysis**: Checking model assumptions through residual patterns.\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always scale features** when using regularized models\n",
    "2. **Use cross-validation** for robust performance estimation\n",
    "3. **Analyze residuals** to validate model assumptions\n",
    "4. **Consider regularization** to prevent overfitting\n",
    "5. **Test on real data** to validate model generalization\n",
    "\n",
    "### When to Use Linear Regression\n",
    "\n",
    "✅ **Good for:**\n",
    "- Linear relationships between features and target\n",
    "- Interpretability is important\n",
    "- Baseline model for comparison\n",
    "- Small to medium datasets\n",
    "- When you need to understand feature importance\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Highly non-linear relationships\n",
    "- Very high-dimensional data without regularization\n",
    "- When interpretability is not needed and accuracy is paramount\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Explore **ensemble methods** like Random Forest\n",
    "2. Try **non-linear models** like SVM or Neural Networks\n",
    "3. Implement **feature engineering** techniques\n",
    "4. Learn about **advanced regularization** methods\n",
    "5. Study **time series** regression for temporal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66c519",
   "metadata": {},
   "source": [
    "## 13. Practice Exercises\n",
    "\n",
    "### Exercise 1: Feature Engineering\n",
    "Create new features from existing ones (e.g., price per square foot, age categories) and see how they affect model performance.\n",
    "\n",
    "### Exercise 2: Outlier Detection\n",
    "Implement outlier detection and removal techniques and analyze their impact on model performance.\n",
    "\n",
    "### Exercise 3: Different Datasets\n",
    "Apply the same pipeline to other regression datasets (e.g., Boston housing, automobile prices).\n",
    "\n",
    "### Exercise 4: Hyperparameter Tuning\n",
    "Use GridSearchCV to find optimal regularization parameters for Ridge and Lasso regression.\n",
    "\n",
    "### Exercise 5: Advanced Metrics\n",
    "Implement additional evaluation metrics like MAPE (Mean Absolute Percentage Error) and analyze when each metric is most appropriate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
