{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3a8bcd",
   "metadata": {},
   "source": [
    "# Chapter 17: Gaussian Mixture Models (GMM)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this chapter, you will learn:\n",
    "- **Mathematical foundations** of mixture models and EM algorithm\n",
    "- **Expectation-Maximization** parameter estimation\n",
    "- **Model selection** using information criteria\n",
    "- **Implementation** from scratch using NumPy\n",
    "- **Applications** in clustering and density estimation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Gaussian Mixture Models (GMM) represent data as a mixture of multiple Gaussian distributions, providing a probabilistic approach to clustering and density estimation.\n",
    "\n",
    "**Mathematical Foundation**: GMM assumes data is generated from a mixture of Gaussian distributions with unknown parameters estimated using the EM algorithm.\n",
    "\n",
    "## Mathematical Theory\n",
    "\n",
    "### Mixture Model\n",
    "\n",
    "The probability density function is:\n",
    "\n",
    "$$p(x) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x | \\mu_k, \\Sigma_k)$$\n",
    "\n",
    "Where:\n",
    "- $K$ is the number of components\n",
    "- $\\pi_k$ are the mixing coefficients ($\\sum_{k=1}^{K} \\pi_k = 1$)\n",
    "- $\\mathcal{N}(x | \\mu_k, \\Sigma_k)$ is the $k$-th Gaussian component\n",
    "\n",
    "### EM Algorithm\n",
    "\n",
    "**E-step**: Calculate responsibilities\n",
    "$$\\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x_n | \\mu_j, \\Sigma_j)}$$\n",
    "\n",
    "**M-step**: Update parameters using maximum likelihood estimates\n",
    "\n",
    "**Citation**: Mixture models and the EM algorithm are extensively covered in statistical learning literature."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
